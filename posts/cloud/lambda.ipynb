{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7c2c1250",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Beginner's Guide to Deploying Tensorflow Models on AWS Lambda (Free-Tier)\"\n",
    "description: \"A from-scratch guide to deploying a tensorflow-lite model on cloud using AWS Free-tier\"\n",
    "author: \"Shubham Shinde\"\n",
    "date: \"10/08/2023\"\n",
    "draft: false\n",
    "categories:\n",
    "  - data-science\n",
    "  - deep-learning\n",
    "  - cloud\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b3245",
   "metadata": {},
   "source": [
    "## What's This Post About"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784713db",
   "metadata": {},
   "source": [
    "If you are a data scientist, have no idea about cloud, and want to do something hands-on instead of courses- you are in the right place.\n",
    "\n",
    "![Generated Using SDXL](assets/coverart.png)\n",
    "\n",
    "In this blogpost, we will signup on AWS Free-tier account and use the Lambda service to deploy a trained ML model within the free-tier limits. The code used in this post is available [in this repo](https://github.com/shindeshu/aws_lambda_tflite).\n",
    "\n",
    "Pre-requisites:\n",
    "\n",
    "1. You should be able to know how to train a machine learning model. Here, we are taking an already trained keras model and start from that point. It is a CNN model that classifies given image into either `cat` or `dog`. But if you are interested in the model training aspect, check out [this kaggle notebook](https://www.kaggle.com/code/shindeshubham85/a-friendly-introduction-to-cnns-in-keras)!\n",
    "\n",
    "2. You should be familiar with basics of Docker and REST API.\n",
    "\n",
    "The contents of this blogpost:\n",
    "\n",
    "- Take the trained Keras Model, and convert to tf-lite format (Why? Discussed later.)\n",
    "- Write and test the inference functions for tf-lite model.\n",
    "- Create a docker image using an AWS Lambda image as base, and test it out.\n",
    "- Signup for an AWS Free-tier account. Configure the AWS CLI and ECR (Discussed later.)\n",
    "- Create an AWS Lambda function using the docker image. Test it out.\n",
    "- Create an API Gateway and expose it. That is it!\n",
    "\n",
    "A visual depiction of the flow:\n",
    "\n",
    "![Flow](assets/flow.png)\n",
    "\n",
    "This process is taken from the chapter 9 of the [free course \"ML Zoomcamp\"](https://www.youtube.com/watch?v=MqI8vt3-cag&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR) by Alexey Grigorev, with some modifications. Follow the videos if you want a detailed walkthrough.\n",
    "\n",
    "### What is AWS Lambda / Serverless\n",
    "\n",
    "AWS Lambda is a compute service that lets you run code without provisioning or managing servers.\n",
    "\n",
    "All it asks is for you to bring the code- and the service will do everything else- provisioning servers, managing them, auto-scaling them if the demand increases, and shutting them off if they are not in use, or restarting them. Since we don't need to worry at all about servers, it is also called serverless.\n",
    "\n",
    "In the ML scenario, we will bring the code and the model to the service, and it will produce the predictions of our inputs. Instead of giving the code and the model directly, we will first package them inside a docker image, and use the image to deploy on Lambda.\n",
    "\n",
    "At the end of this post, we can give a URL of an image to a REST API endpoint, and get back the image classification result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399526ba",
   "metadata": {},
   "source": [
    "## Conversion to TF-Lite\n",
    "\n",
    "### Why TF-Lite\n",
    "Our starting point is a file called `cats_and_dogs.h5` which is a keras model trained to classify given image into cat or dog. We can easily load it into our environment using the keras library.\n",
    "\n",
    "```python\n",
    "import tensorflow\n",
    "model = tensorflow.keras.models.load_model(\"cats_and_dogs.h5\")\n",
    "```\n",
    "However, this may not be the best case for deploying the model. Installation of tensorflow itself can take up a LOT of memory. Importing the library also adds latency. We only need to do inference, while the library contains tools for many operations like training, etc. AWS Lambda does not support GPUs, so we don't those sections of codebase either.\n",
    "\n",
    "Other constraints are the free-tier limits of AWS: Since we are deploying in the cloud, the sizes of our docker image need to be under 500MB.\n",
    "\n",
    "One way to remove the boilerplate from the libraries is to convert the model to tf-lite and use the tf-lite library, which we will be employing here.\n",
    "\n",
    "::: {.callout-warning}\n",
    "Note that AWS Lambda only supports CPUs, not GPUs. So we need to deploy models only if they can perform well on CPUs.\n",
    "\n",
    "While TF-Lite supports GPUs, if your deployment is in a GPU-environment, you might be better off with other well-supported compilers like TensorRT.\n",
    ":::\n",
    "\n",
    "### Conversion\n",
    "\n",
    "The code for conversion is pretty straightforward:\n",
    "\n",
    "```python\n",
    "import tensorflow.lite as tflite \n",
    "converter = tflite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with tf.io.gfile.GFile('artifacts/cats_and_dogs.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "```\n",
    "\n",
    "The tricky part is feeding the input and retrieving the output. Fortunately its all wrapped up neat here:\n",
    "\n",
    "```python\n",
    "def load_tflite():\n",
    "    \"\"\"\n",
    "    this function is used to load the tflite model and initialize the \n",
    "    interpreter, and also return the input and output indexes\n",
    "    \"\"\"\n",
    "    interpreter = tflite.Interpreter(model_path='cats_and_dogs.tflite')\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    input_index = input_details[0]['index']\n",
    "    output_details = interpreter.get_output_details()\n",
    "    output_index = output_details[0]['index']\n",
    "    return interpreter, (input_index, output_index)\n",
    "\n",
    "def predict_with_tflite(interpreter, indexes, img):\n",
    "    \"\"\"\n",
    "    this function takes the interpreter, indexes and input image as input\n",
    "    performs the inference, does some postprocessing and returns the result.\n",
    "    \"\"\"\n",
    "    interpreter.set_tensor(indexes[0], img)\n",
    "    interpreter.invoke()\n",
    "    preds = interpreter.get_tensor(indexes[1])\n",
    "    return postprocess(preds)\n",
    "```\n",
    "\n",
    "Another tricky part is the pre-processing function. During training the model, we simply used the `keras.preprocessing` module to perform the pre-processing. Since we are not installing keras or tensorflow, how can we do the pre-processing?\n",
    "\n",
    "Its easy, we do it ourselves. We load the image using PIL library, convert to numpy array, and scale it. The dependencies are light: PIL, and NumPy.\n",
    "\n",
    "Now, we have the complete code for inference ([check the github repo](https://github.com/shindeshu/aws_lambda_tflite/tree/main))- including the pre-processing steps, and prediction with the tf-lite model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927acab",
   "metadata": {},
   "source": [
    "### The Docker Image for Lambda Function\n",
    "\n",
    "Now, we will be creating a docker image to be used in lambda function. (Don't worry about the AWS Account yet.) \n",
    "\n",
    "While preparing a docker image we generally start with a \"base image\". This often is based on Ubuntu. But this will not work here. We need to use the official AWS Lambda as base image which is based on CentOS and includes other artifacts related to Lambda. We will prepare our image from this base image, and upload to AWS Lambda for deployment.\n",
    "\n",
    "How will we prepare the dockerfile?\n",
    "\n",
    "1. Select the appropriate base image\n",
    "2. Install the dependencies\n",
    "3. Copy the lambda handler file\n",
    "4. Copy the artifacts (model file etc.)\n",
    "5. Specify the CMD command\n",
    "\n",
    "#### Creating the Lambda Handler File\n",
    "\n",
    "\n",
    "Before we start preparing the Dockerfile, we need to write a python script `lambda_function.py` that will perform all the pre-processing steps, load the model and do prediction. This is going to be the entrypoint script for the Lambda function.\n",
    "\n",
    "This script should have a function called `lambda_handler` that should take the input from API and return result as json.\n",
    "\n",
    "```python\n",
    "def lambda_handler(event, context):\n",
    "    url = event['url']\n",
    "    X = load_image(url)\n",
    "    result = predict_with_tflite(X)\n",
    "    return result\n",
    "```\n",
    "Here, `load_image` contains the preprocessing logic and `predict_with_tflite` contains the prediction steps. Check the complete `lambda_function.py` file in the github repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0be322",
   "metadata": {},
   "source": [
    "#### Resolving Some Errors\n",
    "\n",
    "While installing the dependencies, you might come across some errors. This is related to the library `tf-lite`, the PyPI binaries are based on Ubuntu/Debian OS and not supported for CentOS. Hence, you will have to install this library using wheel files compiled for that OS- which are fortunately available for us on github, [compiled by Alexey Grigorev](https://github.com/alexeygrigorev/tflite-aws-lambda). A copy of the file is also present in the repo we are working from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87e276",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "\n",
    "Once the required code and model files are ready, we start building the docker image. The sequence of steps is simple:\n",
    "\n",
    "```dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.8\n",
    "\n",
    "COPY artifacts/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl .\n",
    "\n",
    "RUN pip3 install tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl --no-cache-dir\n",
    "RUN pip install requests Pillow\n",
    "\n",
    "COPY artifacts/cats_and_dogs.tflite .\n",
    "COPY artifacts/lambda_function.py .\n",
    "\n",
    "CMD [\"lambda_function.lambda_handler\"]\n",
    "```\n",
    "\n",
    "We build the image using\n",
    "```bash\n",
    "docker build -t cats_and_dogs .\n",
    "```\n",
    "And start running the container using\n",
    "```bash\n",
    "docker run -it --rm cats_and_dogs:latest -p 8080:8080 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7df58",
   "metadata": {},
   "source": [
    "### Testing the Docker Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31348c74",
   "metadata": {},
   "source": [
    "Now, the docker container is running, and presumably our model within is running as well. But how do we use it or test it if it's working?\n",
    "\n",
    "We send a request using the python `requests` library. If you go to our lambda handler script, the `event` argument is basically the json that we are sending over using a POST request. We retrieve the URL from the json payload, process it to get the prediction, and return the result as json response. Let's look at the code:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/2015-03-31/functions/function/invocations\" \n",
    "# this is how the URL for AWS Lambda always looks like\n",
    "# this is the URL where the application inside docker container is listening for requests\n",
    "\n",
    "data = {\"url\": \"https://t4.ftcdn.net/jpg/00/97/58/97/360_F_97589769_t45CqXyzjz0KXwoBZT9PRaWGHRk5hQqQ.jpg\"}\n",
    "# this is the input we want to give to the lambda_handler script\n",
    "\n",
    "results = requests.post(url, json=data).json()\n",
    "# this is the result after prediction and post-processing.\n",
    "\n",
    "print(results)\n",
    "```\n",
    "\n",
    "If you are seeing a dictionary printed that contains the probabilities of the model classes, the docker container is running successfully! Congratulations - all our offline work is complete. All that's left is signing up and moving this image to cloud.\n",
    "\n",
    "::: {.callout-note}\n",
    "Note that we did not install any client like FastAPI/Flask, yet we are able to listen to a REST API. The base docker image likely has a built-in server.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca9d54",
   "metadata": {},
   "source": [
    "## Setup your Free-tier AWS Account\n",
    "\n",
    "### Signup for a Free-tier Account\n",
    "\n",
    "If you don't have any experience in handling cloud, and want to learn- AWS Free-tier is a good option. But it can also be dreaded option, since some might be afraid of racking up large costs accidently. However if you understand the limits, it's very easy to stay below the specified thresholds and still learn the nuts-and-bolts of AWS using toy projects.\n",
    "\n",
    "Before you start creating a free-tier account, it's best if you go through which services are free and what are their limits- [https://aws.amazon.com/free/](https://aws.amazon.com/free/). Also go through a [video example of setting up an account](https://www.youtube.com/watch?v=SFaSB6vgp8k) so there's less surprises.\n",
    "\n",
    "Once the setup is complete, there's still more steps to do for configuring your workspace.\n",
    "\n",
    "### Creating an IAM User\n",
    "\n",
    "Once you have the account set up, you will have one user called 'root'. However it's a best practice to create custom users called \"IAM Users\" for development activity. Create one such user with admin access, and create an access key for this user. We will do our development through this access key.\n",
    "\n",
    "### Setup AWS-CLI\n",
    "\n",
    "We can use AWS through our browser and the UI, but the best Developer Experience is through AWS Command Line Interface. This allows us to work from the terminal. Setting it up is easy,\n",
    "\n",
    "```bash\n",
    "pip install awscli\n",
    "```\n",
    "Configure it using `aws configure` and input the access ID and secret access keys you generated earlier. Input a default region (e.g. us-east-1, ap-south-1) and default format (e.g. json).\n",
    "\n",
    "### Create and Link Amazon ECR\n",
    "\n",
    "ECR is Amazon's container registry, where we can upload docker images. These images we can then use for various applications like AWS Lambda or AWS EKS etc. We will setup ECR and connect it our docker client (Docker Desktop), so that we can upload docker images from docker to ECR.\n",
    "\n",
    "1. We first create an ECR Repo, and *note down the URI*\n",
    "\n",
    "`aws ecr create-repository --repository-name cats_and_dogs_images`\n",
    "\n",
    "You can go to the AWS dashboard in your browser and check if the repository has been created.\n",
    "\n",
    "2. Authenticate docker to work with ECR.\n",
    "\n",
    "```bash\n",
    "$ $(aws ecr get-login)\n",
    "```\n",
    "\n",
    "::: {.callout-warning}\n",
    "For windows, Step2 might result in an error. [This solution from stackoverflow](https://stackoverflow.com/a/65858904) worked for me.\n",
    ":::\n",
    "\n",
    "Following these steps, Docker is successfully authenticated with AWS-ECR.\n",
    "\n",
    "### Upload the Docker Image to ECR Repo\n",
    "\n",
    "1. Tag the image of interest with the remote repository URI\n",
    "\n",
    "```bash\n",
    "docker tag cats_and_dogs:latest ${REMOTE_URI}\n",
    "```\n",
    "\n",
    "2. Push to ECR\n",
    "\n",
    "```bash\n",
    "docker push\n",
    "```\n",
    "\n",
    "Check in browser if the image has been published.\n",
    "\n",
    "## Creating the Lambda Function\n",
    "\n",
    "Now, your AWS Account is set up and you've also uploaded the docker image to ECR. In this section we will use the image to create a lambda function, and also set an API trigger using API Gateway.\n",
    "\n",
    "Creating a Lambda function is easy, from the AWS dashboard we go to AWS Lambda -> Create Function -> From Image -> Select the Uploaded image.\n",
    "\n",
    "![](assets/create.png)\n",
    "\n",
    "### Test the Image\n",
    "\n",
    "By default the timeout is 3s, which we need to increase to 30s in the settings. This is because the model loading for the first time may take upto 7s. We can test the lambda function by giving the json payload in the test area.\n",
    "\n",
    "If you get the familiar output of classes and probabilities, your Lambda function is working!\n",
    "\n",
    "However, currently you can only give the input through the test section. How will the users access the function? How will they connect to Lambda? We can use Amazon's API Gateway for that.\n",
    "\n",
    "### Create a API Gateway\n",
    "\n",
    "From Dashboard, go to API Gateway -> REST API -> new resource. Here you define the endpoint name.\n",
    "Then, define the request type (i.e. POST)\n",
    "In POST, integration type should be changed to Lambda function. Give the region name and name of the lambda function we created earlier.\n",
    "\n",
    "So you have now exposed the Lambda function using a REST API! You can test it there itself in the request body section.\n",
    "\n",
    "But this API Endpoint is still private- we can open it up to other users by going to Actions -> Deploy API. We copy the URL, and test it using python requests library.\n",
    "\n",
    "And this is how we deploy an ML model using AWS Lambda!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
