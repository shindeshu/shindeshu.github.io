{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5df8d7a6",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Creating AI Art of Myself With Stable Diffusion + Dreambooth\"\n",
    "description: \"I tried my hands on stable diffusion, trying to create styled images of myself. Here's what I found.\"\n",
    "author: \"Shubham Shinde\"\n",
    "date: \"11/30/2022\"\n",
    "draft: false\n",
    "categories:\n",
    "  - ai-art\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a13a8bf",
   "metadata": {},
   "source": [
    "## What is  Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f908be10",
   "metadata": {},
   "source": [
    "For years, style transfer and image generation was the domain of an architecture called GANs. It produced results that were, well, not enough to impress a person not into the topic. And it look a lot of time, compute, and didn't always converge. That is history anyway, so we do not need to trouble ourselves.\n",
    "\n",
    "This year changed everything in generative AI. Three models, particularly Stable Diffusion took the world of art by storm. The text-to-image model could produce images with high fidelity, coherence, and able to produce images with so many styles. It was a 4GB checkpoint that contained in itself majority of the visual world created by man. With a bunch of text it could bring imagination to reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbccf00",
   "metadata": {},
   "source": [
    "## The Magic\n",
    "\n",
    "All of these images are generated by AI completely on its own. There are some sites like [lexica.art](lexica.art) where you can scroll through tons of AI-generated art like this, as well as the prompts that led to them.\n",
    "\n",
    "![Some Examples of AI generated art](assets/art_ex3.webp){height=40%}\n",
    "\n",
    "You can try it for yourself at any one of the many huggingface spaces:\n",
    "\n",
    "- [App for Stability Diffusion 1.5](https://huggingface.co/spaces/runwayml/stable-diffusion-v1-5)\n",
    "- [App for Stability Diffusion 2, recently released](https://huggingface.co/spaces/stabilityai/stable-diffusion)\n",
    "- [App that runs models for many styles](https://huggingface.co/spaces/anzorq/finetuned_diffusion)\n",
    "\n",
    "It can take 10-20s to generate an image.\n",
    "\n",
    "If you prefer a more hands-on approach, use this colab notebook\n",
    "- [fast_stable_diffusion_AUTOMATIC1111.ipynb](https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_AUTOMATIC1111.ipynb#scrollTo=PjzwxTkPSPHf), in this notebook, give the path to model hosted on huggingface (I used `runwayml/stable-diffusion-v1-5` for my base model). The notebook has all the code for installing dependencies, and will launch a GUI which you can use for experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e68333d",
   "metadata": {},
   "source": [
    "## Dreambooth\n",
    "\n",
    "However, the publicly released stable diffusion model includes only those concepts, people, artstyles that were present in the training dataset of it. What if you want to add new people/concepts/styles, and use them to generate art?\n",
    "\n",
    "That's where dreambooth comes in. This technique allows you to add a person/concept to the stable diffusion model using only ~20 images of it, and takes only ~1 hour on free colab to train! Because of dreambooth, fine-tuning the stable diffusion model on a given style or person is a breeze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ea026",
   "metadata": {},
   "source": [
    "## Finetuning Approach\n",
    "\n",
    "My primary resource for fine-tuning, i.e. adding myself to the stable diffusion space was this blog post: [How to Use DreamBooth to Fine-Tune Stable Diffusion (Colab)](https://bytexd.com/how-to-use-dreambooth-to-fine-tune-stable-diffusion-colab/). Which points to a beautiful colab notebook - [TheLastBen/fast-stable-diffusion](https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast-DreamBooth.ipynb#scrollTo=iAZGngFcI8hq), which has it all ready- you only need to give the links to the base model, huggingface token (you need to signup to this website), and upload your images, and hit train.\n",
    "\n",
    "Follow the advice in the blogpost and the notebook, and you'll have a gradio app in your browser. Now you can add any text prompt you want in the text box, and generate results!\n",
    "\n",
    "## Finding the Right Prompts\n",
    "Yeah, except, *any* prompt will not give you the best results. Prompt Engineering is a big deal here. You have to construct a prompt, add labels and styles in order to direct the model to get an acceptable image. You also can tinker with the hyperparameters- adjust the CFG scale, sampling steps, to get better results.\n",
    "\n",
    "My experience- prompt engineering sucks. It's more of an art that engineering, and there's no other way but trial and error. Fortunately there are resources on the internet that can help you.\n",
    "\n",
    "For me the process looked like:\n",
    "\n",
    "1. Go to a resource like [lexica.art](lexica.art), and find an image you like\n",
    "2. Copy the prompt and settings, and modify the prompt to include your token used during training.\n",
    "\n",
    "This will generally give you good results, and how I got most of my good images. However tuning of hyperparameters requireds trial and error, and hence time. I didn't spend too much time on this, so I fixed my sampling method at euler-a and steps at 40-50. I only tinked with CFG scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a54d5f",
   "metadata": {},
   "source": [
    "## Results on Myself\n",
    "\n",
    "Finally, the meat of the post- let's see the actual results on myself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8c707",
   "metadata": {},
   "source": [
    "::: { layout-ncol=3}\n",
    "\n",
    "![](assets/port1.png)\n",
    "\n",
    "![](assets/port2.png)\n",
    "\n",
    "![](assets/port3.png)\n",
    "\n",
    "![As a Naruto(?) Character](assets/port6.png)\n",
    "\n",
    "![Style of Japanese Woodblock](assets/port5.png){width=20%}\n",
    "\n",
    "![Me As a Pixar Character](assets/shubham_pixar.png){width=20%}\n",
    "\n",
    "Some Portraits of Me\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2dab51",
   "metadata": {},
   "source": [
    "Some generated art of my wife and me.\n",
    "\n",
    "::: { layout-ncol=3}\n",
    "\n",
    "![As a Disney Princess](assets/ss1.png)\n",
    "\n",
    "![As a Studio Ghibli Character](assets/ss2.png)\n",
    "\n",
    "![As Pixar Characters](assets/ss3.png)\n",
    "\n",
    "![Anime Character](assets/ss4.png)\n",
    "\n",
    "![Stylized Portrait](assets/ss5.png){width=20%}\n",
    "\n",
    "![As a Japanese Woodblock](assets/ss6.png){width=20%}\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c852658",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
