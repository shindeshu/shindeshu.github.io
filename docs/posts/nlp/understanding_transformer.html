<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Shubham Shinde">
<meta name="dcterms.date" content="2023-02-03">
<meta name="description" content="The phenomenon of our age is quite simple at its heart!">

<title>Shubham Shinde - Understanding the Transformer</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-E3Z2GSDBB7"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-E3Z2GSDBB7', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Shubham Shinde - Understanding the Transformer">
<meta property="og:description" content="The phenomenon of our age is quite simple at its heart!">
<meta property="og:site-name" content="Shubham Shinde">
<meta name="twitter:title" content="Shubham Shinde - Understanding the Transformer">
<meta name="twitter:description" content="The phenomenon of our age is quite simple at its heart!">
<meta name="twitter:creator" content="@ShindeShubham85">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shubham Shinde</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">Home</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">About</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html">Blog</a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#transformers---the-phenomenon-of-our-age" id="toc-transformers---the-phenomenon-of-our-age" class="nav-link active" data-scroll-target="#transformers---the-phenomenon-of-our-age">Transformers - the Phenomenon of Our Age</a></li>
  <li><a href="#attention-vs-sequential-models" id="toc-attention-vs-sequential-models" class="nav-link" data-scroll-target="#attention-vs-sequential-models">Attention vs Sequential Models</a></li>
  <li><a href="#self-attention-is-translation-invariant" id="toc-self-attention-is-translation-invariant" class="nav-link" data-scroll-target="#self-attention-is-translation-invariant">Self-Attention Is Translation Invariant</a></li>
  <li><a href="#why-attention-is-important" id="toc-why-attention-is-important" class="nav-link" data-scroll-target="#why-attention-is-important">Why Attention is Important</a></li>
  <li><a href="#a-single-dense-layer-vs-attention" id="toc-a-single-dense-layer-vs-attention" class="nav-link" data-scroll-target="#a-single-dense-layer-vs-attention">A Single Dense Layer vs Attention</a></li>
  <li><a href="#q-k-v-in-simple-terms" id="toc-q-k-v-in-simple-terms" class="nav-link" data-scroll-target="#q-k-v-in-simple-terms">Q-K-V in Simple Terms</a></li>
  <li><a href="#q-k-v-matrices" id="toc-q-k-v-matrices" class="nav-link" data-scroll-target="#q-k-v-matrices">Q-K-V Matrices</a>
  <ul class="collapse">
  <li><a href="#start-with-an-embedding-for-each-word" id="toc-start-with-an-embedding-for-each-word" class="nav-link" data-scroll-target="#start-with-an-embedding-for-each-word">Start with an Embedding for Each Word</a></li>
  <li><a href="#get-the-q-k-v-vectors-for-all-words" id="toc-get-the-q-k-v-vectors-for-all-words" class="nav-link" data-scroll-target="#get-the-q-k-v-vectors-for-all-words">Get the Q, K, V vectors for All Words</a></li>
  <li><a href="#compute-the-weights-from-q-k-and-multiply-them-with-v" id="toc-compute-the-weights-from-q-k-and-multiply-them-with-v" class="nav-link" data-scroll-target="#compute-the-weights-from-q-k-and-multiply-them-with-v">Compute the ‘Weights’ from Q, K and Multiply Them with V</a></li>
  <li><a href="#finally-send-this-to-a-dense-layer" id="toc-finally-send-this-to-a-dense-layer" class="nav-link" data-scroll-target="#finally-send-this-to-a-dense-layer">Finally, Send This to a Dense Layer</a></li>
  </ul></li>
  <li><a href="#transformer-components" id="toc-transformer-components" class="nav-link" data-scroll-target="#transformer-components">Transformer Components</a>
  <ul class="collapse">
  <li><a href="#types-of-transformers" id="toc-types-of-transformers" class="nav-link" data-scroll-target="#types-of-transformers">Types of Transformers</a></li>
  <li><a href="#components" id="toc-components" class="nav-link" data-scroll-target="#components">Components</a></li>
  <li><a href="#multi-headed-attention" id="toc-multi-headed-attention" class="nav-link" data-scroll-target="#multi-headed-attention">Multi-Headed Attention</a></li>
  <li><a href="#position-embeddings" id="toc-position-embeddings" class="nav-link" data-scroll-target="#position-embeddings">Position Embeddings</a></li>
  <li><a href="#residual-connections-and-layernorm" id="toc-residual-connections-and-layernorm" class="nav-link" data-scroll-target="#residual-connections-and-layernorm">Residual Connections and LayerNorm</a></li>
  </ul></li>
  <li><a href="#decoder" id="toc-decoder" class="nav-link" data-scroll-target="#decoder">Decoder</a></li>
  <li><a href="#beam-search" id="toc-beam-search" class="nav-link" data-scroll-target="#beam-search">Beam-Search</a></li>
  <li><a href="#links" id="toc-links" class="nav-link" data-scroll-target="#links">Links</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Understanding the Transformer</h1>
  <div class="quarto-categories">
    <div class="quarto-category">deep-learning</div>
    <div class="quarto-category">nlp</div>
  </div>
  </div>

<div>
  <div class="description">
    The phenomenon of our age is quite simple at its heart!
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Shubham Shinde </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 3, 2023</p>
    </div>
  </div>
    
  </div>
  

</header>

<section id="transformers---the-phenomenon-of-our-age" class="level2">
<h2 class="anchored" data-anchor-id="transformers---the-phenomenon-of-our-age">Transformers - the Phenomenon of Our Age</h2>
<p>It is not hyperbolic to call transformers as the greatest invention in last 5 years or so. With ChatGPT and Bing AI, the quality of machine generated text has shattered everyone’s projections about artificial intelligence. However, what’s fascinating that the underlying architecture that has enabled all these advancements in NLP is relatively quite simple and easy to understand. In this post, we’ll try to understand it.</p>
<p><strong>Contents of this Post:</strong> A brief comparison of transformers and sequential models, importance of attention, Q-K-V in simpler terms, and in detail, other components of transformers, decoder, beam-search.</p>
</section>
<section id="attention-vs-sequential-models" class="level2">
<h2 class="anchored" data-anchor-id="attention-vs-sequential-models">Attention vs Sequential Models</h2>
<p>Many people do have an understanding of RNNs / LSTMs, probably since they’ve been around for a long time, they’re a part of many a coursework, etc. And hence they try to understand transformers from that angle, as sort of a natural evolution. However transformers are extremely different from sequential models because they, well, are not sequential models.</p>
<p>In sequential models, we have units one after another, in a chain. Each unit/word has direct access to information of only the preceding unit/word. This is limiting - dependencies between words at different positions is not modelled well. For sequence-to-sequence prediction, the entirety of input is encoded in a single vector, and the decoder is supposed to decode everything from that. This causes information loss because all of it is crushed into a single vector. Transformer solves both of these issues and more.</p>
<p>For a silly comparison think of sequential models as the assassination chain meme, whereas an attention model as the mexican standoff meme. In attention, the mexican standoff has mutant people with dozens of arms pointing at everybody!</p>
<div id="silly-com" class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/mexican_standoff.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Attention, a Mexican Standoff</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/assa_chain_meme.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Sequential Assassination Chain</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="self-attention-is-translation-invariant" class="level2">
<h2 class="anchored" data-anchor-id="self-attention-is-translation-invariant">Self-Attention Is Translation Invariant</h2>
<p>Self-Attention is translation invariant by nature, which means even if you exchange few words, the output will be same. To incorporate the position information, we have a separate embedding called position embedding that we’ll look at later. But in itself, self-attention does not have a “sequence” among the input words. This is really powerful since - it allows each word in a sentence to be considered for computing output - it allows us to parallelize operations on GPUs, enabling us to build larger powerful models.</p>
</section>
<section id="why-attention-is-important" class="level2">
<h2 class="anchored" data-anchor-id="why-attention-is-important">Why Attention is Important</h2>
<p>Attention is the underlying mechanism that powers transformers. Let us see <em>why</em> it is necessary. Take this sentence as example</p>
<p><code>The animal didn't cross the street because it was too tired</code></p>
<p>What does the word “it” refer to? We know that it refers to “animal”, but the model should also be able to connect to the relevant word and use that information while predicting. Self-Attention allows this. Here, the attention weights visualized indeed show us that.</p>
<p><img src="assets/att_example.png" class="img-fluid" style="width:70.0%"></p>
</section>
<section id="a-single-dense-layer-vs-attention" class="level2">
<h2 class="anchored" data-anchor-id="a-single-dense-layer-vs-attention">A Single Dense Layer vs Attention</h2>
<p>Now let’s understand a simpler situation that’s kinda analogous to self-attention- you simply multiply the inputs with a matrix (in other words, pass through a linear layer).</p>
<p>Let’s say the input sequence is of 8 words, and the vocabulary is of 1000. Then, we will first map each word to a embedding vector of 32, which is going to be our embedding dimension. This is like the word embedding of each word. Here, T = 8, and C = 32. Hence, our input is a matrix of [8 x 1000], which will be projected to [8 x 32].</p>
<p>Assume that W is our weights matrix of size TxT. Each element of it is a weight that corresponds to every single word to every other word, including towards itself (the diagonal elements). We can simply multiply it with our embedding matrix like this → [T, T] x [T, 32] → [T, 32], this being our updated representation!</p>
<p>Viola, we have an updated representation that considers every single word for computing output! BUT this is <strong>NOT</strong> attention, because in this case the weights are fixed. What we saw here is</p>
<p><span class="math inline">\(y = A \cdot x\)</span></p>
<p>whereas attention is something like</p>
<p><span class="math inline">\(y = A(x) \cdot x\)</span></p>
<p>where the weights matrix changes depending on what the input words are! In this example, the weights are fixed, attention is simply this example but with dynamic weights. How do we implement such dynamic weights? We use the Q-K-V method.</p>
</section>
<section id="q-k-v-in-simple-terms" class="level2">
<h2 class="anchored" data-anchor-id="q-k-v-in-simple-terms">Q-K-V in Simple Terms</h2>
<p>There are three matrices Q, K, V. By multiplying them with our word embeddings, for each word we get three vectors q, k, v. Each word’s q &amp; k are used to create the weights matrix, which is multiplied with v to get the attention output for that word. Finally, v is passed through a linear layer to get the output of the single transformer block.</p>
<p>Think of an analogue where instead of words, we have six people - you, your grandma, your dog, your professor, your best friend, a random person. The first person, which is you, wants some advice (say, college admission). Here, query is a vector from you (what do I do?). All other people return keys to you (here’s what I say), and you compute a weights vector corresponding to your query and the keys. Finally you weigh the final decision (objectively, here’s the answer) based on these weights. Other people will similarly have their own questions. Not a perfect example, but good enough to get you started.</p>
</section>
<section id="q-k-v-matrices" class="level2">
<h2 class="anchored" data-anchor-id="q-k-v-matrices">Q-K-V Matrices</h2>
<p>Doing the q-k-v computation for each word will be extremely inefficient - we can use matrix multiplication to speedrun them.</p>
<section id="start-with-an-embedding-for-each-word" class="level3">
<h3 class="anchored" data-anchor-id="start-with-an-embedding-for-each-word">Start with an Embedding for Each Word</h3>
<p>We have T words as input, since that’s our time dimension/context size. Our total vocabulary is of size V. Hence our input is [T, V], each vector as a one-hot vector for that word.</p>
<p>Using a linear layer, we’ll project this vector [T, V] to [T, C]. Here, C is called our embedding dimension. This matrix <strong>[T, C]</strong> will go inside a transformer block.</p>
</section>
<section id="get-the-q-k-v-vectors-for-all-words" class="level3">
<h3 class="anchored" data-anchor-id="get-the-q-k-v-vectors-for-all-words">Get the Q, K, V vectors for All Words</h3>
<p>We define three matrices <span class="math inline">\(W_q\)</span>, <span class="math inline">\(W_k\)</span>, <span class="math inline">\(W_v\)</span>, each of size [C, 16]. Here, 16 is called the head size denoted by <span class="math inline">\(d\)</span>, and is smaller than embedding dimension by choice. Reminder - C is the embedding dimension, this size of vector is going to come in for each word, and be output for each word. In previous example, we had defined C=32. It can be 512 (USE) or 768 (BERT) or 12268 (in case of GPT)</p>
<p>To get the query vectors for all words simply multiply the embedding matrix by <span class="math inline">\(W_q\)</span>.</p>
<p><span class="math inline">\(q = E \cdot W_q\)</span> which in matrix form → [T, C] x [C, 16] → [T, 16]. Likewise for key and value vectors.</p>
<p><span class="math inline">\(k = E \cdot W_k\)</span> which in matrix form → [T, C] x [C, 16] → [T, 16]</p>
<p><span class="math inline">\(v = E \cdot W_v\)</span> which in matrix form → [T, C] x [C, 16] → [T, 16]</p>
</section>
<section id="compute-the-weights-from-q-k-and-multiply-them-with-v" class="level3">
<h3 class="anchored" data-anchor-id="compute-the-weights-from-q-k-and-multiply-them-with-v">Compute the ‘Weights’ from Q, K and Multiply Them with V</h3>
<p>To compute the final representation after attention, First multiply q and k vectors to get the weights matrix, but you will have to transpose k first to multiply them.</p>
<p><span class="math inline">\(w = (q \cdot k^T)\)</span>, In matrix shapes, that is: [T, 16] x [16, T] → [T, T]</p>
<p>You apply softmax multiply these weights with the v vector to get your attention output</p>
<p><span class="math inline">\(output = softmax(w) \cdot v\)</span></p>
<p>In matrix shapes that is [T, T] x [T, 16] → <strong>[T, 16]</strong></p>
<p>Putting all together:</p>
<p><span class="math inline">\(attention(q, k, v) = (\frac{1}{\sqrt{d_k}})softmax(q \cdot k^T) \cdot v\)</span></p>
<p>The <span class="math inline">\(\sqrt{d_k}\)</span> division is to normalize the outputs.</p>
</section>
<section id="finally-send-this-to-a-dense-layer" class="level3">
<h3 class="anchored" data-anchor-id="finally-send-this-to-a-dense-layer">Finally, Send This to a Dense Layer</h3>
<p>After self-attention, the output is then again sent to one dense layer, which will allow the model to think deeper about the computed representations after the attention. Output of this layer is then given as input to the next transformer block- repeated n times till defined. This layer is called Feed-Forward Neural Network or FFNN.</p>
<p>[T, 16] → <strong>[T, 32]</strong></p>
<p>This output now becomes input to the next encoder block.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/qkv.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Q-K-V Computation</figcaption><p></p>
</figure>
</div>
<p>In case the computation is easier to understand in code, here’s some pytorch code from Andrej Karpathy’s video:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">32</span> <span class="co"># batch, time, channels</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># input </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># let's see a single Head perform self-attention</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># create Q, K, V layers</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the Q, K vectors for each word</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> key(x)   <span class="co"># (B, T, 16)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> query(x) <span class="co"># (B, T, 16)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the weights using Q &amp; K</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>wei <span class="op">=</span>  q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># softmax+normalize the weights matrix</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co"># tril = torch.tril(torch.ones(T, T))</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># wei = wei.masked_fill(tril == 0, float('-inf')) # this line is for a decoder block</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> value(x)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> wei <span class="op">@</span> v</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co">#out = wei @ x</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>out.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="transformer-components" class="level2">
<h2 class="anchored" data-anchor-id="transformer-components">Transformer Components</h2>
<p>With self-attention out of the way, let’s move on to the entire transformer tower.</p>
<section id="types-of-transformers" class="level3">
<h3 class="anchored" data-anchor-id="types-of-transformers">Types of Transformers</h3>
<p>There can two be components - Encoder, and a Decoder. This is a sequence-to-sequence model, used for tasks like machine translation (which was the topic of intereset of the original transformer paper). However, there can be encoder-only transformers geared towards text understanding and applications like sentiment analysis, named entity recognition, etc. BERT is an example of encoder-model. There can also be decoder-only models like GPT which are geared towards production of text given a small input.</p>
</section>
<section id="components" class="level3">
<h3 class="anchored" data-anchor-id="components">Components</h3>
<p>Encoding component is actually six encoders, stacked on top of each other. They have the same structure, but they do not share weights. Each encoder has two parts - Self-Attention and Feed-Forward Neural Network. The FFNN weights are same for all words, for a given encoder.</p>
<p>Decoding Component too has these two components, but an extra one in the middle - Encoder-Decoder attention to focus on the encodings. For an encoder-decoder attention block, the K&amp;V matrices come from the last encoder block, while Q matrices come from itself.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/enc_dec.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Encoder-Decoder</figcaption><p></p>
</figure>
</div>
</section>
<section id="multi-headed-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-headed-attention">Multi-Headed Attention</h3>
<p>We discussed self-attention, however skipped an important aspect of transformers that is multi-headed attention.</p>
<p>Let’s go back to the same example. The word <code>it</code> is correctly identified by the attention outputs to be related to <code>animal</code>. But there can be multiple contact points of information and interaction in the same sentence! For example the word <code>tired</code> is also related to <code>animal</code> but this relationship is not capture in one self-attention.</p>
<p>This can be remedied by having multiple attention heads! This means that for a single transformer block, we’ll have multiple sets of Q, K, V matrices instead of one. And before passing to the Feed Forward Neural Network, we’ll concatenate their outputs to aggregate their information.</p>
<p>This helps us capture multi-tiered information, like shown in this example:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/att_example2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Attention With Two Heads</figcaption><p></p>
</figure>
</div>
</section>
<section id="position-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="position-embeddings">Position Embeddings</h3>
<p>So far in self-attention, we have not considered the <em>order</em> or the <em>arrangement</em> of the words in the sentence at all. The model simply looks around for attention irrespective of the query’s position. However positional information is important (obviously), so it needs to be incorporated somehow.</p>
<p>We create one more embedding of size [T, C]. Here, T is the number of words in a sentence, and C is the embedding dimension of the words. We simply add this embedding to the word / token embedding before computing the attention.</p>
</section>
<section id="residual-connections-and-layernorm" class="level3">
<h3 class="anchored" data-anchor-id="residual-connections-and-layernorm">Residual Connections and LayerNorm</h3>
<p>Another component in the transformer architecture is the residual connections and Layer Norm.</p>
<p>Layer Norm is different from Batch Normalization, in that latter normalizes using the properties of the entire batch, whereas former normalizes using the properties of the embedding dimension.</p>
<p>After adding all these components, we are able to see the diagram from the paper</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/arxiv.PNG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Diagram from the Paper</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="decoder" class="level2">
<h2 class="anchored" data-anchor-id="decoder">Decoder</h2>
<p>The decoder components are exactly the same as encoders - except for one change. Between self-attention and FFN, there’s an additional component called cross-attention. In this attention stage, the K and V matrices come from the encoder, while the Q matrices come from the decoder itself. This helps to aggregate the encoded information while decoding the sequence.</p>
<p>There’s one more difference with Decoder- while computing the attention, the <em>future</em> words are not attended to. That is, while decoding the first word, you cannot get any information from rest of the words. For first two words, no info from the third and onwards. This is implemented using by masking the weights matrix using a lower triangular matrix.</p>
<p>Decoders, like Encoders, are also stacked one above the another, and the outputs keep bubbling up till the last decoder layer. After that, there’s a linear layer that transforms the embeddings into probabilities across the entire vocabulary. Assume the model’s vocabulary is 10,000. Then for each output word there will be 10,000 probabilities. You can pick the highest probability and call it as your result, and discard the rest. This is called greedy search.</p>
</section>
<section id="beam-search" class="level2">
<h2 class="anchored" data-anchor-id="beam-search">Beam-Search</h2>
<p>You can pick the highest probability and call it as your result, and discard the rest. This is called greedy search.</p>
<p>Or, for the first word, you can keep the top two words, “I” and “He”. For the next word, you can run the model twice - one assuming the first word “I” was correct, and the second time assuming “He” was correct. Whichever produced less error considering the two words predicted, is kept. This is called beam search. In our case, beam size was 2- it can be bigger.</p>
</section>
<section id="links" class="level2">
<h2 class="anchored" data-anchor-id="links">Links</h2>
<p>This was an overview of the transformer architecture. A lot of the things here are from either Jay Alammar’s incredibly beautiful blogpost on transformers, or from Andrej Karpathy 2-hour video on implementing GPT from scratch. Both are fantastic resources, do check them out as well. And of course, the original “Attention” paper</p>
<ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar - The Illustrated Transformer</a></li>
<li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Andrej Karpathy - Let’s build GPT: from scratch, in code, spelled out.</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>