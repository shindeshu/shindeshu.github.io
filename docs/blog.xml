<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Shubham Shinde</title>
<link>https://shindeshu.github.io/blog.html</link>
<atom:link href="https://shindeshu.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description>Blog for data science related topics</description>
<generator>quarto-1.1.251</generator>
<lastBuildDate>Sat, 07 Oct 2023 18:30:00 GMT</lastBuildDate>
<item>
  <title>A Beginner’s Guide to Deploying Tensorflow Models on AWS Lambda (Free-Tier)</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/cloud/lambda.html</link>
  <description><![CDATA[ 



<section id="whats-this-post-about" class="level2">
<h2 class="anchored" data-anchor-id="whats-this-post-about">What’s This Post About</h2>
<p>If you are a data scientist, have no idea about cloud, and want to do something hands-on instead of courses- you are in the right place.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/cloud/assets/coverart.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Generated Using SDXL</figcaption><p></p>
</figure>
</div>
<p>In this blogpost, we will signup on AWS Free-tier account and use the Lambda service to deploy a trained ML model within the free-tier limits. The code used in this post is available <a href="https://github.com/shindeshu/aws_lambda_tflite">in this repo</a>.</p>
<p>Pre-requisites:</p>
<ol type="1">
<li><p>You should be able to know how to train a machine learning model. Here, we are taking an already trained keras model and start from that point. It is a CNN model that classifies given image into either <code>cat</code> or <code>dog</code>. But if you are interested in the model training aspect, check out <a href="https://www.kaggle.com/code/shindeshubham85/a-friendly-introduction-to-cnns-in-keras">this kaggle notebook</a>!</p></li>
<li><p>You should be familiar with basics of Docker and REST API.</p></li>
</ol>
<p>The contents of this blogpost:</p>
<ul>
<li>Take the trained Keras Model, and convert to tf-lite format (Why? Discussed later.)</li>
<li>Write and test the inference functions for tf-lite model.</li>
<li>Create a docker image using an AWS Lambda image as base, and test it out.</li>
<li>Signup for an AWS Free-tier account. Configure the AWS CLI and ECR (Discussed later.)</li>
<li>Create an AWS Lambda function using the docker image. Test it out.</li>
<li>Create an API Gateway and expose it. That is it!</li>
</ul>
<p>A visual depiction of the flow:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/cloud/assets/flow.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Flow</figcaption><p></p>
</figure>
</div>
<p>This process is taken from the chapter 9 of the <a href="https://www.youtube.com/watch?v=MqI8vt3-cag&amp;list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR">free course “ML Zoomcamp”</a> by Alexey Grigorev, with some modifications. Follow the videos if you want a detailed walkthrough.</p>
<section id="what-is-aws-lambda-serverless" class="level3">
<h3 class="anchored" data-anchor-id="what-is-aws-lambda-serverless">What is AWS Lambda / Serverless</h3>
<p>AWS Lambda is a compute service that lets you run code without provisioning or managing servers.</p>
<p>All it asks is for you to bring the code- and the service will do everything else- provisioning servers, managing them, auto-scaling them if the demand increases, and shutting them off if they are not in use, or restarting them. Since we don’t need to worry at all about servers, it is also called serverless.</p>
<p>In the ML scenario, we will bring the code and the model to the service, and it will produce the predictions of our inputs. Instead of giving the code and the model directly, we will first package them inside a docker image, and use the image to deploy on Lambda.</p>
<p>At the end of this post, we can give a URL of an image to a REST API endpoint, and get back the image classification result.</p>
</section>
</section>
<section id="conversion-to-tf-lite" class="level2">
<h2 class="anchored" data-anchor-id="conversion-to-tf-lite">Conversion to TF-Lite</h2>
<section id="why-tf-lite" class="level3">
<h3 class="anchored" data-anchor-id="why-tf-lite">Why TF-Lite</h3>
<p>Our starting point is a file called <code>cats_and_dogs.h5</code> which is a keras model trained to classify given image into cat or dog. We can easily load it into our environment using the keras library.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> tensorflow</span>
<span id="cb1-2">model <span class="op" style="color: #5E5E5E;">=</span> tensorflow.keras.models.load_model(<span class="st" style="color: #20794D;">"cats_and_dogs.h5"</span>)</span></code></pre></div>
<p>However, this may not be the best case for deploying the model. Installation of tensorflow itself can take up a LOT of memory. Importing the library also adds latency. We only need to do inference, while the library contains tools for many operations like training, etc. AWS Lambda does not support GPUs, so we don’t those sections of codebase either.</p>
<p>Other constraints are the free-tier limits of AWS: Since we are deploying in the cloud, the sizes of our docker image need to be under 500MB.</p>
<p>One way to remove the boilerplate from the libraries is to convert the model to tf-lite and use the tf-lite library, which we will be employing here.</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that AWS Lambda only supports CPUs, not GPUs. So we need to deploy models only if they can perform well on CPUs.</p>
<p>While TF-Lite supports GPUs, if your deployment is in a GPU-environment, you might be better off with other well-supported compilers like TensorRT.</p>
</div>
</div>
</section>
<section id="conversion" class="level3">
<h3 class="anchored" data-anchor-id="conversion">Conversion</h3>
<p>The code for conversion is pretty straightforward:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">import</span> tensorflow.lite <span class="im" style="color: #00769E;">as</span> tflite </span>
<span id="cb2-2">converter <span class="op" style="color: #5E5E5E;">=</span> tflite.TFLiteConverter.from_keras_model(model)</span>
<span id="cb2-3">tflite_model <span class="op" style="color: #5E5E5E;">=</span> converter.convert()</span>
<span id="cb2-4"></span>
<span id="cb2-5"><span class="cf" style="color: #003B4F;">with</span> tf.io.gfile.GFile(<span class="st" style="color: #20794D;">'artifacts/cats_and_dogs.tflite'</span>, <span class="st" style="color: #20794D;">'wb'</span>) <span class="im" style="color: #00769E;">as</span> f:</span>
<span id="cb2-6">    f.write(tflite_model)</span></code></pre></div>
<p>The tricky part is feeding the input and retrieving the output. Fortunately its all wrapped up neat here:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;">def</span> load_tflite():</span>
<span id="cb3-2">    <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;">    this function is used to load the tflite model and initialize the </span></span>
<span id="cb3-4"><span class="co" style="color: #5E5E5E;">    interpreter, and also return the input and output indexes</span></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;">    """</span></span>
<span id="cb3-6">    interpreter <span class="op" style="color: #5E5E5E;">=</span> tflite.Interpreter(model_path<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'cats_and_dogs.tflite'</span>)</span>
<span id="cb3-7">    interpreter.allocate_tensors()</span>
<span id="cb3-8">    input_details <span class="op" style="color: #5E5E5E;">=</span> interpreter.get_input_details()</span>
<span id="cb3-9">    input_index <span class="op" style="color: #5E5E5E;">=</span> input_details[<span class="dv" style="color: #AD0000;">0</span>][<span class="st" style="color: #20794D;">'index'</span>]</span>
<span id="cb3-10">    output_details <span class="op" style="color: #5E5E5E;">=</span> interpreter.get_output_details()</span>
<span id="cb3-11">    output_index <span class="op" style="color: #5E5E5E;">=</span> output_details[<span class="dv" style="color: #AD0000;">0</span>][<span class="st" style="color: #20794D;">'index'</span>]</span>
<span id="cb3-12">    <span class="cf" style="color: #003B4F;">return</span> interpreter, (input_index, output_index)</span>
<span id="cb3-13"></span>
<span id="cb3-14"><span class="kw" style="color: #003B4F;">def</span> predict_with_tflite(interpreter, indexes, img):</span>
<span id="cb3-15">    <span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb3-16"><span class="co" style="color: #5E5E5E;">    this function takes the interpreter, indexes and input image as input</span></span>
<span id="cb3-17"><span class="co" style="color: #5E5E5E;">    performs the inference, does some postprocessing and returns the result.</span></span>
<span id="cb3-18"><span class="co" style="color: #5E5E5E;">    """</span></span>
<span id="cb3-19">    interpreter.set_tensor(indexes[<span class="dv" style="color: #AD0000;">0</span>], img)</span>
<span id="cb3-20">    interpreter.invoke()</span>
<span id="cb3-21">    preds <span class="op" style="color: #5E5E5E;">=</span> interpreter.get_tensor(indexes[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb3-22">    <span class="cf" style="color: #003B4F;">return</span> postprocess(preds)</span></code></pre></div>
<p>Another tricky part is the pre-processing function. During training the model, we simply used the <code>keras.preprocessing</code> module to perform the pre-processing. Since we are not installing keras or tensorflow, how can we do the pre-processing?</p>
<p>Its easy, we do it ourselves. We load the image using PIL library, convert to numpy array, and scale it. The dependencies are light: PIL, and NumPy.</p>
<p>Now, we have the complete code for inference (<a href="https://github.com/shindeshu/aws_lambda_tflite/tree/main">check the github repo</a>)- including the pre-processing steps, and prediction with the tf-lite model.</p>
</section>
<section id="the-docker-image-for-lambda-function" class="level3">
<h3 class="anchored" data-anchor-id="the-docker-image-for-lambda-function">The Docker Image for Lambda Function</h3>
<p>Now, we will be creating a docker image to be used in lambda function. (Don’t worry about the AWS Account yet.)</p>
<p>While preparing a docker image we generally start with a “base image”. This often is based on Ubuntu. But this will not work here. We need to use the official AWS Lambda as base image which is based on CentOS and includes other artifacts related to Lambda. We will prepare our image from this base image, and upload to AWS Lambda for deployment.</p>
<p>How will we prepare the dockerfile?</p>
<ol type="1">
<li>Select the appropriate base image</li>
<li>Install the dependencies</li>
<li>Copy the lambda handler file</li>
<li>Copy the artifacts (model file etc.)</li>
<li>Specify the CMD command</li>
</ol>
<section id="creating-the-lambda-handler-file" class="level4">
<h4 class="anchored" data-anchor-id="creating-the-lambda-handler-file">Creating the Lambda Handler File</h4>
<p>Before we start preparing the Dockerfile, we need to write a python script <code>lambda_function.py</code> that will perform all the pre-processing steps, load the model and do prediction. This is going to be the entrypoint script for the Lambda function.</p>
<p>This script should have a function called <code>lambda_handler</code> that should take the input from API and return result as json.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;">def</span> lambda_handler(event, context):</span>
<span id="cb4-2">    url <span class="op" style="color: #5E5E5E;">=</span> event[<span class="st" style="color: #20794D;">'url'</span>]</span>
<span id="cb4-3">    X <span class="op" style="color: #5E5E5E;">=</span> load_image(url)</span>
<span id="cb4-4">    result <span class="op" style="color: #5E5E5E;">=</span> predict_with_tflite(X)</span>
<span id="cb4-5">    <span class="cf" style="color: #003B4F;">return</span> result</span></code></pre></div>
<p>Here, <code>load_image</code> contains the preprocessing logic and <code>predict_with_tflite</code> contains the prediction steps. Check the complete <code>lambda_function.py</code> file in the github repo.</p>
</section>
<section id="resolving-some-errors" class="level4">
<h4 class="anchored" data-anchor-id="resolving-some-errors">Resolving Some Errors</h4>
<p>While installing the dependencies, you might come across some errors. This is related to the library <code>tf-lite</code>, the PyPI binaries are based on Ubuntu/Debian OS and not supported for CentOS. Hence, you will have to install this library using wheel files compiled for that OS- which are fortunately available for us on github, <a href="https://github.com/alexeygrigorev/tflite-aws-lambda">compiled by Alexey Grigorev</a>. A copy of the file is also present in the repo we are working from.</p>
</section>
</section>
<section id="build-the-docker-image" class="level3">
<h3 class="anchored" data-anchor-id="build-the-docker-image">Build the Docker Image</h3>
<p>Once the required code and model files are ready, we start building the docker image. The sequence of steps is simple:</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode dockerfile code-with-copy"><code class="sourceCode dockerfile"><span id="cb5-1"><span class="kw" style="color: #003B4F;">FROM</span> public.ecr.aws/lambda/python:3.8</span>
<span id="cb5-2"></span>
<span id="cb5-3"><span class="kw" style="color: #003B4F;">COPY</span> artifacts/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl .</span>
<span id="cb5-4"></span>
<span id="cb5-5"><span class="kw" style="color: #003B4F;">RUN</span> <span class="ex" style="color: null;">pip3</span> install tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl <span class="at" style="color: #657422;">--no-cache-dir</span></span>
<span id="cb5-6"><span class="kw" style="color: #003B4F;">RUN</span> <span class="ex" style="color: null;">pip</span> install requests Pillow</span>
<span id="cb5-7"></span>
<span id="cb5-8"><span class="kw" style="color: #003B4F;">COPY</span> artifacts/cats_and_dogs.tflite .</span>
<span id="cb5-9"><span class="kw" style="color: #003B4F;">COPY</span> artifacts/lambda_function.py .</span>
<span id="cb5-10"></span>
<span id="cb5-11"><span class="kw" style="color: #003B4F;">CMD</span> [<span class="st" style="color: #20794D;">"lambda_function.lambda_handler"</span>]</span></code></pre></div>
<p>We build the image using</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><span class="ex" style="color: null;">docker</span> build <span class="at" style="color: #657422;">-t</span> cats_and_dogs .</span></code></pre></div>
<p>And start running the container using</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><span class="ex" style="color: null;">docker</span> run <span class="at" style="color: #657422;">-it</span> <span class="at" style="color: #657422;">--rm</span> cats_and_dogs:latest <span class="at" style="color: #657422;">-p</span> 8080:8080 </span></code></pre></div>
</section>
<section id="testing-the-docker-application" class="level3">
<h3 class="anchored" data-anchor-id="testing-the-docker-application">Testing the Docker Application</h3>
<p>Now, the docker container is running, and presumably our model within is running as well. But how do we use it or test it if it’s working?</p>
<p>We send a request using the python <code>requests</code> library. If you go to our lambda handler script, the <code>event</code> argument is basically the json that we are sending over using a POST request. We retrieve the URL from the json payload, process it to get the prediction, and return the result as json response. Let’s look at the code:</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;">import</span> requests</span>
<span id="cb8-2"></span>
<span id="cb8-3">url <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"http://localhost:8080/2015-03-31/functions/function/invocations"</span> </span>
<span id="cb8-4"><span class="co" style="color: #5E5E5E;"># this is how the URL for AWS Lambda always looks like</span></span>
<span id="cb8-5"><span class="co" style="color: #5E5E5E;"># this is the URL where the application inside docker container is listening for requests</span></span>
<span id="cb8-6"></span>
<span id="cb8-7">data <span class="op" style="color: #5E5E5E;">=</span> {<span class="st" style="color: #20794D;">"url"</span>: <span class="st" style="color: #20794D;">"https://t4.ftcdn.net/jpg/00/97/58/97/360_F_97589769_t45CqXyzjz0KXwoBZT9PRaWGHRk5hQqQ.jpg"</span>}</span>
<span id="cb8-8"><span class="co" style="color: #5E5E5E;"># this is the input we want to give to the lambda_handler script</span></span>
<span id="cb8-9"></span>
<span id="cb8-10">results <span class="op" style="color: #5E5E5E;">=</span> requests.post(url, json<span class="op" style="color: #5E5E5E;">=</span>data).json()</span>
<span id="cb8-11"><span class="co" style="color: #5E5E5E;"># this is the result after prediction and post-processing.</span></span>
<span id="cb8-12"></span>
<span id="cb8-13"><span class="bu" style="color: null;">print</span>(results)</span></code></pre></div>
<p>If you are seeing a dictionary printed that contains the probabilities of the model classes, the docker container is running successfully! Congratulations - all our offline work is complete. All that’s left is signing up and moving this image to cloud.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that we did not install any client like FastAPI/Flask, yet we are able to listen to a REST API. The base docker image likely has a built-in server.</p>
</div>
</div>
</section>
</section>
<section id="setup-your-free-tier-aws-account" class="level2">
<h2 class="anchored" data-anchor-id="setup-your-free-tier-aws-account">Setup your Free-tier AWS Account</h2>
<section id="signup-for-a-free-tier-account" class="level3">
<h3 class="anchored" data-anchor-id="signup-for-a-free-tier-account">Signup for a Free-tier Account</h3>
<p>If you don’t have any experience in handling cloud, and want to learn- AWS Free-tier is a good option. But it can also be dreaded option, since some might be afraid of racking up large costs accidently. However if you understand the limits, it’s very easy to stay below the specified thresholds and still learn the nuts-and-bolts of AWS using toy projects.</p>
<p>Before you start creating a free-tier account, it’s best if you go through which services are free and what are their limits- <a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a>. Also go through a <a href="https://www.youtube.com/watch?v=SFaSB6vgp8k">video example of setting up an account</a> so there’s less surprises.</p>
<p>Once the setup is complete, there’s still more steps to do for configuring your workspace.</p>
</section>
<section id="creating-an-iam-user" class="level3">
<h3 class="anchored" data-anchor-id="creating-an-iam-user">Creating an IAM User</h3>
<p>Once you have the account set up, you will have one user called ‘root’. However it’s a best practice to create custom users called “IAM Users” for development activity. Create one such user with admin access, and create an access key for this user. We will do our development through this access key.</p>
</section>
<section id="setup-aws-cli" class="level3">
<h3 class="anchored" data-anchor-id="setup-aws-cli">Setup AWS-CLI</h3>
<p>We can use AWS through our browser and the UI, but the best Developer Experience is through AWS Command Line Interface. This allows us to work from the terminal. Setting it up is easy,</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><span class="ex" style="color: null;">pip</span> install awscli</span></code></pre></div>
<p>Configure it using <code>aws configure</code> and input the access ID and secret access keys you generated earlier. Input a default region (e.g.&nbsp;us-east-1, ap-south-1) and default format (e.g.&nbsp;json).</p>
</section>
<section id="create-and-link-amazon-ecr" class="level3">
<h3 class="anchored" data-anchor-id="create-and-link-amazon-ecr">Create and Link Amazon ECR</h3>
<p>ECR is Amazon’s container registry, where we can upload docker images. These images we can then use for various applications like AWS Lambda or AWS EKS etc. We will setup ECR and connect it our docker client (Docker Desktop), so that we can upload docker images from docker to ECR.</p>
<ol type="1">
<li>We first create an ECR Repo, and <em>note down the URI</em></li>
</ol>
<p><code>aws ecr create-repository --repository-name cats_and_dogs_images</code></p>
<p>You can go to the AWS dashboard in your browser and check if the repository has been created.</p>
<ol start="2" type="1">
<li>Authenticate docker to work with ECR.</li>
</ol>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><span class="ex" style="color: null;">$</span> <span class="va" style="color: #111111;">$(</span><span class="ex" style="color: null;">aws</span> ecr get-login<span class="va" style="color: #111111;">)</span></span></code></pre></div>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>For windows, Step2 might result in an error. <a href="https://stackoverflow.com/a/65858904">This solution from stackoverflow</a> worked for me.</p>
</div>
</div>
<p>Following these steps, Docker is successfully authenticated with AWS-ECR.</p>
</section>
<section id="upload-the-docker-image-to-ecr-repo" class="level3">
<h3 class="anchored" data-anchor-id="upload-the-docker-image-to-ecr-repo">Upload the Docker Image to ECR Repo</h3>
<ol type="1">
<li>Tag the image of interest with the remote repository URI</li>
</ol>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><span class="ex" style="color: null;">docker</span> tag cats_and_dogs:latest <span class="va" style="color: #111111;">${REMOTE_URI}</span></span></code></pre></div>
<ol start="2" type="1">
<li>Push to ECR</li>
</ol>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><span class="ex" style="color: null;">docker</span> push</span></code></pre></div>
<p>Check in browser if the image has been published.</p>
</section>
</section>
<section id="creating-the-lambda-function" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-lambda-function">Creating the Lambda Function</h2>
<p>Now, your AWS Account is set up and you’ve also uploaded the docker image to ECR. In this section we will use the image to create a lambda function, and also set an API trigger using API Gateway.</p>
<p>Creating a Lambda function is easy, from the AWS dashboard we go to AWS Lambda -&gt; Create Function -&gt; From Image -&gt; Select the Uploaded image.</p>
<p><img src="https://shindeshu.github.io/posts/cloud/assets/create.png" class="img-fluid"></p>
<section id="test-the-image" class="level3">
<h3 class="anchored" data-anchor-id="test-the-image">Test the Image</h3>
<p>By default the timeout is 3s, which we need to increase to 30s in the settings. This is because the model loading for the first time may take upto 7s. We can test the lambda function by giving the json payload in the test area.</p>
<p>If you get the familiar output of classes and probabilities, your Lambda function is working!</p>
<p>However, currently you can only give the input through the test section. How will the users access the function? How will they connect to Lambda? We can use Amazon’s API Gateway for that.</p>
</section>
<section id="create-a-api-gateway" class="level3">
<h3 class="anchored" data-anchor-id="create-a-api-gateway">Create a API Gateway</h3>
<p>From Dashboard, go to API Gateway -&gt; REST API -&gt; new resource. Here you define the endpoint name. Then, define the request type (i.e.&nbsp;POST) In POST, integration type should be changed to Lambda function. Give the region name and name of the lambda function we created earlier.</p>
<p>So you have now exposed the Lambda function using a REST API! You can test it there itself in the request body section.</p>
<p>But this API Endpoint is still private- we can open it up to other users by going to Actions -&gt; Deploy API. We copy the URL, and test it using python requests library.</p>
<p>And this is how we deploy an ML model using AWS Lambda!</p>


</section>
</section>

 ]]></description>
  <category>data-science</category>
  <category>deep-learning</category>
  <category>cloud</category>
  <guid>https://shindeshu.github.io/posts/cloud/lambda.html</guid>
  <pubDate>Sat, 07 Oct 2023 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/cloud/assets/coverart.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>How Uber Used GNNs to Detect Fraud</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/gnns/uber_graphs.html</link>
  <description><![CDATA[ 



<section id="fraud-and-ml" class="level2">
<h2 class="anchored" data-anchor-id="fraud-and-ml">Fraud and ML</h2>
<p>Fraud remains a persistent concern in various industries within the digital realm. Because fraud can take the shape of many different types depending on the industry, it becomes one of the most complex use-cases of machine learning. Healthcare fraud, for instance, involves doctors colluding with testing centers to assign more tests that they should (among other types). Financial fraud is another quagmire that keeps constantly evolving.</p>
<p>Uber, considering the volumes of financial transactions, attracts its fair share of fraudsters and hence they have built advanced fraud detection infrastructure. In this blog post, we’ll look at one of the experiments they did with Graph Neural Networks, outline in this Uber <a href="https://www.uber.com/en-IN/blog/fraud-detection/">blog</a>. Yes, this is a blog post about a blog post.</p>
<p><img src="https://shindeshu.github.io/posts/gnns/assets/uber_photo.jpg" class="img-fluid"></p>
</section>
<section id="fraud-at-uber" class="level2">
<h2 class="anchored" data-anchor-id="fraud-at-uber">Fraud at Uber</h2>
<p>What kind of fraud happens at Uber? There are roughly two types, according another <a href="https://www.uber.com/en-IN/blog/project-radar-intelligent-early-fraud-detection/">post</a> on Uber.</p>
<p>Following are the 2 primary payment fraud types:</p>
<ul>
<li>DNS stands for “do not settle”, which occurs when the order has been fulfilled or a trip has been completed, but the payment could not be collected</li>
<li>Chargeback happens when the payment has been collected, but the user disputes the transaction, and the bank refunds the money</li>
</ul>
<p>The blog post we are discussing primarily deals with chargebacks, however it does not provide details on its mechanisms. The authors claim that “collusion” among fraud users is what they are targeting, and also make the claim that fraudulent users are connected and clustered (although they offer no explanation or evidence of the same).</p>
</section>
<section id="labeling-a-fraud" class="level2">
<h2 class="anchored" data-anchor-id="labeling-a-fraud">Labeling a Fraud</h2>
<p>Data scientists often have ready-made labels for them to train data on, if not they are easily extractable using some rules. In fraud, however, using simple rules to flag fraud is not feasible. It almost always needs domain experts to distinguish fraudulent activity from normal. Finding fraudulent transactions to flag is a full-time job with judgement calls.</p>
<p>Risk analysts, for this reason, are an integral part of the risk infrastructure even with all the fancy ML algorithms. While models can catch previously seen fraud patterns, they can struggle to catch new/improved fraud patterns. Hence, a combination of rules-based and model-based approach are usually used.</p>
<p>However, the current blog post is only concerned about the model-based section, and for labelling, it uses the simple flag of a chargeback in a given time range. The authors train a graph neural network to output risk scores for each user, and this score itself is fed into another larger model as a feature for fraud detection (Uber has many models, rules, checkpoints to detect fraud).</p>
</section>
<section id="what-are-graph-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="what-are-graph-neural-networks">What Are Graph Neural Networks</h2>
<p><img src="https://shindeshu.github.io/posts/gnns/assets/uber_network.jpg" width="600"></p>
<p>GNNs are a type of architecture that is able to model non-euclidean data, or networked data. Think of a social media graph- where each person has their own features (age, likes) and they also have links to other users (friends, following). Using a GNN, we can model user behavior using their own features as well the features of their linked users (network).</p>
<p>We construct a graph made of nodes and edges. Each node is an entity of our interest, like a user. Each node will have their features (age, likes, etc.). For one node, we will use their own features plus the aggregated features of their neighbors to compute a new representation (embedding) using neural networks. This is one layer of a GNN. We can repeat the process for more layers.</p>
<p>The final representation (embedding) of each node, now incorporates in itself the community information of that node as well as its own information. This embedding can hence be used for downstream tasks to classify the user (into fraud vs non-fraud, or friend suggestions)</p>
<p>The blog post we discuss uses the RGCN architecture, which is a modification to allow different types of nodes in a single graph.</p>
</section>
<section id="data-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="data-and-evaluation">Data and Evaluation</h2>
<p>A single graph is constructed in-memory using drivers and riders. The label of fraud is whether a user has chargeback in a time range. The model outputs two scores (why?): rider risk score, and driver risk score.</p>
<p>The model was trained on 4 months of data upto a specific date, and tested on the following 6 weeks. Now, they added the two risk scores to the primary production model (which I’m assuming to be a tree-based one), which gave them a precision boost of 15% with “minimal increase in false positives”. In the feature importance list, the two fraud scores were ranked 4th and 39th among 2000 features.</p>
</section>
<section id="data-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="data-pipeline">Data Pipeline</h2>
<p>When it comes to industrial applications of interesting algorithms, the most interesting part is not the algorithms per se but the data munging and the infrastructure build that is needed to put all the pieces in place. In the case of Graph Neural Networks, understanding the basic theory behind them is one thing, it is entirely different challenge to deploy them in production. Because GNNs work on networked data, and the data at companies is mostly stored in tabular format, it is a challenge to work them in this relational database paradigm. And this isn’t even the major infrastructural challenge.</p>
<p>The current application involves building a massive single graph containing riders and drivers, and their features and edges. This graph will certainly grow too big to fit in a single computer. How do you do distributed computing on a graph? For massive data that is tabular in nature, there exists solutions like Spark which are standard everywhere. How do you work around on a networked data that is massive? Let’s look at what Uber did.</p>
<p>First step that they have is to create user tables and relationship tables from their existing data. The user table contains the features for each user, and relationship tables would contain the edges between users.</p>
<section id="graph-partitioning" class="level3">
<h3 class="anchored" data-anchor-id="graph-partitioning">Graph Partitioning</h3>
<p>To deal with the issues arising from an extremely large graph, the team partitions the graph into n sub-graphs. They use Cypher query language for dealing with graphs, and the team augmented the language by adding a partition clause.</p>
<p>Among the recent users of Uber, they’ll pick some random n users as seed. For each user, they’ll compute the x-hop subgraph of that user, and will assign that to that partition. Hence, they will have n sub-graphs that can fit on n machines.</p>
<p>However, this also means that some users can be present on more than one sub-graph, and some users (especially the dormant ones) won’t be present in any sub-graph at all. It’s worth pondering if this scheme is losing important information from the users who are not active. The post does not mention what proportion of the original graph is completely untouched by training.</p>
<p><img src="https://shindeshu.github.io/posts/gnns/assets/uber_graph.png" width="600"></p>
</section>
</section>
<section id="future-directions" class="level2">
<h2 class="anchored" data-anchor-id="future-directions">Future Directions</h2>
<p>As the team itself comments, using better architectures than RGCN would be a low hanging fruit. There are architectures and libraries like DGL who now support distributed training and inference on massive graphs. Leveraging them would be a no-brainer, if one is to continue working in this direction.</p>
<p>Fraud is one of the more complex domains of data science, where there is no one-size-fits-all unlike say transformers in NLP. Hence, it may indeed yield dividends in sticking with GNNs. However the networked paradigm is somewhat different from your usual ML or Deep Learning training flows, hence it remains a steep learning curve.</p>
</section>
<section id="uber-risk-models" class="level2">
<h2 class="anchored" data-anchor-id="uber-risk-models">Uber Risk Models</h2>
<p>This model, according to the authors, was not deployed to production, despite yielding 15% better metrics. The reason was not mentioned. If one is to speculate, perhaps the data may not be comprehensive and the results not significant enough to warrant an addition of extra infrastructural complexity. Storing massive graphs and the distributed learning associated with them require more resources and a very distinct infrastructure that will cost to maintain.</p>
<p>So what does tools does Uber actually use for risk? They have wrote about something interesting called <a href="https://www.uber.com/en-IN/blog/project-radar-intelligent-early-fraud-detection/">Project RADAR</a>, a human-in-the-loop system that combines machine learning models with actual risk experts to catch early fraud. This system is able to capture new and evolved fraud patterns as well.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>We have seen an example of using Graph Neural Networks for fraud detection in an actual industrial setting. Even though the particular model did not get deployed to production, there’s still lot that one could learn from this exercise, and who knows, it may work out better for some other domain.</p>


</section>

 ]]></description>
  <category>deep-learning</category>
  <category>gnn</category>
  <category>applied-ml</category>
  <guid>https://shindeshu.github.io/posts/gnns/uber_graphs.html</guid>
  <pubDate>Thu, 09 Mar 2023 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/gnns/assets/uber_photo.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Understanding the Transformer</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/nlp/understanding_transformer.html</link>
  <description><![CDATA[ 



<section id="transformers---the-phenomenon-of-our-age" class="level2">
<h2 class="anchored" data-anchor-id="transformers---the-phenomenon-of-our-age">Transformers - the Phenomenon of Our Age</h2>
<p>It is not hyperbolic to call transformers as the greatest invention in last 5 years or so. With ChatGPT and Bing AI, the quality of machine generated text has shattered everyone’s projections about artificial intelligence. However, what’s fascinating that the underlying architecture that has enabled all these advancements in NLP is relatively quite simple and easy to understand. In this post, we’ll try to understand it.</p>
<p><strong>Contents of this Post:</strong> A brief comparison of transformers and sequential models, importance of attention, Q-K-V in simpler terms, and in detail, other components of transformers, decoder, beam-search.</p>
</section>
<section id="attention-vs-sequential-models" class="level2">
<h2 class="anchored" data-anchor-id="attention-vs-sequential-models">Attention vs Sequential Models</h2>
<p>Many people do have an understanding of RNNs / LSTMs, probably since they’ve been around for a long time, they’re a part of many a coursework, etc. And hence they try to understand transformers from that angle, as sort of a natural evolution. However transformers are extremely different from sequential models because they, well, are not sequential models.</p>
<p>In sequential models, we have units one after another, in a chain. Each unit/word has direct access to information of only the preceding unit/word. This is limiting - dependencies between words at different positions is not modelled well. For sequence-to-sequence prediction, the entirety of input is encoded in a single vector, and the decoder is supposed to decode everything from that. This causes information loss because all of it is crushed into a single vector. Transformer solves both of these issues and more.</p>
<p>For a silly comparison think of sequential models as the assassination chain meme, whereas an attention model as the mexican standoff meme. In attention, the mexican standoff has mutant people with dozens of arms pointing at everybody!</p>
<div id="silly-com" class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/nlp/assets/mexican_standoff.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Attention, a Mexican Standoff</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/nlp/assets/assa_chain_meme.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Sequential Assassination Chain</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="self-attention-is-translation-invariant" class="level2">
<h2 class="anchored" data-anchor-id="self-attention-is-translation-invariant">Self-Attention Is Translation Invariant</h2>
<p>Self-Attention is translation invariant by nature, which means even if you exchange few words, the output will be same. To incorporate the position information, we have a separate embedding called position embedding that we’ll look at later. But in itself, self-attention does not have a “sequence” among the input words. This is really powerful since - it allows each word in a sentence to be considered for computing output - it allows us to parallelize operations on GPUs, enabling us to build larger powerful models.</p>
</section>
<section id="why-attention-is-important" class="level2">
<h2 class="anchored" data-anchor-id="why-attention-is-important">Why Attention is Important</h2>
<p>Attention is the underlying mechanism that powers transformers. Let us see <em>why</em> it is necessary. Take this sentence as example</p>
<p><code>The animal didn't cross the street because it was too tired</code></p>
<p>What does the word “it” refer to? We know that it refers to “animal”, but the model should also be able to connect to the relevant word and use that information while predicting. Self-Attention allows this. Here, the attention weights visualized indeed show us that.</p>
<p><img src="https://shindeshu.github.io/posts/nlp/assets/att_example.png" class="img-fluid" style="width:70.0%"></p>
</section>
<section id="a-single-dense-layer-vs-attention" class="level2">
<h2 class="anchored" data-anchor-id="a-single-dense-layer-vs-attention">A Single Dense Layer vs Attention</h2>
<p>Now let’s understand a simpler situation that’s kinda analogous to self-attention- you simply multiply the inputs with a matrix (in other words, pass through a linear layer).</p>
<p>Let’s say the input sequence is of 8 words, and the vocabulary is of 1000. Then, we will first map each word to a embedding vector of 32, which is going to be our embedding dimension. This is like the word embedding of each word. Here, T = 8, and C = 32. Hence, our input is a matrix of [8 x 1000], which will be projected to [8 x 32].</p>
<p>Assume that W is our weights matrix of size TxT. Each element of it is a weight that corresponds to every single word to every other word, including towards itself (the diagonal elements). We can simply multiply it with our embedding matrix like this → [T, T] x [T, 32] → [T, 32], this being our updated representation!</p>
<p>Viola, we have an updated representation that considers every single word for computing output! BUT this is <strong>NOT</strong> attention, because in this case the weights are fixed. What we saw here is</p>
<p><img src="https://latex.codecogs.com/png.latex?y%20=%20A%20%5Ccdot%20x"></p>
<p>whereas attention is something like</p>
<p><img src="https://latex.codecogs.com/png.latex?y%20=%20A(x)%20%5Ccdot%20x"></p>
<p>where the weights matrix changes depending on what the input words are! In this example, the weights are fixed, attention is simply this example but with dynamic weights. How do we implement such dynamic weights? We use the Q-K-V method.</p>
</section>
<section id="q-k-v-in-simple-terms" class="level2">
<h2 class="anchored" data-anchor-id="q-k-v-in-simple-terms">Q-K-V in Simple Terms</h2>
<p>There are three matrices Q, K, V. By multiplying them with our word embeddings, for each word we get three vectors q, k, v. Each word’s q &amp; k are used to create the weights matrix, which is multiplied with v to get the attention output for that word. Finally, v is passed through a linear layer to get the output of the single transformer block.</p>
<p>Think of an analogue where instead of words, we have six people - you, your grandma, your dog, your professor, your best friend, a random person. The first person, which is you, wants some advice (say, college admission). Here, query is a vector from you (what do I do?). All other people return keys to you (here’s what I say), and you compute a weights vector corresponding to your query and the keys. Finally you weigh the final decision (objectively, here’s the answer) based on these weights. Other people will similarly have their own questions. Not a perfect example, but good enough to get you started.</p>
</section>
<section id="q-k-v-matrices" class="level2">
<h2 class="anchored" data-anchor-id="q-k-v-matrices">Q-K-V Matrices</h2>
<p>Doing the q-k-v computation for each word will be extremely inefficient - we can use matrix multiplication to speedrun them.</p>
<section id="start-with-an-embedding-for-each-word" class="level3">
<h3 class="anchored" data-anchor-id="start-with-an-embedding-for-each-word">Start with an Embedding for Each Word</h3>
<p>We have T words as input, since that’s our time dimension/context size. Our total vocabulary is of size V. Hence our input is [T, V], each vector as a one-hot vector for that word.</p>
<p>Using a linear layer, we’ll project this vector [T, V] to [T, C]. Here, C is called our embedding dimension. This matrix <strong>[T, C]</strong> will go inside a transformer block.</p>
</section>
<section id="get-the-q-k-v-vectors-for-all-words" class="level3">
<h3 class="anchored" data-anchor-id="get-the-q-k-v-vectors-for-all-words">Get the Q, K, V vectors for All Words</h3>
<p>We define three matrices <img src="https://latex.codecogs.com/png.latex?W_q">, <img src="https://latex.codecogs.com/png.latex?W_k">, <img src="https://latex.codecogs.com/png.latex?W_v">, each of size [C, 16]. Here, 16 is called the head size denoted by <img src="https://latex.codecogs.com/png.latex?d">, and is smaller than embedding dimension by choice. Reminder - C is the embedding dimension, this size of vector is going to come in for each word, and be output for each word. In previous example, we had defined C=32. It can be 512 (USE) or 768 (BERT) or 12268 (in case of GPT)</p>
<p>To get the query vectors for all words simply multiply the embedding matrix by <img src="https://latex.codecogs.com/png.latex?W_q">.</p>
<p><img src="https://latex.codecogs.com/png.latex?q%20=%20E%20%5Ccdot%20W_q"> which in matrix form → [T, C] x [C, 16] → [T, 16]. Likewise for key and value vectors.</p>
<p><img src="https://latex.codecogs.com/png.latex?k%20=%20E%20%5Ccdot%20W_k"> which in matrix form → [T, C] x [C, 16] → [T, 16]</p>
<p><img src="https://latex.codecogs.com/png.latex?v%20=%20E%20%5Ccdot%20W_v"> which in matrix form → [T, C] x [C, 16] → [T, 16]</p>
</section>
<section id="compute-the-weights-from-q-k-and-multiply-them-with-v" class="level3">
<h3 class="anchored" data-anchor-id="compute-the-weights-from-q-k-and-multiply-them-with-v">Compute the ‘Weights’ from Q, K and Multiply Them with V</h3>
<p>To compute the final representation after attention, First multiply q and k vectors to get the weights matrix, but you will have to transpose k first to multiply them.</p>
<p><img src="https://latex.codecogs.com/png.latex?w%20=%20(q%20%5Ccdot%20k%5ET)">, In matrix shapes, that is: [T, 16] x [16, T] → [T, T]</p>
<p>You apply softmax multiply these weights with the v vector to get your attention output</p>
<p><img src="https://latex.codecogs.com/png.latex?output%20=%20softmax(w)%20%5Ccdot%20v"></p>
<p>In matrix shapes that is [T, T] x [T, 16] → <strong>[T, 16]</strong></p>
<p>Putting all together:</p>
<p><img src="https://latex.codecogs.com/png.latex?attention(q,%20k,%20v)%20=%20(%5Cfrac%7B1%7D%7B%5Csqrt%7Bd_k%7D%7D)softmax(q%20%5Ccdot%20k%5ET)%20%5Ccdot%20v"></p>
<p>The <img src="https://latex.codecogs.com/png.latex?%5Csqrt%7Bd_k%7D"> division is to normalize the outputs.</p>
</section>
<section id="finally-send-this-to-a-dense-layer" class="level3">
<h3 class="anchored" data-anchor-id="finally-send-this-to-a-dense-layer">Finally, Send This to a Dense Layer</h3>
<p>After self-attention, the output is then again sent to one dense layer, which will allow the model to think deeper about the computed representations after the attention. Output of this layer is then given as input to the next transformer block- repeated n times till defined. This layer is called Feed-Forward Neural Network or FFNN.</p>
<p>[T, 16] → <strong>[T, 32]</strong></p>
<p>This output now becomes input to the next encoder block.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/nlp/assets/qkv.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Q-K-V Computation</figcaption><p></p>
</figure>
</div>
<p>In case the computation is easier to understand in code, here’s some pytorch code from Andrej Karpathy’s video:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">B,T,C <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">32</span> <span class="co" style="color: #5E5E5E;"># batch, time, channels</span></span>
<span id="cb1-2"><span class="co" style="color: #5E5E5E;"># input </span></span>
<span id="cb1-3">x <span class="op" style="color: #5E5E5E;">=</span> torch.randn(B,T,C)</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;"># let's see a single Head perform self-attention</span></span>
<span id="cb1-6">head_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">16</span></span>
<span id="cb1-7"></span>
<span id="cb1-8"><span class="co" style="color: #5E5E5E;"># create Q, K, V layers</span></span>
<span id="cb1-9">key <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(C, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb1-10">query <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(C, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb1-11">value <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(C, head_size, bias<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="co" style="color: #5E5E5E;"># compute the Q, K vectors for each word</span></span>
<span id="cb1-14">k <span class="op" style="color: #5E5E5E;">=</span> key(x)   <span class="co" style="color: #5E5E5E;"># (B, T, 16)</span></span>
<span id="cb1-15">q <span class="op" style="color: #5E5E5E;">=</span> query(x) <span class="co" style="color: #5E5E5E;"># (B, T, 16)</span></span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="co" style="color: #5E5E5E;"># compute the weights using Q &amp; K</span></span>
<span id="cb1-18">wei <span class="op" style="color: #5E5E5E;">=</span>  q <span class="op" style="color: #5E5E5E;">@</span> k.transpose(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>, <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>) <span class="co" style="color: #5E5E5E;"># (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)</span></span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="co" style="color: #5E5E5E;"># softmax+normalize the weights matrix</span></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;"># tril = torch.tril(torch.ones(T, T))</span></span>
<span id="cb1-22"><span class="co" style="color: #5E5E5E;"># wei = wei.masked_fill(tril == 0, float('-inf')) # this line is for a decoder block</span></span>
<span id="cb1-23">wei <span class="op" style="color: #5E5E5E;">=</span> F.softmax(wei, dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb1-24"></span>
<span id="cb1-25">v <span class="op" style="color: #5E5E5E;">=</span> value(x)</span>
<span id="cb1-26">out <span class="op" style="color: #5E5E5E;">=</span> wei <span class="op" style="color: #5E5E5E;">@</span> v</span>
<span id="cb1-27"><span class="co" style="color: #5E5E5E;">#out = wei @ x</span></span>
<span id="cb1-28"></span>
<span id="cb1-29">out.shape</span></code></pre></div>
</div>
</section>
</section>
<section id="transformer-components" class="level2">
<h2 class="anchored" data-anchor-id="transformer-components">Transformer Components</h2>
<p>With self-attention out of the way, let’s move on to the entire transformer tower.</p>
<section id="types-of-transformers" class="level3">
<h3 class="anchored" data-anchor-id="types-of-transformers">Types of Transformers</h3>
<p>There can two be components - Encoder, and a Decoder. This is a sequence-to-sequence model, used for tasks like machine translation (which was the topic of intereset of the original transformer paper). However, there can be encoder-only transformers geared towards text understanding and applications like sentiment analysis, named entity recognition, etc. BERT is an example of encoder-model. There can also be decoder-only models like GPT which are geared towards production of text given a small input.</p>
</section>
<section id="components" class="level3">
<h3 class="anchored" data-anchor-id="components">Components</h3>
<p>Encoding component is actually six encoders, stacked on top of each other. They have the same structure, but they do not share weights. Each encoder has two parts - Self-Attention and Feed-Forward Neural Network. The FFNN weights are same for all words, for a given encoder.</p>
<p>Decoding Component too has these two components, but an extra one in the middle - Encoder-Decoder attention to focus on the encodings. For an encoder-decoder attention block, the K&amp;V matrices come from the last encoder block, while Q matrices come from itself.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/nlp/assets/enc_dec.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Encoder-Decoder</figcaption><p></p>
</figure>
</div>
</section>
<section id="multi-headed-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-headed-attention">Multi-Headed Attention</h3>
<p>We discussed self-attention, however skipped an important aspect of transformers that is multi-headed attention.</p>
<p>Let’s go back to the same example. The word <code>it</code> is correctly identified by the attention outputs to be related to <code>animal</code>. But there can be multiple contact points of information and interaction in the same sentence! For example the word <code>tired</code> is also related to <code>animal</code> but this relationship is not capture in one self-attention.</p>
<p>This can be remedied by having multiple attention heads! This means that for a single transformer block, we’ll have multiple sets of Q, K, V matrices instead of one. And before passing to the Feed Forward Neural Network, we’ll concatenate their outputs to aggregate their information.</p>
<p>This helps us capture multi-tiered information, like shown in this example:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/nlp/assets/att_example2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Attention With Two Heads</figcaption><p></p>
</figure>
</div>
</section>
<section id="position-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="position-embeddings">Position Embeddings</h3>
<p>So far in self-attention, we have not considered the <em>order</em> or the <em>arrangement</em> of the words in the sentence at all. The model simply looks around for attention irrespective of the query’s position. However positional information is important (obviously), so it needs to be incorporated somehow.</p>
<p>We create one more embedding of size [T, C]. Here, T is the number of words in a sentence, and C is the embedding dimension of the words. We simply add this embedding to the word / token embedding before computing the attention.</p>
</section>
<section id="residual-connections-and-layernorm" class="level3">
<h3 class="anchored" data-anchor-id="residual-connections-and-layernorm">Residual Connections and LayerNorm</h3>
<p>Another component in the transformer architecture is the residual connections and Layer Norm.</p>
<p>Layer Norm is different from Batch Normalization, in that latter normalizes using the properties of the entire batch, whereas former normalizes using the properties of the embedding dimension.</p>
<p>After adding all these components, we are able to see the diagram from the paper</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/nlp/assets/arxiv.PNG" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Diagram from the Paper</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="decoder" class="level2">
<h2 class="anchored" data-anchor-id="decoder">Decoder</h2>
<p>The decoder components are exactly the same as encoders - except for one change. Between self-attention and FFN, there’s an additional component called cross-attention. In this attention stage, the K and V matrices come from the encoder, while the Q matrices come from the decoder itself. This helps to aggregate the encoded information while decoding the sequence.</p>
<p>There’s one more difference with Decoder- while computing the attention, the <em>future</em> words are not attended to. That is, while decoding the first word, you cannot get any information from rest of the words. For first two words, no info from the third and onwards. This is implemented using by masking the weights matrix using a lower triangular matrix.</p>
<p>Decoders, like Encoders, are also stacked one above the another, and the outputs keep bubbling up till the last decoder layer. After that, there’s a linear layer that transforms the embeddings into probabilities across the entire vocabulary. Assume the model’s vocabulary is 10,000. Then for each output word there will be 10,000 probabilities. You can pick the highest probability and call it as your result, and discard the rest. This is called greedy search.</p>
</section>
<section id="beam-search" class="level2">
<h2 class="anchored" data-anchor-id="beam-search">Beam-Search</h2>
<p>You can pick the highest probability and call it as your result, and discard the rest. This is called greedy search.</p>
<p>Or, for the first word, you can keep the top two words, “I” and “He”. For the next word, you can run the model twice - one assuming the first word “I” was correct, and the second time assuming “He” was correct. Whichever produced less error considering the two words predicted, is kept. This is called beam search. In our case, beam size was 2- it can be bigger.</p>
</section>
<section id="links" class="level2">
<h2 class="anchored" data-anchor-id="links">Links</h2>
<p>This was an overview of the transformer architecture. A lot of the things here are from either Jay Alammar’s incredibly beautiful blogpost on transformers, or from Andrej Karpathy 2-hour video on implementing GPT from scratch. Both are fantastic resources, do check them out as well. And of course, the original “Attention” paper</p>
<ul>
<li><a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar - The Illustrated Transformer</a></li>
<li><a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Andrej Karpathy - Let’s build GPT: from scratch, in code, spelled out.</a></li>
</ul>


</section>

 ]]></description>
  <category>deep-learning</category>
  <category>nlp</category>
  <guid>https://shindeshu.github.io/posts/nlp/understanding_transformer.html</guid>
  <pubDate>Thu, 02 Feb 2023 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/nlp/assets/mexican_standoff.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>A Comprehensive Guide to Question-Answering in NLP</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/nlp/qa_guide.html</link>
  <description><![CDATA[ 



<section id="the-unlocking-of-knowledge" class="level2">
<h2 class="anchored" data-anchor-id="the-unlocking-of-knowledge">The Unlocking Of Knowledge</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/nlp/assets/sphinx_robot.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">A Greek Sphinx</figcaption><p></p>
</figure>
</div>
<p>The phenomenon of knowledge being locked is common to almost every organization. Knowledge- institutional, technical, introductory etc. is locked in several documents in several places, and needs active search with much efforts going through tomes in order to understand some piece of lore. What if you had an artificial brain who could digest all of this knowledge, and you simply converse with this brain to understand pieces of your lore-verse?</p>
<p>This is no longer science-fiction. Massive language models like ChatGPT have quickly become the norm, and the capabilities of large language models are still being found out. Turns out, you don’t always need a service like ChatGPT, and you can service a lot of your needs using open-source models that can run on a single GPU.</p>
<p>Using NLP models, not only can you unlock your organizational information by enabling intelligent search over large corpora, but can build question-answering apps on top of them, so information retrieval can be automated to an astonishing degree. Q&amp;A apps are one of the best ways to make sense of text data. In this post, we’ll go over this particular application of question-answering, alongwith some basic code to show how to implement them.</p>
<p><strong>Contents</strong>: This post gives an overview of the types of question-answering tasks. It covers the different components in a question answering pipeline, alongwith some basic code implementations, reference libraries and examples.</p>
</section>
<section id="types-of-question-answering-tasks" class="level2">
<h2 class="anchored" data-anchor-id="types-of-question-answering-tasks">Types of Question-Answering Tasks</h2>
<p>There are two ways to divide Question-Answering applications</p>
<ol type="1">
<li>Open-Book vs Closed-Book</li>
<li>Extractive vs Generative</li>
</ol>
<section id="open-book-and-closed-book" class="level3">
<h3 class="anchored" data-anchor-id="open-book-and-closed-book">🕮 Open-Book and Closed-Book</h3>
<p>The most common type of Q&amp;A task is where you give a context/document to the model, as well as the question. The model then gives you an answer to the question. Since you are giving a context alongwith the question to the model, this is called an open-book QA task. The model answers the question only using the information given. This is analogous to open-book exams, where you are allowed to bring a book to the exam. Then there’s closed-book task where you are not given any context, the model is supposed to answer based on the information is has stored in itself. Because we are passing a ground truth in open-book tasks, they usually perform better, and hence are the preferred setting.</p>
<p><strong>The Problem</strong>: Giving the context is extra trouble- what if I have thousands of documents? How can the model find answer through thousands of documents? Let’s answer this question in the next section.</p>
</section>
<section id="extractive-vs-generative" class="level3">
<h3 class="anchored" data-anchor-id="extractive-vs-generative">🖋️ Extractive vs Generative</h3>
<p>Then we come to the second classification- Extractive vs Generative. In Extractive QA, the answer will be a sentence/phrase from the given document. It will not create new words/sentences, but simply find the most relevant text in the context and return it. Hence it is sort of an advanced search algorithm that retrieves the section you need from the document. Because the returned phrase exists in the ground truth, this method is useful when you need accurate, small answers.</p>
<p>In generative QA, the model will craft an answer based on it’s understanding of the context, instead of simply returning an existing phrase. Because the model is generating text, it is more prone to making errors.</p>
<p>In both of above applications, the returned answers tend to be small, one-liners. Long-form QA is a variant of generative QA, where the answers are supposed to be long.</p>
</section>
</section>
<section id="a-small-example" class="level2">
<h2 class="anchored" data-anchor-id="a-small-example">A Small Example</h2>
<p>This is a small example of an extractive QA. Note that the pipeline requires both a context and the question.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> pipeline</span>
<span id="cb1-2">question_answerer <span class="op" style="color: #5E5E5E;">=</span> pipeline(<span class="st" style="color: #20794D;">"question-answering"</span>, </span>
<span id="cb1-3">                             model<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'distilbert-base-cased-distilled-squad'</span>)</span>
<span id="cb1-4"></span>
<span id="cb1-5">context <span class="op" style="color: #5E5E5E;">=</span> <span class="vs" style="color: #20794D;">r"""</span></span>
<span id="cb1-6"><span class="vs" style="color: #20794D;">Extractive Question Answering is the task of extracting an answer from a text </span></span>
<span id="cb1-7"><span class="vs" style="color: #20794D;">given a question. An example of a question answering dataset is the SQuAD dataset, </span></span>
<span id="cb1-8"><span class="vs" style="color: #20794D;">which is entirely based on that task. If you would like to fine-tune a model </span></span>
<span id="cb1-9"><span class="vs" style="color: #20794D;">on a SQuAD task, you may leverage the </span></span>
<span id="cb1-10"><span class="vs" style="color: #20794D;">examples/pytorch/question-answering/run_squad.py script.</span></span>
<span id="cb1-11"><span class="vs" style="color: #20794D;">"""</span></span>
<span id="cb1-12"></span>
<span id="cb1-13">question <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"What is a good example of a question answering dataset?"</span></span>
<span id="cb1-14"></span>
<span id="cb1-15">result <span class="op" style="color: #5E5E5E;">=</span> question_answerer(question<span class="op" style="color: #5E5E5E;">=</span>question, context<span class="op" style="color: #5E5E5E;">=</span>context)</span>
<span id="cb1-16"></span>
<span id="cb1-17"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Answer: '</span><span class="sc" style="color: #5E5E5E;">{</span>result[<span class="st" style="color: #20794D;">'answer'</span>]<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">', "</span></span>
<span id="cb1-18">      <span class="ss" style="color: #20794D;">f"score: </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">round</span>(result[<span class="st" style="color: #20794D;">'score'</span>], <span class="dv" style="color: #AD0000;">4</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, "</span></span>
<span id="cb1-19">      <span class="ss" style="color: #20794D;">f"start: </span><span class="sc" style="color: #5E5E5E;">{</span>result[<span class="st" style="color: #20794D;">'start'</span>]<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, end: </span><span class="sc" style="color: #5E5E5E;">{</span>result[<span class="st" style="color: #20794D;">'end'</span>]<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span></span>
<span id="cb1-20"></span>
<span id="cb1-21"><span class="co" style="color: #5E5E5E;"># Answer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160</span></span></code></pre></div>
</div>
</section>
<section id="components-of-qa-systems" class="level2">
<h2 class="anchored" data-anchor-id="components-of-qa-systems">Components of QA Systems</h2>
<p>Let’s go back to the problem with open-book answering- each question needs a reference document, which cannot be provided manually. The solution is to use another model to retrieve the most relevant document to the question, which then another model can use to do the answering. Hence this becomes a two-stage system, a retriever followed by a reader.</p>
<p>QA systems will hence have these components-</p>
<ul>
<li>a database of documents (your knowledge base)</li>
<li>a retriever component (to retrieve relevant documents)</li>
<li>an optional reranker component (to re-order the relevant documents)</li>
<li>a reader component (to extract the answer) or a generator component (to generate an answer)</li>
</ul>
<p>If you’re using a large language model like GPT-3, this could as simple as a single generator.</p>
<p>Here’s a diagram that summarizes these components as well as how they’re put together:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/nlp/assets/qa3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Overview of QA Systems</figcaption><p></p>
</figure>
</div>
<section id="retriever---how-find-the-relevant-documents" class="level3">
<h3 class="anchored" data-anchor-id="retriever---how-find-the-relevant-documents">🕵🏻‍ Retriever - How Find the Relevant Documents</h3>
<p>Let’s say you want to build a question-answering system on a bunch of internal documentation. The documentation is a pile of thousands of documents. To get an answer, you need to pass a question, and alongwith a passage that is relevant to that question. Since finding the passage is a huge task in itself, how is the QA app helpful at all?</p>
<p>The solution is to let a model handle this task as well. This will be a different model than the QA model, and will be called “retriever”. All the documents will be passed to this model to get embeddings for each of them.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
What Are Embeddings? (Click to Expand)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Embedding is a numerical/vector representation of an entity, in this case a document. Embeddings of two similar documents will be closer to each other numerically.</p>
</div>
</div>
</div>
<p>Now, when we pass a question, we’ll compute the embeddings of the question as well. We’ll find the closest documents to the given question by comparing their embeddings. This is how finding relevant documents is automated using a model.</p>
<p>Finding the relevant document is in itself an application. Sometimes you don’t need a QA on top, finding a needle in the haystack is beneficial in itself. This is called “Semantic Search”, and is extremely easy to implement with a significant improvement in productivity.</p>
</section>
<section id="reranking" class="level3">
<h3 class="anchored" data-anchor-id="reranking">⚖ Reranking</h3>
<p>For thousands and millions of documents, the performance of a retriever alone can be insufficient. It could return documents that may be related to some keywords in the query, but not related to the query at all. In order to enhance the results of our search, we could do reranking of the results.</p>
<p>The retriever returns us a list of 100 documents, and reranker will again evaluate which of these 100 are most relevant to the query. Reranking has been proven to significantly improve the results for tasks like QA. However an extra model will be compute-intensive too. If the number of documents is not too high, re-ranking may not be necessary.</p>
</section>
<section id="get-answer-using-reader-generator" class="level3">
<h3 class="anchored" data-anchor-id="get-answer-using-reader-generator">✍🏻 Get Answer Using Reader / Generator</h3>
<p>Now that you have a context for the given question, you can pass both to a QA model that can (extract or generate) answer. But first, let us define this “Reader” model.</p>
<p>If this is an extractive QA, i.e.&nbsp;highlighting the relevant phrase, you can use models like BERT fine-tuned on open-source QA datasets like SQuAD. If this is a generative QA, we use a model like BART/T5/GPT-2</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
The Types of Transformers (Click to Expand)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>There are three types of transformer models: Encoder, Decoder, and encoder-decoder. Encoders, like BERT, are suited to understand given text, Decoders like GPTs are suited to generate new text, Encoder-Decoders like BART/T5 are suited to generate text based on given texts.</p>
</div>
</div>
</div>
<p>Here are examples for all the three types of components- a retriever, an answer reader and an answer generator</p>
<p>The retriever example uses small sentences as corpus, but it also performs remarkably well if the corpus is large paragraphs and documents.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Retriever</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Extractive QA</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Generative QA</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> sentence_transformers <span class="im" style="color: #00769E;">import</span> SentenceTransformer, util</span>
<span id="cb2-2"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="co" style="color: #5E5E5E;"># get the embedding model</span></span>
<span id="cb2-5">embedder <span class="op" style="color: #5E5E5E;">=</span> SentenceTransformer(<span class="st" style="color: #20794D;">'all-MiniLM-L6-v2'</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7"><span class="co" style="color: #5E5E5E;"># Corpus with example sentences</span></span>
<span id="cb2-8"><span class="co" style="color: #5E5E5E;"># works well with large paragraphs too</span></span>
<span id="cb2-9">corpus <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">'A man is eating food.'</span>,</span>
<span id="cb2-10">          <span class="st" style="color: #20794D;">'A man is eating a piece of bread.'</span>,</span>
<span id="cb2-11">          <span class="st" style="color: #20794D;">'The girl is carrying a baby.'</span>,</span>
<span id="cb2-12">          <span class="st" style="color: #20794D;">'A man is riding a horse.'</span>,</span>
<span id="cb2-13">          <span class="st" style="color: #20794D;">'A woman is playing violin.'</span>,</span>
<span id="cb2-14">          <span class="st" style="color: #20794D;">'Two men pushed carts through the woods.'</span>,</span>
<span id="cb2-15">          <span class="st" style="color: #20794D;">'A man is riding a white horse on an enclosed ground.'</span>,</span>
<span id="cb2-16">          <span class="st" style="color: #20794D;">'A monkey is playing drums.'</span>,</span>
<span id="cb2-17">          <span class="st" style="color: #20794D;">'A cheetah is running behind its prey.'</span></span>
<span id="cb2-18">          ]</span>
<span id="cb2-19"><span class="co" style="color: #5E5E5E;"># compute embeddings for the entire corpus- one-time step.</span></span>
<span id="cb2-20">corpus_embeddings <span class="op" style="color: #5E5E5E;">=</span> embedder.encode(corpus, convert_to_tensor<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-21"></span>
<span id="cb2-22"><span class="co" style="color: #5E5E5E;"># give the query for which most similar document is to be retrieved.</span></span>
<span id="cb2-23">query <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'A man is eating pasta.'</span></span>
<span id="cb2-24"></span>
<span id="cb2-25"><span class="co" style="color: #5E5E5E;"># get the embedding for the query</span></span>
<span id="cb2-26">query_embedding <span class="op" style="color: #5E5E5E;">=</span> embedder.encode(query, convert_to_tensor<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-27"></span>
<span id="cb2-28"><span class="co" style="color: #5E5E5E;"># get the closest 5 documents to the query in embedding space.</span></span>
<span id="cb2-29">cos_scores <span class="op" style="color: #5E5E5E;">=</span> util.cos_sim(query_embedding, corpus_embeddings)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb2-30">top_results <span class="op" style="color: #5E5E5E;">=</span> torch.topk(cos_scores, k<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb2-31"></span>
<span id="cb2-32"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Query:"</span>, query, <span class="st" style="color: #20794D;">"</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">Similar Documents:"</span>)</span>
<span id="cb2-33"><span class="cf" style="color: #003B4F;">for</span> score, idx <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">zip</span>(top_results[<span class="dv" style="color: #AD0000;">0</span>], top_results[<span class="dv" style="color: #AD0000;">1</span>]):</span>
<span id="cb2-34">    <span class="bu" style="color: null;">print</span>(corpus[idx], <span class="st" style="color: #20794D;">"(Score: </span><span class="sc" style="color: #5E5E5E;">{:.4f}</span><span class="st" style="color: #20794D;">)"</span>.<span class="bu" style="color: null;">format</span>(score))</span></code></pre></div>
<p>Output:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;">Query:</span> A man is eating pasta. </span>
<span id="cb3-2"><span class="ex" style="color: null;">Similar</span> Documents:</span>
<span id="cb3-3"><span class="ex" style="color: null;">A</span> man is eating food. <span class="er" style="color: #AD0000;">(</span><span class="ex" style="color: null;">Score:</span> 0.7035<span class="kw" style="color: #003B4F;">)</span></span>
<span id="cb3-4"><span class="ex" style="color: null;">A</span> man is eating a piece of bread. <span class="er" style="color: #AD0000;">(</span><span class="ex" style="color: null;">Score:</span> 0.5272<span class="kw" style="color: #003B4F;">)</span></span>
<span id="cb3-5"><span class="ex" style="color: null;">A</span> man is riding a horse. <span class="er" style="color: #AD0000;">(</span><span class="ex" style="color: null;">Score:</span> 0.1889<span class="kw" style="color: #003B4F;">)</span></span>
<span id="cb3-6"><span class="ex" style="color: null;">A</span> man is riding a white horse on an enclosed ground. <span class="er" style="color: #AD0000;">(</span><span class="ex" style="color: null;">Score:</span> 0.1047<span class="kw" style="color: #003B4F;">)</span></span>
<span id="cb3-7"><span class="ex" style="color: null;">A</span> cheetah is running behind its prey. <span class="er" style="color: #AD0000;">(</span><span class="ex" style="color: null;">Score:</span> 0.0980<span class="kw" style="color: #003B4F;">)</span></span></code></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> pipeline</span>
<span id="cb4-2">question_answerer <span class="op" style="color: #5E5E5E;">=</span> pipeline(<span class="st" style="color: #20794D;">"question-answering"</span>, model<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'distilbert-base-cased-distilled-squad'</span>)</span>
<span id="cb4-3"></span>
<span id="cb4-4">context <span class="op" style="color: #5E5E5E;">=</span> <span class="vs" style="color: #20794D;">r"""</span></span>
<span id="cb4-5"><span class="vs" style="color: #20794D;">Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a</span></span>
<span id="cb4-6"><span class="vs" style="color: #20794D;">question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune</span></span>
<span id="cb4-7"><span class="vs" style="color: #20794D;">a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.</span></span>
<span id="cb4-8"><span class="vs" style="color: #20794D;">"""</span></span>
<span id="cb4-9"></span>
<span id="cb4-10">result <span class="op" style="color: #5E5E5E;">=</span> question_answerer(question<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"What is a good example of a question answering dataset?"</span>,     context<span class="op" style="color: #5E5E5E;">=</span>context)</span>
<span id="cb4-11"><span class="bu" style="color: null;">print</span>(</span>
<span id="cb4-12"><span class="ss" style="color: #20794D;">f"Answer: '</span><span class="sc" style="color: #5E5E5E;">{</span>result[<span class="st" style="color: #20794D;">'answer'</span>]<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">', score: </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">round</span>(result[<span class="st" style="color: #20794D;">'score'</span>], <span class="dv" style="color: #AD0000;">4</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, start: </span><span class="sc" style="color: #5E5E5E;">{</span>result[<span class="st" style="color: #20794D;">'start'</span>]<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">, end: </span><span class="sc" style="color: #5E5E5E;">{</span>result[<span class="st" style="color: #20794D;">'end'</span>]<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span></span>
<span id="cb4-13"></span>
<span id="cb4-14"><span class="co" style="color: #5E5E5E;"># Answer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160</span></span></code></pre></div>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;">import</span> torch</span>
<span id="cb5-2"><span class="im" style="color: #00769E;">from</span> transformers <span class="im" style="color: #00769E;">import</span> AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM</span>
<span id="cb5-3"></span>
<span id="cb5-4">model_name <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"vblagoje/bart_lfqa"</span></span>
<span id="cb5-5">device <span class="op" style="color: #5E5E5E;">=</span> torch.device(<span class="st" style="color: #20794D;">'cuda'</span> <span class="cf" style="color: #003B4F;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">'cpu'</span>)</span>
<span id="cb5-6"></span>
<span id="cb5-7">tokenizer <span class="op" style="color: #5E5E5E;">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb5-8">model <span class="op" style="color: #5E5E5E;">=</span> AutoModelForSeq2SeqLM.from_pretrained(model_name)</span>
<span id="cb5-9">model <span class="op" style="color: #5E5E5E;">=</span> model.to(device)</span>
<span id="cb5-10"></span>
<span id="cb5-11"><span class="co" style="color: #5E5E5E;"># it all starts with a question/query</span></span>
<span id="cb5-12">query <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"Why does water heated to room temperature feel colder than the air around it?"</span></span>
<span id="cb5-13"></span>
<span id="cb5-14"><span class="co" style="color: #5E5E5E;"># given the question above suppose these documents below were found in some document store </span></span>
<span id="cb5-15">documents <span class="op" style="color: #5E5E5E;">=</span> [<span class="st" style="color: #20794D;">"when the skin is completely wet. The body continuously loses water by..."</span>,</span>
<span id="cb5-16">             <span class="st" style="color: #20794D;">"at greater pressures. There is an ambiguity, however, as to the meaning of the terms 'heating' and 'cooling'..."</span>,</span>
<span id="cb5-17">             <span class="st" style="color: #20794D;">"are not in a relation of thermal equilibrium, heat will flow from the hotter to the colder, by whatever pathway..."</span>,</span>
<span id="cb5-18">             <span class="st" style="color: #20794D;">"air condition and moving along a line of constant enthalpy toward a state of higher humidity. A simple example ..."</span>,            </span>
<span id="cb5-19">             <span class="st" style="color: #20794D;">"Thermal contact conductance In physics, thermal contact conductance is the study of heat conduction between solid ..."</span>]</span>
<span id="cb5-20"></span>
<span id="cb5-21"><span class="co" style="color: #5E5E5E;"># concatenate question and support documents into BART input</span></span>
<span id="cb5-22">conditioned_doc <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"&lt;P&gt; "</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="st" style="color: #20794D;">" &lt;P&gt; "</span>.join([d <span class="cf" style="color: #003B4F;">for</span> d <span class="kw" style="color: #003B4F;">in</span> documents])</span>
<span id="cb5-23">query_and_docs <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"question: </span><span class="sc" style="color: #5E5E5E;">{}</span><span class="st" style="color: #20794D;"> context: </span><span class="sc" style="color: #5E5E5E;">{}</span><span class="st" style="color: #20794D;">"</span>.<span class="bu" style="color: null;">format</span>(query, conditioned_doc)</span>
<span id="cb5-24"></span>
<span id="cb5-25">model_input <span class="op" style="color: #5E5E5E;">=</span> tokenizer(query_and_docs, truncation<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, padding<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pt"</span>)</span>
<span id="cb5-26"></span>
<span id="cb5-27">generated_answers_encoded <span class="op" style="color: #5E5E5E;">=</span> model.generate(input_ids<span class="op" style="color: #5E5E5E;">=</span>model_input[<span class="st" style="color: #20794D;">"input_ids"</span>].to(device),</span>
<span id="cb5-28">                                           attention_mask<span class="op" style="color: #5E5E5E;">=</span>model_input[<span class="st" style="color: #20794D;">"attention_mask"</span>].to(device))</span>
<span id="cb5-29">tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>,clean_up_tokenization_spaces<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb5-30"></span>
<span id="cb5-31"><span class="co" style="color: #5E5E5E;"># below is the abstractive answer generated by the model</span></span>
<span id="cb5-32"><span class="co" style="color: #5E5E5E;">"""</span></span>
<span id="cb5-33"><span class="co" style="color: #5E5E5E;">When you heat water to room temperature, it loses heat to the air around it. </span></span>
<span id="cb5-34"><span class="co" style="color: #5E5E5E;">When you cool it down, it gains heat back from the air, which is why it feels colder</span></span>
<span id="cb5-35"><span class="co" style="color: #5E5E5E;">than the air surrounding it. It's the same reason why you feel cold when you turn</span></span>
<span id="cb5-36"><span class="co" style="color: #5E5E5E;">on a fan. The air around you is losing heat, and the water is gaining heat.</span></span>
<span id="cb5-37"><span class="co" style="color: #5E5E5E;">"""</span> </span></code></pre></div>
</div>
</div>
</div>
</section>
<section id="retrieval-augmented-generator" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-augmented-generator">🌌 Retrieval-Augmented Generator</h3>
<p>Then there’s another approach where you combine both retriever and generator and train them on the same corpus. Earlier, the retriever and generator were distinct models trained on two different datasets. Here, you have two models trained on the same task, same dataset. This is an active area of research, still not matured- with Deepmind publishing architectures like RETRO, Meta releasing DPR, etc.</p>
</section>
</section>
<section id="question-answering-vs-chatbots" class="level2">
<h2 class="anchored" data-anchor-id="question-answering-vs-chatbots">Question-Answering vs Chatbots</h2>
<p>It’s a natural question- how is question-answering different from a chatbot? A chatbot is a lot of things - it could be a very simple lookup tool, or a massive ChatGPT. A chatbot may not always need a sophisticated transformer-based approach. You could build a simple chatbot that identifies the query and maps it to internal FAQs, and reply with options instead of a natural language reply. Conversational AI also needs to have some session memory, i.e.&nbsp;being able to remember the history of the conversation.</p>
<p>The use of QA and Conversational AI is hence different, although combining both chatbot and Q&amp;A pipeline can be helpful in many cases. This post only covered QA, and not Chatbot.</p>
</section>
<section id="a-real-project-using-generative-qa" class="level2">
<h2 class="anchored" data-anchor-id="a-real-project-using-generative-qa">A Real Project Using Generative QA</h2>
<p>This post only covers some overview of the techniques and terms, but the examples provided are very basic. If you want to apply this to a real world task, it will take some more engineering. Currently I’m working on a project, a generative QA task on a corpus of documents, and that will require a new post entirely.</p>
<p>However there are some interesting huggingface spaces on QA which are worth looking into.</p>
<ul>
<li><a href="https://huggingface.co/spaces/Rschmaelzle/wikipedia-assistant">Wikipedia Assistant</a> - Uses BART model for generative question answering on wikipedia corpus.</li>
<li><a href="https://huggingface.co/spaces/Abhilashvj/haystack_QA">Haystack QA</a> - Uses roBERTa for extractive question answering.</li>
</ul>
<p>There’s also some blogposts on creating support chatbots using GPT-3.</p>
<ul>
<li>[Build a GitHub support bot with GPT3, LangChain, and Python] https://dagster.io/blog/chatgpt-langchain</li>
</ul>
</section>
<section id="domain-adaptation" class="level2">
<h2 class="anchored" data-anchor-id="domain-adaptation">Domain Adaptation</h2>
<p>Most of the models available online have been fine-tuned on datasets like SQuAD, NQ etc. which might be from a completely different domain compared to yours. In this case, will they work? Generally, those abilities should transfer to any domain, but fine-tuning can give you massive boosts in performance. What is the best approach to do this fine-tuning- this is an area of active research, so too early to write. But fine-tuning on a Question-Answer dataset is relatively easy.</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>Deep Learning, unlike other parts of computing, requires you to shell out $ for the most meaningful applications. This is even more true for NLP than computer vision or tabular ML. So there are three tiers of developers- the GPT-3 havers, the GPU havers, the Colab-ers.</p>
<ul>
<li><p><strong>Paid GPT-3 API</strong> - Want to build your own QA application? A lot depends on the budget- if it’s an enterprise application where a paid GPT-3 API is available, there are libraries like LangChain or GPT-Index that abstract away a lot of boilerplate code and let you easily build a question-answering chain in a few lines of code. LangChain also supports some other open-source huggingface models.</p></li>
<li><p><strong>Lot of Cloud Compute</strong> - If using a paid GPT-3 API is not possible, but there’s budget enough for several GPUs- you can use open-source large language models like Flan-T5, OPT-IML, GPT-J/Neo etc. Huggingface hosts all of these, and is really easy to use and fine-tune.</p></li>
<li><p><strong>Free-tier Google Colab</strong> - If you want to build a simple prototype on the free-tier of Google Colab, you’d be off to using the smaller models. Models like Flan-T5 do quite well even at small sizes. Generative QA don’t do that well at these sizes, but if fine-tuning could do the trick.</p></li>
</ul>
<section id="libraries" class="level3">
<h3 class="anchored" data-anchor-id="libraries">Libraries</h3>
<p>The best place for NLP on the internet is, without doubt, <strong>Huggingface</strong>. It has very good documentation for all NLP tasks, a lot of models that can be easily downloaded and used. <a href="https://huggingface.co/tasks/question-answering">This is a good starting page for QA</a>.</p>
<p><strong>Haystack</strong> is library that is built specifically for Question-Answering systems, has easy implementation for retriever/reader/generator etc, and you can use huggingface transformer models with it.</p>
<p>If you’re using GPT-3 API, <strong>LangChain</strong> can very helpful in building a QA app. <strong>GPT-Index</strong> is another library that has useful utilities to build QA pipelines on top of existing document base.</p>
</section>
<section id="links" class="level3">
<h3 class="anchored" data-anchor-id="links">Links</h3>
<ul>
<li><p><a href="https://lilianweng.github.io/posts/2020-10-29-odqa/">How to Build an Open-Domain Question Answering System?</a></p></li>
<li><p><a href="https://yjernite.github.io/lfqa.html">A Model for Open Domain Long Form Question Answering</a></p></li>
</ul>


</section>
</section>

 ]]></description>
  <category>deep-learning</category>
  <category>nlp</category>
  <guid>https://shindeshu.github.io/posts/nlp/qa_guide.html</guid>
  <pubDate>Sat, 28 Jan 2023 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/nlp/assets/sphinx_robot.png" medium="image" type="image/png" height="108" width="144"/>
</item>
<item>
  <title>Why Data Scientists Should Write Tests</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/datascience/whytests.html</link>
  <description><![CDATA[ 



<section id="the-jupyter-habitat" class="level2">
<h2 class="anchored" data-anchor-id="the-jupyter-habitat">The Jupyter Habitat</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/datascience/assets/habitat.JPG" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption"><em>It is natural to use Jupyter</em></figcaption><p></p>
</figure>
</div>
<p>For a data scientist, jupyter notebooks are a natural habitat. They’re usually the first step in any project, and it’s the arena where thousands of lines of code are written, erased and forgotten forever in dumpgrounds. Data science is very exploratory in nature, because of which rapid iteration is necessary to build projects, which is exactly why notebooks are so popular. Constant experimentation and changes are a part of the job.</p>
<p>Yet as time passes and the project increases in scope, you find yourself moving away from a notebooks appraoch. There are many reasons why you’d switch to a traditional repo over a notebook.</p>
</section>
<section id="jupyter-no-longer" class="level2">
<h2 class="anchored" data-anchor-id="jupyter-no-longer">Jupyter No Longer</h2>
<section id="project-grows-in-scope." class="level3">
<h3 class="anchored" data-anchor-id="project-grows-in-scope.">Project Grows in Scope.</h3>
<p>There is simply more stuff to do, more knobs and more switches. At first, you think a utils.py file will take care of it all. But quickly the utils.py swells in size, resembling a Lovecraftian horror of a thousand functions each doing some distinct operation.</p>
<p>It is then you create modules for better delegating things and separation of operations. And the crisis is averted. For now.</p>
</section>
<section id="collaboration" class="level3">
<h3 class="anchored" data-anchor-id="collaboration">Collaboration</h3>
<p>Notebooks are great for one contributor to keep tinkering, but it is not the best approach when working with a team. Having a distributed system can be beneficial since people focus on their part of the repo. Another reason to move away from notebooks.</p>
</section>
<section id="deployment" class="level3">
<h3 class="anchored" data-anchor-id="deployment">Deployment</h3>
<p>While there have been newer tools to facilitate notebooks in production (<code>nbdev</code> etc), deployment to production generally needs you to have all of your code in <code>.py</code> files. This is another reason to move to modules.</p>
</section>
</section>
<section id="the-crisis-of-rapid-iteration" class="level2">
<h2 class="anchored" data-anchor-id="the-crisis-of-rapid-iteration">The Crisis of Rapid Iteration</h2>
<section id="slowing-down" class="level3">
<h3 class="anchored" data-anchor-id="slowing-down">Slowing Down</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/datascience/assets/tuning.jpg" class="img-fluid figure-img" style="width:60.0%"></p>
<p></p><figcaption class="figure-caption"><em>Things become less fun</em></figcaption><p></p>
</figure>
</div>
<p>Except, now that the pivot is complete, you find yourself more frustrated than before. When writing a notebook, we usually ignore what are called “best practices” since they slow us down. We keep ignoring such practices even as the work grows in scope, and problems start rising.</p>
<p>Notebooks are a high velocity medium, which is their biggest strengths. You can quickly put together a bunch of code and get it running. You can quickly erase lines and add to make changes. It is a nimble, flexible craft.</p>
<p>When you are not working on a single notebook, and you have a forest of modules feeding into your cursor, the velocity often takes a hit. You find out that making a single change is a lot more expensive now than it was before. Many things end up breaking, when earlier making changes was easy. To add some functionality, you end up thinking about all the potential modules that could break.</p>
<p>This slows down the pace of development, and you end up captured in the web of your own local imports, scratching your heads through all the scripts present.</p>
</section>
<section id="its-a-mess" class="level3">
<h3 class="anchored" data-anchor-id="its-a-mess">It’s a Mess</h3>
<p>And there’s another important reason why the pace is down- because your code is too convoluted. The classes you defined have low cohesion, i.e.&nbsp;they do all sorts of different things. The modules you’ve defined are extremely dependent on each other, i.e.&nbsp;high coupling. So when you make changes in one module, because of high dependencies, some other module ends up breaking as well.</p>
<p>Managing the project architecture efficiently like this is an art that is learned through experience. If only there were simple disciplines, following which you could write better code. Turns out, writing tests can do that.</p>
</section>
</section>
<section id="what-are-tests" class="level2">
<h2 class="anchored" data-anchor-id="what-are-tests">What are Tests?</h2>
<p><img src="https://shindeshu.github.io/posts/datascience/assets/test.jpg" class="img-fluid" style="width:60.0%"></p>
<p>A few months ago, despite being 4 years into my professional journey, I had little idea about tests. It seemed like a software engineering concept that has no obvious application in a data science project. So if you’re like me, here’s a little primer about tests.</p>
<p>If, after making a change to the codebase, you run the entire workflow to make sure it worked (or broke), you are already testing. Except less efficiently. Running the entire workflow can be time-consuming, and you don’t want to run everything when you’ve made changes to only one section of the codebase. Plus, debugging can be tough. Tests are basically functions that you set up to check whether other modules are doing their job correctly.</p>
<p>Here’s an example of a function in the <code>transforms.py</code> file that does something.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;">def</span> transform_data(df: pd.DataFrame) <span class="op" style="color: #5E5E5E;">-&gt;</span> pd.DataFrame:</span>
<span id="cb1-2">    <span class="co" style="color: #5E5E5E;"># run some transformation</span></span>
<span id="cb1-3">    <span class="cf" style="color: #003B4F;">return</span> df</span></code></pre></div>
</div>
<p>To create a test for this function (and others) create a file called <code>test_transforms.py</code>. Convention is to put all your tests inside a <code>tests/</code> folder at the top of the repo. Inside this file you can write</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">from</span> src.transforms <span class="im" style="color: #00769E;">import</span> transform_data</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="kw" style="color: #003B4F;">def</span> get_mock_data():</span>
<span id="cb2-4">    <span class="co" style="color: #5E5E5E;"># create mock data</span></span>
<span id="cb2-5">    df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame({}) </span>
<span id="cb2-6"></span>
<span id="cb2-7"><span class="kw" style="color: #003B4F;">def</span> test_transform_data():</span>
<span id="cb2-8">    <span class="co" style="color: #5E5E5E;"># the testing function should start with 'test_'</span></span>
<span id="cb2-9">    df <span class="op" style="color: #5E5E5E;">=</span> get_mock_data()</span>
<span id="cb2-10">    transformed_df <span class="op" style="color: #5E5E5E;">=</span> transform_data(df)</span>
<span id="cb2-11">    <span class="co" style="color: #5E5E5E;"># assert that the transform has correctly been completed</span></span>
<span id="cb2-12">    <span class="co" style="color: #5E5E5E;"># you can check any aspects that have been transformed</span></span>
<span id="cb2-13">    <span class="co" style="color: #5E5E5E;"># assume that the number of columns are doubled in transformation.</span></span>
<span id="cb2-14">    <span class="cf" style="color: #003B4F;">assert</span> <span class="bu" style="color: null;">len</span>(transformed_df.columns) <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span><span class="bu" style="color: null;">len</span>(df.columns)</span></code></pre></div>
</div>
<p>This test is completely isolated from the rest of the repo, so it’s failure has only one reason, the function itself.</p>
<p>Now you can run test this by three methods:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><span class="ex" style="color: null;">pytest</span> tests/</span></code></pre></div>
<p>Runs all the tests in the folder.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><span class="ex" style="color: null;">pytest</span> tests/test_transforms.py</span></code></pre></div>
<p>Runs only the tests on transforms</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><span class="ex" style="color: null;">pytest</span> tests.test_transforms.test_transform_data</span></code></pre></div>
<p>Runs only this particular function.</p>
</section>
<section id="why-are-tests" class="level2">
<h2 class="anchored" data-anchor-id="why-are-tests">Why Are Tests?</h2>
<section id="write-tests-to-improve-speed" class="level3">
<h3 class="anchored" data-anchor-id="write-tests-to-improve-speed">Write Tests to Improve Speed</h3>
<p>The single biggest reason why as data scientists need to write tests is that they are monumentally helpful in increasing the speed of experiments. Fast iteration is very important for a data scientist because we generally are constantly running experiments trying different ways to improve our models, trying a different architecture, tinkering with the pre-processing, etc.</p>
<p>When you write unit tests, making changes becomes easier because now you have an easy way of validating whether your changes are breaking the code or not. Moreover, because you are writing tests for each unit, you can identify exactly which part of the codebase is being affected by your change. Now that certain parts of the codebase are anchored in your mind as unbreakable, you are free of the anxiety to go tinker on other units.</p>
</section>
<section id="write-tests-for-better-code" class="level3">
<h3 class="anchored" data-anchor-id="write-tests-for-better-code">Write Tests for Better Code</h3>
<p>Second greatest reason is that writing tests help you write efficient code. Generally you shouldn’t write a function that does more than one thing. But while developing, in the flow you could write such cronenberg functions. When you write unit tests, you find that you need to write distinct tests for the same function, which is an indicator that it can be decomposed.</p>
<p>Similarly, writing tests help your code become orthogonal. You learn to write your code in such a manner that each unit is less dependent on other units. Which leads to modular, understandable code that is also very iteration-friendly.</p>
</section>
<section id="write-tests-for-collaboration" class="level3">
<h3 class="anchored" data-anchor-id="write-tests-for-collaboration">Write Tests for Collaboration</h3>
<p>Working in a team needs some vigilance. How to know if a change someone else made isn’t breaking down a routine specific to my task? Or vice versa? You simply write a test for your task, so your teammates can easily verify it themselves, reducing the feedback loop. Or you will waste time in to-and-fro clarifications.</p>
<p>If you hand over your repo to someone else, like a new team or the deployment team, the tests are helpful to them too for quick onboarding.</p>
</section>
<section id="write-tests-because-theyre-easy-and-cool" class="level3">
<h3 class="anchored" data-anchor-id="write-tests-because-theyre-easy-and-cool">Write Tests Because they’re Easy and Cool</h3>
<p>In python, it’s terribly easy to write tests thanks to the <code>pytest</code> library. Requires no syntax to learn, it’s as easy as writing a function. Writing tests is cool, that’s a fact. It also feels great when you run <code>pytest</code> on your code base and all of your cases pass in a flurry of green dots.</p>
<p>This, in a nutshell, why data scientists should not ignore the power of tests.</p>


</section>
</section>

 ]]></description>
  <category>data-science</category>
  <category>coding</category>
  <guid>https://shindeshu.github.io/posts/datascience/whytests.html</guid>
  <pubDate>Sat, 21 Jan 2023 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/datascience/assets/habitat.JPG" medium="image"/>
</item>
<item>
  <title>A Roadmap for Getting into Data Science</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/roadmap/start.html</link>
  <description><![CDATA[ 



<section id="everyone-wants-to-be-in" class="level2">
<h2 class="anchored" data-anchor-id="everyone-wants-to-be-in">Everyone Wants to Be In</h2>
<p>It’s no news nowadays that folks are flocking towards data science- there are quite many aspiring data scientists around who want to do AI things. I wrote this post as a starter for those beginner data scientists who want to break in. This post does not discuss whether you <em>should</em>, or about the quantum of opportunity in this space, that’s a different topic.</p>
<p>This post also does not give you a big list of links. We try to create a structure of our learning process, and in the end give a few resources.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/roadmap/assets/robot_learning.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption"><em>Images generated using Stable Diffusion</em></figcaption><p></p>
</figure>
</div>
</section>
<section id="what-is-dsmlai" class="level2">
<h2 class="anchored" data-anchor-id="what-is-dsmlai">What is DS/ML/AI?</h2>
<p>Definitions are of no help here. There aren’t well-defined boundaries around these terms, no self-evident ground truths that distinguish ML from AI or DS.</p>
<p>Broadly, the field is about learning patterns from data to solve various business problems. Take for instance, price of a house. Traditionally, you could make up basic if-else rules that incorporate business logic to get a value for a house price. With data science you could utilize algorithms to learn these rules and predict these values given the attributes of a house. Similarly, you could</p>
<ul>
<li>Predict when a given machine part could fail</li>
<li>Flag a Credit Card Fraud</li>
<li>Assess Credit-Worthiness given financial attributes of an individual</li>
</ul>
<p>Ability to identify complex patterns in a data is useful in almost every domain that has any significant data, and underlying structure in it.</p>
<section id="some-real-use-cases-of-data-science" class="level3">
<h3 class="anchored" data-anchor-id="some-real-use-cases-of-data-science">Some Real Use Cases of Data Science</h3>
<ul>
<li><a href="https://www.analyticsvidhya.com/blog/2021/04/how-aviation-industry-uses-data-science/">Read how Aviation Industry Uses DS for Optimizing their Business Activities</a></li>
<li><a href="https://newsroom.prattwhitney.com/2016-02-16-Pratt-Whitney-Applies-Big-Data-to-Predict-Engine-Maintenance-Frequency-and-Planning">Read how Pratt and Whitney Uses DS for Predictive Maintenance of Engines</a></li>
</ul>
</section>
<section id="types-of-data---tabular-image-and-text" class="level3">
<h3 class="anchored" data-anchor-id="types-of-data---tabular-image-and-text">Types of Data - Tabular, Image and Text</h3>
<p>The examples cited above are of tabular data. As the name explains, it is data that’s stored in a tabular format. This is the most common data format, and easier to work on as well. But there’s also image data and text data, both are also increasingly forming the proportion of useful industrial models.</p>
<ul>
<li>On Pinterest, the images that get recommended to you are based on ML models.</li>
<li>Facebook, Twitter recommending feeds to you are recommended using ML models as well.</li>
<li>Face Recognition softwares are used worldwide.</li>
<li>Chatbots, Translation all use text-based ML models underneath them.</li>
</ul>
</section>
</section>
<section id="an-overview-of-requirements" class="level2">
<h2 class="anchored" data-anchor-id="an-overview-of-requirements">An Overview of Requirements</h2>
<p>What do you need to become a data scientist, and what do you do on a day-to-day basis on your job?</p>
<p>I’d divide this into four topics - Technical Skills, Theoretical Knowledge, Domain and Business Knowledge, and Project Management. For a beginner, we’d be only interested in the first two.</p>
<section id="technical-skills" class="level3">
<h3 class="anchored" data-anchor-id="technical-skills">Technical Skills</h3>
<p>These are divided into parts.</p>
<ul>
<li><strong>Software Engineering</strong>: You should know how to code.</li>
<li><strong>Tools Specific to Data Science</strong>: Know thy tools!</li>
</ul>
</section>
<section id="theoretical-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-knowledge">Theoretical Knowledge</h3>
<ul>
<li><strong>Theory of Data Science</strong>: Learn from the wisdom of ancients!</li>
<li><strong>Mathematics</strong>: Basics will do fine.</li>
</ul>
</section>
</section>
<section id="structure-your-machine-learning-journey" class="level2">
<h2 class="anchored" data-anchor-id="structure-your-machine-learning-journey">Structure Your Machine Learning Journey</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/roadmap/assets/structure.jpg" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">.</figcaption><p></p>
</figure>
</div>
<p>Here, we’ll expand on what are these skills, and how do you go about acquiring them, and in what order.</p>
<p><strong>Python</strong>: For a beginner, the first step should be to learn how to code. Python is the most used language in this space, and it is great language for a first-time coder to learn. In fact, the ease of coding is exactly the reason why python came to dominate the machine learning landscape. <a href="https://www.youtube.com/watch?v=rfscVS0vtbw&amp;t=5717s">This</a> is a good free resource to start learning.</p>
<p>Basics of the language are enough to get started- you don’t need to leetcode yourself. You do need to get a hang of jupyter notebooks, an IDE like VS-Code, and be able to install packages.</p>
<p><strong>Tabular vs Deep Learning</strong>: As a newcomer, you should aim to get proficient in tabular data before you move to tackling image or text datasets.</p>
<p><strong>Tools</strong>: Knowing pandas (library that handles tabular data) and matplotlib/seaborn (data visualization library) is another starting step. Knowing basic methods is enough at this stage.</p>
<p>Now that we have warmed up, we’ll step into the main process and discuss the sources to learn from.</p>
<section id="the-problem-of-plenty" class="level3">
<h3 class="anchored" data-anchor-id="the-problem-of-plenty">The Problem Of Plenty</h3>
<p>Where to learn from? There is just so much to choose, so many courses, books, websites, twitter influencers, linkedin influencers, an overflowing bookmarks folder, pdfs of unread papers. The progress is overwhelming too- What’s a standard practice a year ago may not be the same today.</p>
<p>This can be overwhelming for a veteran, let alone a beginner. Separating the noise from the signal can be daunting, as well as finding the best possible source to learn from.</p>
<p>Speaking from experience, what works best for me is learning on the job. Whatever projects I’ve worked on, that knowledge stays with me forever. Courses and Books are tougher to retain. So if you’re employed, give your best in your own project, it’s the best way to upskill.</p>
<p>However, if you’re not employed, or your job is narrow in its scope and you want to upskill, the problem persists. There should be a process to identify and follow the materials. (My personal recommendations are at the end of this post.)</p>
<p><strong>Courses</strong>: Instead of sampling and browsing dozens of courses, it is better to simply pick one and get started. The time you waste in finding the perfect course can be better utilized in completing an okay course. Most important thing is to get moving and not be stagnant.</p>
<p><strong>Books</strong>: There are some books that are structured as tutorials. Don’t delve deep into theory at this stage, if a book contains code snippets, is a latest copy, and is easy to read - that’s the one.</p>
<p><strong>Websites and Blogs</strong>: Keeping browsing medium, towardsdatascience, kdnuggets, etc. Keep it light.</p>
</section>
</section>
<section id="blind-men-and-the-elephant" class="level2">
<h2 class="anchored" data-anchor-id="blind-men-and-the-elephant">Blind Men and the Elephant</h2>
<p>Due to the problem of plenty, it is important to keep things simple. Don’t follow too many threads, pick a few sources and stick with it. Course- pick any one and complete it without distraction, but remember that this is not everything, and it has only shown you a part of the elephant. Much like the parable of the blind men and the elephant, where each of the men describe the elephant differently since they’re feeling a different part of the elephant.</p>
<p>Doing an online course could show you one part of this elephant- the theory behind the many algorithms like linear regression, random forest. There are many helpful books too that can do this. To cover other topics you’ll need other things.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/roadmap/assets/elephant.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Understanding the Elephant</figcaption><p></p>
</figure>
</div>
<section id="it-can-get-overwhelming" class="level3">
<h3 class="anchored" data-anchor-id="it-can-get-overwhelming">It Can Get Overwhelming</h3>
<p>I know that’s a lot of stuff, right? Data science can be overwhelming for a veteran, let alone one who’s just starting their journey. There is so much to learn, from the theory to the latest tools, the latest research, etc. Which is why it is important to remember that it is not necessary to learn <em>everything</em>.</p>
</section>
</section>
<section id="list-of-topics" class="level2">
<h2 class="anchored" data-anchor-id="list-of-topics">List of Topics</h2>
<p>This is a rough outline of how your learning journey could look like. Not exhaustive.</p>
<ul>
<li>Python, SQL (Basics)</li>
<li>Pandas and Matplotlib (Basics)
<ul>
<li>Reading and basic operations on dataframes</li>
<li>Basic plotting</li>
</ul></li>
<li>Data Science Theory
<ul>
<li>Supervised vs Unsupervised Learning</li>
<li>Different ways of representing data (encoding, binning)</li>
<li>supervised learning algorithms (linear regression, trees)</li>
<li>unsupervised learning algorithms (clustering, PCA)</li>
<li>model evaluation and metrics</li>
<li>overfitting, underfitting</li>
<li>ensembles, correlation, feature engineering, outliers, regularization</li>
</ul></li>
<li>Practice
<ul>
<li>use datasets to create models</li>
<li>data visualization and EDA</li>
</ul></li>
<li>Full-Fledged Projects</li>
</ul>
<p>This is a rough outline of the process. My recommended resource to start with is the book <strong>Introduction to Machine Learning using Python</strong> by Andreas Muller. It is a very easy-to-read book that contains code snippets and gradually introduces all topics. You can also complete the <strong>Kaggle Courses</strong> at <a href="https://www.kaggle.com/learn">here</a>, they’re quite friendly and brief tutorials, plus you can run them in your browser itself.</p>
<p>This outline doesn’t include deep learning (computer vision or natural language processing), or model deployment.</p>
</section>
<section id="a-practice-first-discipline" class="level2">
<h2 class="anchored" data-anchor-id="a-practice-first-discipline">A Practice-First Discipline</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/roadmap/assets/blacksmith.jpg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Learn by Doing! <em>Image by mastervector on freepik</em></figcaption><p></p>
</figure>
</div>
<p>The first two blocks are in place, the coding environment and the theory of data science. DS is a very practice-oriented discipline. Theoretical knowledge constitutes a small proportion of the data scientist mindspace. It is in the dirty trenches of the notebooks where the real are proven.</p>
<p>If your resume contains just a bunch of courses, it does not give an interviewer any idea of your prowess. But having a project, notebooks, repos is a sign that you have been there and done those things that matter the most. What things, you ask?</p>
<p>Reading data, understanding, cleaning, processing, preparing is the single most important aspect of data science that is often not given the necessary important even by those long in the game. These skills cannot be taught in any course or book, but be learned by doing.</p>
<p>Exploratory Data Analysis (EDA) is a term you’ll come across quite often, and it is an art more than a science, another skill that you’ll learn through practice. Understanding the contents of your data, asking it the right questions and making it answer, this is our bread and butter.</p>
<p>Then there are feature engineering, dealing with imbalanced data, model training, model evaluation, all skills which are practical in nature. Fortunately for us, there is a single place to learn all of the above- <strong>Kaggle</strong>.</p>
<p>Kaggle is a website where you can train machine learning models for free using many datasets available there. (It also hosts competitions, but at this point we are focussed on learning). Simply copy-and-editing someone else’s publicly available notebooks can be incredibly helpful to understand various work flows. Spend your time creating a chimaera of a dozen code blocks from dozen notebooks in a single task, and it will be exponentially helpful from a learning perspective.</p>
<section id="dont-heed-the-kaggle-haters" class="level3">
<h3 class="anchored" data-anchor-id="dont-heed-the-kaggle-haters">Don’t Heed the Kaggle Haters</h3>
<p>There has been criticism about Kaggle that it is not helpful for a data science job, since you always get a clean dataset in hand which is never the case in real life. While it is true that real life data is much messier than Kaggle, practicing in Kaggle has much more returns than handicap. You learn to max out your feature engineering skills, data visualization and EDA skills, and are also able to learn how to rapid iterations. Kaggle is helpful without a doubt.</p>
<p>You also get to learn about the latest techniques, libraries, models, you observe empirical evidence on what methods work and what don’t.</p>
</section>
</section>
<section id="projects" class="level2">
<h2 class="anchored" data-anchor-id="projects">Projects</h2>
<p>Once you are able to write a kaggle notebook from start to end, you have pretty much achieved the baseline of what you set out to do. So if you’re at this stage, you should sit back and congratulate yourself.</p>
<p>If you want to graduate to the next level, you should start doing full fledged projects that solve a meaningful problem. Here, you can use data that’s not from kaggle for a challenge.</p>
<p>This includes some advanced topics that can be skipped by a beginner, but if you are employed as a data scientist, these would be quite important.</p>
<p>How would this differ from a kaggle project</p>
<ul>
<li>work with .py files instead of a notebook</li>
<li>write comments, docstrings in order to be helpful to the reader</li>
<li>use sklearn pipelines in order to save the pre-processing steps, making it easier for inference.</li>
<li>write modular, readable code.</li>
<li>learn about code-writing guidelines. PEP8. Use black, isort, etc. tools.</li>
<li>write tests.</li>
</ul>
</section>
<section id="domain-knowledge" class="level2">
<h2 class="anchored" data-anchor-id="domain-knowledge">Domain Knowledge</h2>
<p>This section is not applicable for a complete beginner.</p>
<p>Nobody pays you to fit a model to a csv. The purpose of data scientists, at the end of the day, is to use their skills to add value to the business. This purpose, and that of understanding the data, formulating problems to solve, and evaluating solutions, all require you to have knowledge of the business systems that you work in. This includes industry knowledge, how processes flow inside your own firm, the meaning behind the data, and more. Acquiring this knowledge needs communication skills, and ability to ask the right questions.</p>
<p>Unlike specialists, data scientists also need to rotate over different businesses. If today you are building a recommender system for ecommerce, tomorrow you could be building a failure prediction model for aircraft engines. The variety of domains we operate in is vast, so the ability to quickly onboard is important too.</p>
</section>
<section id="deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning">Deep Learning</h2>
<p>Well, we only learned about tabular data! Where’s the cool stuff about neural networks, chat-GPT? This post was supposed to include deep learning topics as well, however the length has gotten too long hence they’ll be covered in a separate post.</p>
<p>However if you need a quick tutorial on them, you can follow <a href="https://www.kaggle.com/learn">the kaggle courses</a>.</p>
<p>Second place to grokk deep learning is reading and running public kernels on kaggle. Third place is the <a href="https://keras.io/examples/">official documentation</a> of libraries like keras.</p>
<p>Deep learning is a very empirical field where practice and theory go hand-in-hand. The traditional serial approach of reading up theory and then taking up examples will not work for deep learning, they have to go together. It is best if, as a beginner, you don’t go too deep into theoretical aspects of deep learning before getting hands-on experience with their applications.</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>In order to avoid the problem of plenty, I will keep this list as short as possible.</p>
<ol type="1">
<li><p>Book - Introduction to Machine Learning by Andreas Muller. This covers theory and practice at the same time while being beginner friendly.</p></li>
<li><p>Course - Most courses like that of Andrew Ng are quite heavy on mathematics and the “backend” of ML algorithms. I personally think this is not a good place to start for a newcomer. However if you have a background in STEM, you might like this approach of thinking from first principles.</p></li>
<li><p>Kaggle Learn - Most people know kaggle for building models and competitions. But kaggle also has a courses section where there are easy-to-follow and brief tutorials on various topics like pandas, data visualization, etc. They are fantastic for a beginner.</p></li>
<li><p>Kaggle Notebooks - Spending time here will teach you about almost everything about model building.</p></li>
</ol>
<p>This is all for this beginner’s notebook. Good luck to your machine learning journey!</p>


</section>

 ]]></description>
  <category>getting-started</category>
  <category>data-science</category>
  <guid>https://shindeshu.github.io/posts/roadmap/start.html</guid>
  <pubDate>Sat, 14 Jan 2023 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/roadmap/assets/robot_learning.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A Friendly ☺️ Introduction to CNNs with Keras</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/tutorials/keras_cnns_intro.html</link>
  <description><![CDATA[ 



<section id="computer-vision" class="level2">
<h2 class="anchored" data-anchor-id="computer-vision">Computer Vision</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/tutorials/assets/computer_vision.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Computer Vision</figcaption><p></p>
</figure>
</div>
<p>This was originally a <a href="https://www.kaggle.com/code/shindeshubham85/a-friendly-introduction-to-cnns-in-keras"><img src="https://shindeshu.github.io/posts/tutorials/assets/kaggle.png" class="img-fluid" style="width:10.0%"></a> notebook.</p>
<p>In a nutshell, CNNs are special deep learning architectures that have revolutionized the field of computer vision. Computer Vision is a field that is concerned about deriving information from images using computers. Some examples of computer vision include- Identifying whether a given image contains an item, like identifying a pedestrian in traffic, identifying cracks in an industrial machine, identifying if an X-Ray is abnormal. Self-Driving cars rely on computer vision algorithms, which are often CNNs. Filters on Instagram, face recognition systems, all use deep learning under the hood.</p>
<p>But some general types of computer vision problems are:</p>
<ul>
<li>Image Classification (Is this image of a cat?)</li>
<li>Object Detection (Is there a cat in this image, and WHERE exactly is it?)</li>
<li>Segmentation (give me the exact outline of the cat in this image, if it exists)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/tutorials/assets/tasks.png" class="img-fluid figure-img" style="width:40.0%"></p>
<p></p><figcaption class="figure-caption">Computer Vision Tasks</figcaption><p></p>
</figure>
</div>
<section id="difference-from-traditional-modeling" class="level3">
<h3 class="anchored" data-anchor-id="difference-from-traditional-modeling">Difference from Traditional Modeling</h3>
<p>If you’re here, it probably means you’re familiar with basic machine learning concepts- like training data, predictions, feature engineering, etc. on tabular data. However image data is different from tabular data due to having a different structure, hence traditional algorithms like random forest cannot be used for classifying images.</p>
</section>
<section id="the-cnn-breakthrough" class="level3">
<h3 class="anchored" data-anchor-id="the-cnn-breakthrough">The CNN Breakthrough</h3>
<p>This is where CNNs made the breakthrough, and achieved tremendous results on image data. Before CNNs, image analytics required a lot of feature engineering and pre-processing (tons of hand-made filters). CNNs outperformed all the traditional methods without requiring such feature engineering. CNNs learnt the features and filters by itself. All you had to do was feed a lot of data to the model.</p>
</section>
</section>
<section id="deep-learning-and-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="deep-learning-and-neural-networks">Deep Learning and Neural Networks</h2>
<p>CNN, that is, Convolutional Neural Networks are a subfield of neural networks, a family of algorithms. A neural network is a collection of nodes or neurons, where each neuron has a weight*. These weights are learnt during the training process such that the model is able to predict the output when input is given. When a lot of such neurons are stacked together, we get a neural network. A neural network with a lot of layers would be called deep neural network, a phenomenon which has driven majority of the AI success in the last decade.</p>
<p>In CNNs, the neurons are arranged and stacked in a manner suitable for images.</p>
<section id="convolutional-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="convolutional-neural-networks">Convolutional Neural Networks</h3>
<p>In CNN, we have filters (which are tiny 3x3 matrices) which “convolve” an image to get a transformed matrix. We won’t worry about the theory and filters here. All you need to know, that filters transform the image to a new matrix. This matrix is made smaller by a method called Pooling. These two operations create one Convolution Layer, and several such Layers create a CNN. This order isn’t mandatory, as we’ll see later.</p>
<p>This is a <a href="https://www.youtube.com/watch?v=YRhxdVk_sIs">nice animation</a> that showcases the convolution operation.</p>
<div data-align="center">
<iframe align="middle" width="790" height="440" src="https://www.youtube.com/embed/YRhxdVk_sIs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
</div>
</section>
<section id="transfer-learning" class="level3">
<h3 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h3>
<p>There’s a neat trick in deep learning called transfer learning- which is covered at the end of the notebook in case you make it.</p>
<p>That’s quite a lot of theory, on to the problem at hand.</p>
</section>
</section>
<section id="problem-at-hand" class="level2">
<h2 class="anchored" data-anchor-id="problem-at-hand">Problem At Hand</h2>
<p>The task at hand is an image classification task. You’re given a ton of images that are either a cat image or a dog image. Now, if you give a new image, you should be able to predict if it’s of a dog or a cat.</p>
<p>We are going to train a CNN to do this. Using the keras library.</p>
</section>
<section id="typical-workflow" class="level2">
<h2 class="anchored" data-anchor-id="typical-workflow">Typical Workflow</h2>
<p>Typically when you work on a CNN task, this is how your notebook flow will look like: Whatever time you spend with CNNs, it will be in one of these sections.</p>
<ol type="1">
<li><strong>Get the Images</strong></li>
</ol>
<p>(collecting images itself can be either a herculean task or sometimes ready-made data is available, time and effort varies with dataset)</p>
<p>Difficulty Level: Varies Time Needed: Varies</p>
<ol start="2" type="1">
<li><strong>Look at the Images and the Targets</strong></li>
</ol>
<p>(see how the images actually look like, what are the classes, how many of them.)</p>
<ol start="3" type="1">
<li><strong>Create a Data Loader</strong></li>
</ol>
<p>(in most libraries you need a guy that reads the images and feeds to the model, and does the intermediate work- batching, augmentation, split, multiprocessing, etc. configuring this step will be a good chunk of your time )</p>
<ol start="4" type="1">
<li><strong>Define Your Model</strong></li>
</ol>
<p>(how many CNN layers? How many filters, the optimizer, the loss function? this could be as easy as downloading/pasting an existing model in ten minutes, or the experiments could go on forever)</p>
<ol start="5" type="1">
<li><strong>Train the Model</strong></li>
</ol>
<p>(now throw the dataloader function on the model and let it train. sit back and sip coffee.)</p>
<ol start="6" type="1">
<li><strong>Get The Predictions</strong></li>
</ol>
<p>(here you actually use the model. for some task, or just to check if it’s doing good. evaluating whether the model is giving good predictions can also be challenging in some use cases.)</p>
<ol start="7" type="1">
<li><strong>Debugging</strong></li>
</ol>
<p>If you keep it simple, load pre-built modules, the model will work. But there could be many possible problems that might arise in the task. These will be covered at the end.</p>
<p>::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-01-07T11:37:46.774008Z”,“iopub.status.busy”:“2023-01-07T11:37:46.773068Z”,“iopub.status.idle”:“2023-01-07T11:37:52.956681Z”,“shell.execute_reply”:“2023-01-07T11:37:52.955647Z”}’ papermill=‘{“duration”:6.19509,“end_time”:“2023-01-07T11:37:52.959543”,“exception”:false,“start_time”:“2023-01-07T11:37:46.764453”,“status”:“completed”}’ tags=‘[]’ execution_count=1}</p>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd </span>
<span id="cb1-3"><span class="im" style="color: #00769E;">from</span> tensorflow.keras.preprocessing.image <span class="im" style="color: #00769E;">import</span> ImageDataGenerator, load_img</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">from</span> tensorflow.keras.utils <span class="im" style="color: #00769E;">import</span> plot_model</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">from</span> tensorflow.keras.utils <span class="im" style="color: #00769E;">import</span> to_categorical</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;">import</span> train_test_split</span>
<span id="cb1-7"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-8"><span class="im" style="color: #00769E;">import</span> random</span>
<span id="cb1-9"><span class="im" style="color: #00769E;">import</span> os</span>
<span id="cb1-10"><span class="im" style="color: #00769E;">from</span> PIL <span class="im" style="color: #00769E;">import</span> Image</span>
<span id="cb1-11"><span class="im" style="color: #00769E;">import</span> glob</span></code></pre></div>
<p>:::</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:37:52.977159Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:37:52.975428Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:37:53.924902Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:37:53.923709Z&quot;}" data-papermill="{&quot;duration&quot;:0.960419,&quot;end_time&quot;:&quot;2023-01-07T11:37:53.927312&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:37:52.966893&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="op" style="color: #5E5E5E;">!</span>ls ..<span class="op" style="color: #5E5E5E;">/</span><span class="bu" style="color: null;">input</span><span class="op" style="color: #5E5E5E;">/</span>dogs<span class="op" style="color: #5E5E5E;">-</span>vs<span class="op" style="color: #5E5E5E;">-</span>cats<span class="op" style="color: #5E5E5E;">/</span></span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="co" style="color: #5E5E5E;"># the images are in a zip file in this folder</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>sampleSubmission.csv  test1.zip  train.zip</code></pre>
</div>
</div>
</section>
<section id="get-the-images" class="level2">
<h2 class="anchored" data-anchor-id="get-the-images">1. Get the Images</h2>
<p>Extract the images from the zip file. Now there are two folders: train/ and test1/</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:37:53.959044Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:37:53.958255Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:38:10.454469Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:38:10.453237Z&quot;}" data-papermill="{&quot;duration&quot;:16.507658,&quot;end_time&quot;:&quot;2023-01-07T11:38:10.457557&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:37:53.949899&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;">import</span> zipfile</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="cf" style="color: #003B4F;">with</span> zipfile.ZipFile(<span class="st" style="color: #20794D;">"../input/dogs-vs-cats/train.zip"</span>,<span class="st" style="color: #20794D;">"r"</span>) <span class="im" style="color: #00769E;">as</span> z:</span>
<span id="cb4-4">    z.extractall(<span class="st" style="color: #20794D;">"."</span>)</span>
<span id="cb4-5">    </span>
<span id="cb4-6"><span class="cf" style="color: #003B4F;">with</span> zipfile.ZipFile(<span class="st" style="color: #20794D;">"../input/dogs-vs-cats/test1.zip"</span>,<span class="st" style="color: #20794D;">"r"</span>) <span class="im" style="color: #00769E;">as</span> z:</span>
<span id="cb4-7">    z.extractall(<span class="st" style="color: #20794D;">"."</span>)</span>
<span id="cb4-8"><span class="op" style="color: #5E5E5E;">!</span>ls <span class="op" style="color: #5E5E5E;">/</span>kaggle<span class="op" style="color: #5E5E5E;">/</span>working<span class="op" style="color: #5E5E5E;">/</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>__notebook__.ipynb  test1  train</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:38:10.480187Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:38:10.479766Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:38:10.598919Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:38:10.597850Z&quot;}" data-papermill="{&quot;duration&quot;:0.133456,&quot;end_time&quot;:&quot;2023-01-07T11:38:10.601783&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:38:10.468327&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="co" style="color: #5E5E5E;"># let's look at any five files in the train folder</span></span>
<span id="cb6-2">glob.glob(<span class="st" style="color: #20794D;">"/kaggle/working/train/*.jpg"</span>)[:<span class="dv" style="color: #AD0000;">5</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>['/kaggle/working/train/dog.8701.jpg',
 '/kaggle/working/train/dog.9481.jpg',
 '/kaggle/working/train/cat.104.jpg',
 '/kaggle/working/train/cat.8800.jpg',
 '/kaggle/working/train/dog.6199.jpg']</code></pre>
</div>
</div>
<p>You now notice that the label is encoded in the filename itself- cat or dog! We need to extract that to be able to train the model.</p>
<p>We are collecting each filename, and the corresponding label in a pandas dataframe (needed later)</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:38:10.645446Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:38:10.645005Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:38:10.800698Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:38:10.799509Z&quot;}" data-papermill="{&quot;duration&quot;:0.169767,&quot;end_time&quot;:&quot;2023-01-07T11:38:10.803124&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:38:10.633357&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">train_files <span class="op" style="color: #5E5E5E;">=</span> glob.glob(<span class="st" style="color: #20794D;">"/kaggle/working/train/*.jpg"</span>)</span>
<span id="cb8-2">train_labels <span class="op" style="color: #5E5E5E;">=</span> [i.strip(<span class="st" style="color: #20794D;">'/kaggle/working/train/'</span>)[:<span class="dv" style="color: #AD0000;">3</span>] <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> train_files]</span>
<span id="cb8-3">train_df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame({<span class="st" style="color: #20794D;">'filename'</span>: train_files, <span class="st" style="color: #20794D;">'class'</span>: train_labels})</span>
<span id="cb8-4">train_df.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>filename</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>/kaggle/working/train/dog.8701.jpg</td>
      <td>dog</td>
    </tr>
    <tr>
      <th>1</th>
      <td>/kaggle/working/train/dog.9481.jpg</td>
      <td>dog</td>
    </tr>
    <tr>
      <th>2</th>
      <td>/kaggle/working/train/cat.104.jpg</td>
      <td>cat</td>
    </tr>
    <tr>
      <th>3</th>
      <td>/kaggle/working/train/cat.8800.jpg</td>
      <td>cat</td>
    </tr>
    <tr>
      <th>4</th>
      <td>/kaggle/working/train/dog.6199.jpg</td>
      <td>dog</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>First step is done- we have the images, and we have the labels. Let’s move to the second step.</p>
</section>
<section id="look-at-them" class="level2">
<h2 class="anchored" data-anchor-id="look-at-them">2. Look at Them!</h2>
<p>We now observe what the images look like. We’ll look at four random images from the data.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:38:10.848325Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:38:10.848020Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:38:11.537208Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:38:11.536377Z&quot;}" data-papermill="{&quot;duration&quot;:0.704933,&quot;end_time&quot;:&quot;2023-01-07T11:38:11.544590&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:38:10.839657&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">fig, axs <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>, figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">10</span>,<span class="dv" style="color: #AD0000;">10</span>))</span>
<span id="cb9-2">axs <span class="op" style="color: #5E5E5E;">=</span> axs.ravel()</span>
<span id="cb9-3"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">4</span>):</span>
<span id="cb9-4">    idx <span class="op" style="color: #5E5E5E;">=</span> random.choice(train_df.index)</span>
<span id="cb9-5">    axs[i].imshow(Image.<span class="bu" style="color: null;">open</span>(train_df[<span class="st" style="color: #20794D;">'filename'</span>][idx]))</span>
<span id="cb9-6">    axs[i].set_title(train_df[<span class="st" style="color: #20794D;">'class'</span>][idx])</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://shindeshu.github.io/posts/tutorials/keras_cnns_intro_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This is a pretty clean dataset- that’s good. The subjects of the image are in center, occupy a majority of the image, no blurriness or anything. (Ideally you’d look at more images than a dozen tho,:) )</p>
<p>You might’ve noticed that the dimensions of the images are not constant, they vary a lot. This will be a problem for the CNN- it expects images of a fixed size! How do we take care of it? We don’t, our data loader guy will do it for us.</p>
<p>(Well, sometimes we might want to do it ourselves, if we think the resizing needs extra attention- but in this case, we’ll let it be automated.)</p>
</section>
<section id="release-the-loader" class="level2">
<h2 class="anchored" data-anchor-id="release-the-loader">3. Release the Loader!</h2>
<p>Dataloaders are the unsung heroes of the CNN world- they take care of a lot of nasty work that would be a nightmare if we had to do it by hand. What they essentially do is simple- read the data from the disk, and feed to the model. But under the hood they take care of many things, like…</p>
<ol type="1">
<li>Resizing, as we discussed. Ensuring that each image is of a fixed size.</li>
<li>Batching. Feeding images one-by-one to the model is tedious, would take a lot of time. It’s better to feed a large number of them at once (as much your computer will allow)</li>
<li>Label Encoding. Computers don’t understand string like ‘cat’ or ‘dog’, you have to convert them to numbers like 0 or 1.</li>
<li>Data Augmentation. Create more images by slightly modifying an image (flipping it, rotating it teeny bit, adding some spots, etc.)</li>
<li>Validation Split. Keras now has support for validation splitting.</li>
<li>Sometimes the data is too large to fit into memory (10+ GB, say), then loaders can iterate through the dataset on disk chunk-by-chunk instead of loading everything at once.</li>
</ol>
<p>We are using keras’s <code>ImageDataGenerator</code> to create our training data loader.</p>
<p>Two steps:</p>
<ol type="1">
<li>Define a <code>ImageDataGenerator</code> instance, and specify the augmentation strategies.</li>
<li>Create a generator from this instance by specifying the image file paths and labels. Pass this generator to the model for training.</li>
</ol>
<p>In pytorch there’s <code>torch.utils.data.Dataset</code> and<code>torch.utils.data.DataLoader</code>. Sometimes you may need to define a custom dataloader, but the default is good enough for most use cases.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:38:11.626785Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:38:11.626434Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:38:11.632274Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:38:11.631254Z&quot;}" data-papermill="{&quot;duration&quot;:0.022781,&quot;end_time&quot;:&quot;2023-01-07T11:38:11.634597&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:38:11.611816&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">train_datagen <span class="op" style="color: #5E5E5E;">=</span> ImageDataGenerator(</span>
<span id="cb10-2">    rotation_range<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>,</span>
<span id="cb10-3">    rescale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1.</span><span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">255</span>,</span>
<span id="cb10-4">    horizontal_flip<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>,</span>
<span id="cb10-5">    shear_range<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>,</span>
<span id="cb10-6">    zoom_range<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>,</span>
<span id="cb10-7">    validation_split<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>)</span>
<span id="cb10-8"></span>
<span id="cb10-9"><span class="co" style="color: #5E5E5E;"># create a image data generator object. </span></span>
<span id="cb10-10"><span class="co" style="color: #5E5E5E;"># all these are data augmentation parameters.</span></span>
<span id="cb10-11"></span>
<span id="cb10-12"><span class="co" style="color: #5E5E5E;"># now let's specify the image size to which each image will be resized to</span></span>
<span id="cb10-13">img_height, img_width <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">224</span>, <span class="dv" style="color: #AD0000;">224</span></span>
<span id="cb10-14">batch_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">64</span></span></code></pre></div>
</div>
<p>There are two ways to create data generators/loaders from above instance. I recommend going through the <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator">ImageDataGenerator API page</a>, but the two methods are:</p>
<ol type="1">
<li>Flow from Dataframe (here, you can contain the filenames and labels in a pandas dataframe, and pass the dataframe), we are using this, remember the dataframe we created earlier?</li>
<li>Flow from Directory (here, you can pass the path of a directory. This directory should contain subfolders corresponding to each class. You will have to rearrange your directory so that it looks like this.</li>
</ol>
<pre><code>Train/
---| Dog/
   ---| Dog1.jpg
   ---| Dog1.jpg
---| Cat/
   ---| Cat1.jpg
   ---| Cat2.jpg
Val/
---| Dog/
   ---| Dog5.jpg
   ---| Dog6.jpg
---| Cat/
   ---| Cat7.jpg
   ---| Cat8.jpg</code></pre>
<p>Since we are using method1, we will not be rearranging the folders.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:38:11.689506Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:38:11.689194Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:38:12.066046Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:38:12.064842Z&quot;}" data-papermill="{&quot;duration&quot;:0.393957,&quot;end_time&quot;:&quot;2023-01-07T11:38:12.068358&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:38:11.674401&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">train_generator <span class="op" style="color: #5E5E5E;">=</span> train_datagen.flow_from_dataframe(</span>
<span id="cb12-2">    train_df,</span>
<span id="cb12-3">    target_size<span class="op" style="color: #5E5E5E;">=</span>(img_height, img_width),</span>
<span id="cb12-4">    batch_size<span class="op" style="color: #5E5E5E;">=</span>batch_size,</span>
<span id="cb12-5">    class_mode<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'categorical'</span>,</span>
<span id="cb12-6">    subset<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'training'</span>) <span class="co" style="color: #5E5E5E;"># set as training data</span></span>
<span id="cb12-7"></span>
<span id="cb12-8"><span class="co" style="color: #5E5E5E;"># remember we put 0.2 validation split while defining ImageDataGenerator?</span></span>
<span id="cb12-9">validation_generator <span class="op" style="color: #5E5E5E;">=</span> train_datagen.flow_from_dataframe(</span>
<span id="cb12-10">    train_df, <span class="co" style="color: #5E5E5E;"># same directory as training data</span></span>
<span id="cb12-11">    target_size<span class="op" style="color: #5E5E5E;">=</span>(img_height, img_width),</span>
<span id="cb12-12">    batch_size<span class="op" style="color: #5E5E5E;">=</span>batch_size,</span>
<span id="cb12-13">    class_mode<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'categorical'</span>,</span>
<span id="cb12-14">    subset<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'validation'</span>) <span class="co" style="color: #5E5E5E;"># set as validation data</span></span>
<span id="cb12-15"></span>
<span id="cb12-16"><span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Class Indices:"</span>, train_generator.class_indices)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 20000 validated image filenames belonging to 2 classes.
Found 5000 validated image filenames belonging to 2 classes.
Class Indices: {'cat': 0, 'dog': 1}</code></pre>
</div>
</div>
</section>
<section id="define-a-model" class="level2">
<h2 class="anchored" data-anchor-id="define-a-model">4. Define A Model</h2>
<p>Now we come to the meaty part- defining the CNN network, the engine of our application. As we discussed earlier, we are going to define a series of Convolution Layers, and each convolution layer consists of a convolution operation followed by a max pool layer.</p>
<p>This order is not mandatory- you can have two Convolution operations followed by one max pool, or three, or an average pool- that’s the neat part of neural networks, they’re so adaptable and malleable, and the best configurations are often found out by trial and error. In this case, we are going with the wisdom of our elders, and go by this order.</p>
<section id="head-of-a-cnn" class="level3">
<h3 class="anchored" data-anchor-id="head-of-a-cnn">Head of A CNN</h3>
<p>As we apply more Conv Layers, you will get a transformed matrix of somesize x somesize. But what good is a matrix to us? We need a simple answer- 0 or 1! In order to get this answer, we “flatten” the final matrix to a single vector of size somesize-times-2 x 1. Then we pass it through more neural network neurons to get a single neuron at the end. This neuron’s output is constrained between 0 and 1. This is our final probability! If it’s greater than 0.5, the prediction is 1, if not, it’s 0.</p>
<p>If you have more than 2 classes, like predicting a digit. In this case, there would be ten neurons at the end. Each of their output would be the probability of that class.</p>
</section>
<section id="model-hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="model-hyperparameters">Model Hyperparameters</h3>
<p>Apart from number of Conv layers, there are other design choices while designing a CNN- they include selecting the optimizer, the learning rate, the loss function, the number of filters. For an introductory notebook, discussion on those is not necessary.</p>
<p>What’s important to note: passing the input shape to the first layer. Ensuring that the last layer corresponds to the number of classes.</p>
<p>Try tinkering with this configuration to see how the results change. Try using only one Conv Layer, reducing number of filters, increasing number of filters, etc.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:38:12.123985Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:38:12.123655Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:38:15.006157Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:38:15.004003Z&quot;}" data-papermill="{&quot;duration&quot;:2.899066,&quot;end_time&quot;:&quot;2023-01-07T11:38:15.008260&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:38:12.109194&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;">from</span> keras.models <span class="im" style="color: #00769E;">import</span> Sequential</span>
<span id="cb14-2"><span class="im" style="color: #00769E;">from</span> keras.layers <span class="im" style="color: #00769E;">import</span> Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization</span>
<span id="cb14-3"></span>
<span id="cb14-4">model <span class="op" style="color: #5E5E5E;">=</span> Sequential()</span>
<span id="cb14-5"></span>
<span id="cb14-6">model.add(Conv2D(<span class="dv" style="color: #AD0000;">32</span>, (<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>), activation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'relu'</span>, </span>
<span id="cb14-7">                 input_shape<span class="op" style="color: #5E5E5E;">=</span>(img_width, img_height, <span class="dv" style="color: #AD0000;">3</span>)))</span>
<span id="cb14-8"><span class="co" style="color: #5E5E5E;"># 32 here means this layer will contain 32 filters of size 3x3 being learnt</span></span>
<span id="cb14-9">model.add(BatchNormalization())</span>
<span id="cb14-10"><span class="co" style="color: #5E5E5E;"># batchnorm is a useful layer that helps in convergence</span></span>
<span id="cb14-11">model.add(MaxPooling2D(pool_size<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)))</span>
<span id="cb14-12"><span class="co" style="color: #5E5E5E;"># maxpooling will reduce the size of the image</span></span>
<span id="cb14-13">model.add(Dropout(<span class="fl" style="color: #AD0000;">0.25</span>))</span>
<span id="cb14-14"><span class="co" style="color: #5E5E5E;"># dropout is used for regularization, ensuring that model doesn't overfit</span></span>
<span id="cb14-15"></span>
<span id="cb14-16">model.add(Conv2D(<span class="dv" style="color: #AD0000;">64</span>, (<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>), activation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'relu'</span>))</span>
<span id="cb14-17">model.add(BatchNormalization())</span>
<span id="cb14-18">model.add(MaxPooling2D(pool_size<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)))</span>
<span id="cb14-19">model.add(Dropout(<span class="fl" style="color: #AD0000;">0.25</span>))</span>
<span id="cb14-20"></span>
<span id="cb14-21">model.add(Conv2D(<span class="dv" style="color: #AD0000;">128</span>, (<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">3</span>), activation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'relu'</span>))</span>
<span id="cb14-22">model.add(BatchNormalization())</span>
<span id="cb14-23">model.add(MaxPooling2D(pool_size<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)))</span>
<span id="cb14-24">model.add(Dropout(<span class="fl" style="color: #AD0000;">0.25</span>))</span>
<span id="cb14-25"></span>
<span id="cb14-26"><span class="co" style="color: #5E5E5E;"># convolutional block is complete. now on to defining the "head"</span></span>
<span id="cb14-27"></span>
<span id="cb14-28"><span class="co" style="color: #5E5E5E;"># first flatten the matrix to get a single array</span></span>
<span id="cb14-29">model.add(Flatten())</span>
<span id="cb14-30"><span class="co" style="color: #5E5E5E;"># adding a dense hidden layer of neurons</span></span>
<span id="cb14-31">model.add(Dense(<span class="dv" style="color: #AD0000;">512</span>, activation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'relu'</span>))</span>
<span id="cb14-32">model.add(BatchNormalization())</span>
<span id="cb14-33">model.add(Dropout(<span class="fl" style="color: #AD0000;">0.5</span>))</span>
<span id="cb14-34"><span class="co" style="color: #5E5E5E;"># finally the output layer with neurons=number of classes and softmax activation</span></span>
<span id="cb14-35">model.add(Dense(<span class="dv" style="color: #AD0000;">2</span>, activation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'softmax'</span>)) <span class="co" style="color: #5E5E5E;"># 2 because we have cat and dog classes</span></span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2023-01-07 11:38:12.212718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 11:38:12.342711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 11:38:12.343593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 11:38:12.345511: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-07 11:38:12.345912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 11:38:12.346654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 11:38:12.347302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 11:38:14.495879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 11:38:14.496731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 11:38:14.497497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 11:38:14.498130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -&gt; device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0</code></pre>
</div>
</div>
<p>Let’s visualize how our network looks like, and what the shapes of input and output at each layer.</p>
<p>The shapes of input and output can be useful for debugging. If there’s a mismatch between output of one layer and input of next, model will throw up error.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:38:15.065000Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:38:15.064096Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:38:15.999864Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:38:15.998734Z&quot;}" data-papermill="{&quot;duration&quot;:0.952978,&quot;end_time&quot;:&quot;2023-01-07T11:38:16.002149&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:38:15.049171&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">plot_model(model, show_shapes<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, show_layer_names<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>, dpi<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">60</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<p><img src="https://shindeshu.github.io/posts/tutorials/keras_cnns_intro_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:38:16.036028Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:38:16.034444Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T11:38:16.050816Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T11:38:16.049980Z&quot;}" data-papermill="{&quot;duration&quot;:0.034609,&quot;end_time&quot;:&quot;2023-01-07T11:38:16.052796&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:38:16.018187&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;"># compile the model while defining a loss, optimizer, and metrics to track, </span></span>
<span id="cb17-2"><span class="co" style="color: #5E5E5E;"># and add callbacks if necessary</span></span>
<span id="cb17-3">model.<span class="bu" style="color: null;">compile</span>(loss<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'categorical_crossentropy'</span>, </span>
<span id="cb17-4">              optimizer<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'adam'</span>, </span>
<span id="cb17-5">              metrics<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">'accuracy'</span>])</span></code></pre></div>
</div>
</section>
</section>
<section id="train-the-model" class="level2">
<h2 class="anchored" data-anchor-id="train-the-model">5. Train the Model</h2>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T11:38:16.085075Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T11:38:16.083469Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T12:36:10.273935Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T12:36:10.272894Z&quot;}" data-papermill="{&quot;duration&quot;:3474.20881,&quot;end_time&quot;:&quot;2023-01-07T12:36:10.276450&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T11:38:16.067640&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb18-2">history <span class="op" style="color: #5E5E5E;">=</span> model.fit(</span>
<span id="cb18-3">    train_generator, </span>
<span id="cb18-4">    epochs<span class="op" style="color: #5E5E5E;">=</span>epochs,</span>
<span id="cb18-5">    validation_data<span class="op" style="color: #5E5E5E;">=</span>validation_generator,</span>
<span id="cb18-6">)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2023-01-07 11:38:16.953970: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>2023-01-07 11:38:20.190526: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>313/313 [==============================] - 334s 1s/step - loss: 0.7747 - accuracy: 0.6353 - val_loss: 1.8673 - val_accuracy: 0.4876
Epoch 2/10
313/313 [==============================] - 324s 1s/step - loss: 0.5450 - accuracy: 0.7254 - val_loss: 0.5757 - val_accuracy: 0.7110
Epoch 3/10
313/313 [==============================] - 325s 1s/step - loss: 0.4821 - accuracy: 0.7671 - val_loss: 0.4726 - val_accuracy: 0.7732
Epoch 4/10
313/313 [==============================] - 326s 1s/step - loss: 0.4332 - accuracy: 0.7987 - val_loss: 0.4627 - val_accuracy: 0.7800
Epoch 5/10
313/313 [==============================] - 323s 1s/step - loss: 0.3989 - accuracy: 0.8184 - val_loss: 0.4141 - val_accuracy: 0.8128
Epoch 6/10
313/313 [==============================] - 322s 1s/step - loss: 0.3702 - accuracy: 0.8349 - val_loss: 0.4598 - val_accuracy: 0.7822
Epoch 7/10
313/313 [==============================] - 323s 1s/step - loss: 0.3409 - accuracy: 0.8484 - val_loss: 0.3489 - val_accuracy: 0.8442
Epoch 8/10
313/313 [==============================] - 324s 1s/step - loss: 0.3169 - accuracy: 0.8641 - val_loss: 0.3765 - val_accuracy: 0.8478
Epoch 9/10
313/313 [==============================] - 323s 1s/step - loss: 0.3124 - accuracy: 0.8629 - val_loss: 0.4741 - val_accuracy: 0.7906
Epoch 10/10
313/313 [==============================] - 324s 1s/step - loss: 0.2925 - accuracy: 0.8721 - val_loss: 0.6446 - val_accuracy: 0.7484</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T12:36:10.625999Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T12:36:10.625243Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T12:36:11.152627Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T12:36:11.151742Z&quot;}" data-papermill="{&quot;duration&quot;:0.703214,&quot;end_time&quot;:&quot;2023-01-07T12:36:11.154843&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T12:36:10.451629&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="kw" style="color: #003B4F;">def</span> plot_loss(history):</span>
<span id="cb23-2">    fig, (ax1, ax2) <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>, figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">6</span>, <span class="dv" style="color: #AD0000;">6</span>))</span>
<span id="cb23-3">    ax1.plot(history.history[<span class="st" style="color: #20794D;">'loss'</span>], color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'b'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Training loss"</span>)</span>
<span id="cb23-4">    ax1.plot(history.history[<span class="st" style="color: #20794D;">'val_loss'</span>], color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'r'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"validation loss"</span>)</span>
<span id="cb23-5">    ax1.set_xticks(np.arange(<span class="dv" style="color: #AD0000;">1</span>, epochs, <span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb23-6"></span>
<span id="cb23-7">    ax2.plot(history.history[<span class="st" style="color: #20794D;">'accuracy'</span>], color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'b'</span>, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Training accuracy"</span>)</span>
<span id="cb23-8">    ax2.plot(history.history[<span class="st" style="color: #20794D;">'val_accuracy'</span>], color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'r'</span>,label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Validation accuracy"</span>)</span>
<span id="cb23-9">    ax2.set_xticks(np.arange(<span class="dv" style="color: #AD0000;">1</span>, epochs, <span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb23-10"></span>
<span id="cb23-11">    legend <span class="op" style="color: #5E5E5E;">=</span> plt.legend(loc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'best'</span>, shadow<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb23-12">    plt.tight_layout()</span>
<span id="cb23-13">    plt.show()</span>
<span id="cb23-14">plot_loss(history)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://shindeshu.github.io/posts/tutorials/keras_cnns_intro_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="get-the-predictions" class="level2">
<h2 class="anchored" data-anchor-id="get-the-predictions">6. Get the Predictions</h2>
<p>Now that the model is trained, let’s check if the model is giving us good predictions, by trying it out on the test data.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T12:36:11.897834Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T12:36:11.897458Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T12:36:12.023708Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T12:36:12.022626Z&quot;}" data-papermill="{&quot;duration&quot;:0.353908,&quot;end_time&quot;:&quot;2023-01-07T12:36:12.026546&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T12:36:11.672638&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="14">
<div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">test_files <span class="op" style="color: #5E5E5E;">=</span> glob.glob(<span class="st" style="color: #20794D;">'/kaggle/working/test1/*.jpg'</span>)</span>
<span id="cb24-2">test_df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame({<span class="st" style="color: #20794D;">'filename'</span>: test_files})</span>
<span id="cb24-3">test_gen <span class="op" style="color: #5E5E5E;">=</span> ImageDataGenerator(rescale<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1.</span><span class="op" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">255</span>)</span>
<span id="cb24-4">test_generator <span class="op" style="color: #5E5E5E;">=</span> test_gen.flow_from_dataframe(</span>
<span id="cb24-5">    test_df, </span>
<span id="cb24-6">    x_col<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'filename'</span>,</span>
<span id="cb24-7">    y_col<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>,</span>
<span id="cb24-8">    class_mode<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">None</span>,</span>
<span id="cb24-9">    target_size<span class="op" style="color: #5E5E5E;">=</span>(img_height, img_width),</span>
<span id="cb24-10">    batch_size<span class="op" style="color: #5E5E5E;">=</span>batch_size,</span>
<span id="cb24-11">    shuffle<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 12500 validated image filenames.</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T12:36:12.374320Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T12:36:12.373921Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T12:36:19.003695Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T12:36:19.002898Z&quot;}" data-papermill="{&quot;duration&quot;:6.812598,&quot;end_time&quot;:&quot;2023-01-07T12:36:19.011415&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T12:36:12.198817&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="kw" style="color: #003B4F;">def</span> visualize_predictions(test_generator, model):</span>
<span id="cb26-2">    plt.figure(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">12</span>))</span>
<span id="cb26-3">    <span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">15</span>):</span>
<span id="cb26-4">        plt.subplot(<span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">3</span>, i<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb26-5">        <span class="cf" style="color: #003B4F;">for</span> X_batch <span class="kw" style="color: #003B4F;">in</span> test_generator:</span>
<span id="cb26-6">            prediction <span class="op" style="color: #5E5E5E;">=</span> model.predict(X_batch)[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb26-7">            image <span class="op" style="color: #5E5E5E;">=</span> X_batch[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb26-8">            plt.imshow(image)</span>
<span id="cb26-9">            plt.title(<span class="st" style="color: #20794D;">'cat'</span> <span class="cf" style="color: #003B4F;">if</span> np.argmax(prediction)<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span> <span class="cf" style="color: #003B4F;">else</span> <span class="st" style="color: #20794D;">"dog"</span>)</span>
<span id="cb26-10">            <span class="cf" style="color: #003B4F;">break</span></span>
<span id="cb26-11">    plt.tight_layout()</span>
<span id="cb26-12">    plt.show()</span>
<span id="cb26-13">visualize_predictions(test_generator, model)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://shindeshu.github.io/posts/tutorials/keras_cnns_intro_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="transfer-learning-1" class="level2">
<h2 class="anchored" data-anchor-id="transfer-learning-1">Transfer Learning</h2>
<p>If you made it here, we’ll talk about deep learning’s most important tricks- transfer learning!</p>
<p>Neural networks are notoriously data hungry- they can eat millions of images and digest them to be able to generalize upon their features. In this case, what if you don’t have millions of images?</p>
<p>In this case, you use a model that has been trained on millions of images. And take it as your starting point. And train your model from them. Those massive datasets don’t necessarily have to be related to your image classes.</p>
<p>There are many publicly available models like resnet, xception, convnext (particular architectures of CNNs) trained on ImageNet dataset (a very large image dataset with 100+ different classes). You can simply download them, and use it for your task (classifying dogs), and it will work much better than defining a model from new.</p>
<p>We’ll implement a model using transfer learning below.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T12:36:19.734242Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T12:36:19.733878Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T12:36:21.707074Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T12:36:21.706057Z&quot;}" data-papermill="{&quot;duration&quot;:2.157794,&quot;end_time&quot;:&quot;2023-01-07T12:36:21.710237&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T12:36:19.552443&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="16">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><span class="im" style="color: #00769E;">from</span> tensorflow.keras.applications <span class="im" style="color: #00769E;">import</span> ResNet50, Xception</span>
<span id="cb27-2"><span class="im" style="color: #00769E;">from</span> tensorflow.keras <span class="im" style="color: #00769E;">import</span> Input</span>
<span id="cb27-3"><span class="im" style="color: #00769E;">from</span> tensorflow.keras.models <span class="im" style="color: #00769E;">import</span> Model</span>
<span id="cb27-4"></span>
<span id="cb27-5"><span class="co" style="color: #5E5E5E;"># ResNet50 is our "backbone" of sorts, a CNN architecture </span></span>
<span id="cb27-6"><span class="co" style="color: #5E5E5E;"># pretrained on the imagenet dataset</span></span>
<span id="cb27-7"><span class="co" style="color: #5E5E5E;"># we are only taking the CNN portion of it (include_top = False)</span></span>
<span id="cb27-8"><span class="co" style="color: #5E5E5E;"># and dropping the dense layer, we'll initialize a dense network of our own</span></span>
<span id="cb27-9"></span>
<span id="cb27-10">basemodel <span class="op" style="color: #5E5E5E;">=</span> Xception(include_top <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span>, </span>
<span id="cb27-11">                   weights <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'imagenet'</span>,</span>
<span id="cb27-12">                  input_shape<span class="op" style="color: #5E5E5E;">=</span>(img_height, img_width, <span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb27-13"></span>
<span id="cb27-14">basemodel.trainable <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">False</span></span>
<span id="cb27-15"></span>
<span id="cb27-16"><span class="co" style="color: #5E5E5E;"># use the output of the baseModel to create a "head"</span></span>
<span id="cb27-17">headModel <span class="op" style="color: #5E5E5E;">=</span> basemodel.output</span>
<span id="cb27-18">headModel <span class="op" style="color: #5E5E5E;">=</span> MaxPooling2D(pool_size<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">5</span>))(headModel)</span>
<span id="cb27-19">headModel <span class="op" style="color: #5E5E5E;">=</span> Flatten(name<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"flatten"</span>)(headModel)</span>
<span id="cb27-20">headModel <span class="op" style="color: #5E5E5E;">=</span> Dense(<span class="dv" style="color: #AD0000;">128</span>, activation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"relu"</span>)(headModel)</span>
<span id="cb27-21">headModel <span class="op" style="color: #5E5E5E;">=</span> Dropout(<span class="fl" style="color: #AD0000;">0.8</span>)(headModel)</span>
<span id="cb27-22"><span class="co" style="color: #5E5E5E;"># headModel = Dense(32, activation="relu")(headModel)</span></span>
<span id="cb27-23"><span class="co" style="color: #5E5E5E;"># headModel = Dropout(0.5)(headModel)</span></span>
<span id="cb27-24">headModel <span class="op" style="color: #5E5E5E;">=</span> Dense(<span class="dv" style="color: #AD0000;">2</span>, activation<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"softmax"</span>)(headModel)</span>
<span id="cb27-25"><span class="co" style="color: #5E5E5E;"># at the end, we'll have two neurons, for two of the classes</span></span>
<span id="cb27-26"></span>
<span id="cb27-27"><span class="co" style="color: #5E5E5E;"># we're "disabling" the backbone, and only training the head for this task</span></span>
<span id="cb27-28"><span class="co" style="color: #5E5E5E;"># we're assuming that the backbone is already sufficiently trained to generate</span></span>
<span id="cb27-29"><span class="co" style="color: #5E5E5E;"># features from images like ours.</span></span>
<span id="cb27-30"><span class="co" style="color: #5E5E5E;"># we can also "disable" all CNN layers except last 4</span></span>
<span id="cb27-31"></span>
<span id="cb27-32"><span class="co" style="color: #5E5E5E;"># create a model object</span></span>
<span id="cb27-33">model <span class="op" style="color: #5E5E5E;">=</span> Model(inputs<span class="op" style="color: #5E5E5E;">=</span>basemodel.<span class="bu" style="color: null;">input</span>, outputs<span class="op" style="color: #5E5E5E;">=</span>headModel)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5
83689472/83683744 [==============================] - 1s 0us/step
83697664/83683744 [==============================] - 1s 0us/step</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T12:36:22.271148Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T12:36:22.270579Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T12:36:22.275448Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T12:36:22.274642Z&quot;}" data-papermill="{&quot;duration&quot;:0.294383,&quot;end_time&quot;:&quot;2023-01-07T12:36:22.279392&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T12:36:21.985009&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="17">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="co" style="color: #5E5E5E;"># plot_model(model, show_shapes=True, show_layer_names=False, dpi=60)</span></span></code></pre></div>
</div>
</section>
<section id="experiment-tracking" class="level2">
<h2 class="anchored" data-anchor-id="experiment-tracking">Experiment Tracking</h2>
<p>In earlier case, we plotted our losses and metrics after the training was done, manually using matplotlib. But there are tools available using which we can observe real-time how our training is progressing. They also log system metrics like GPU usage, and can keep track of multiple experiments, hyperparameters etc. One such tool is wandb.ai, using which you can track your model even on phone as it’s running in background.</p>
<p>Using it is very simple, signup on <a href="wandb.ai">wandb.ai</a>, and add only few lines of code. Get the API token, and go through <a href="https://www.kaggle.com/general/209530">this discussion</a> on how to add it as a kaggle secret.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T12:36:23.178436Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T12:36:23.178088Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T12:36:25.450810Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T12:36:25.449820Z&quot;}" data-papermill="{&quot;duration&quot;:2.457549,&quot;end_time&quot;:&quot;2023-01-07T12:36:25.453033&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T12:36:22.995484&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="18">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1">use_wandb <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">True</span> <span class="co" style="color: #5E5E5E;"># set to false if you don't want to use wandb for tracking</span></span>
<span id="cb30-2"><span class="cf" style="color: #003B4F;">if</span> use_wandb:</span>
<span id="cb30-3">    <span class="im" style="color: #00769E;">from</span> kaggle_secrets <span class="im" style="color: #00769E;">import</span> UserSecretsClient</span>
<span id="cb30-4">    <span class="im" style="color: #00769E;">import</span> wandb</span>
<span id="cb30-5">    <span class="im" style="color: #00769E;">from</span> wandb.keras <span class="im" style="color: #00769E;">import</span> WandbCallback</span>
<span id="cb30-6">    user_secrets <span class="op" style="color: #5E5E5E;">=</span> UserSecretsClient()</span>
<span id="cb30-7">    wandb_api <span class="op" style="color: #5E5E5E;">=</span> user_secrets.get_secret(<span class="st" style="color: #20794D;">"wandb_api"</span>)</span>
<span id="cb30-8">    wandb.login(key<span class="op" style="color: #5E5E5E;">=</span>wandb_api)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>wandb: W&amp;B API key is configured. Use `wandb login --relogin` to force relogin
wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.
wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.
wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc</code></pre>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T12:36:25.823097Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T12:36:25.822725Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T12:36:32.713179Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T12:36:32.712167Z&quot;}" data-papermill="{&quot;duration&quot;:7.080441,&quot;end_time&quot;:&quot;2023-01-07T12:36:32.715909&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T12:36:25.635468&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="19">
<div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><span class="cf" style="color: #003B4F;">if</span> use_wandb:</span>
<span id="cb32-2">    wandb.init(project<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"keras_cats_and_dogs"</span>, config<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"batch_size"</span>: batch_size})</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>wandb: Currently logged in as: shindeshu. Use `wandb login --relogin` to force relogin</code></pre>
</div>
<div class="cell-output cell-output-display">
wandb version 0.13.7 is available!  To upgrade, please run:
 $ pip install wandb --upgrade
</div>
<div class="cell-output cell-output-display">
Tracking run with wandb version 0.12.21
</div>
<div class="cell-output cell-output-display">
Run data is saved locally in <code>/kaggle/working/wandb/run-20230107_123626-2bw5qpbs</code>
</div>
<div class="cell-output cell-output-display">
Syncing run <strong><a href="https://wandb.ai/shindeshu/keras_cats_and_dogs/runs/2bw5qpbs" target="_blank">fancy-plant-6</a></strong> to <a href="https://wandb.ai/shindeshu/keras_cats_and_dogs" target="_blank">Weights &amp; Biases</a> (<a href="https://wandb.me/run" target="_blank">docs</a>)<br>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T12:36:33.344004Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T12:36:33.343415Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T12:36:33.365175Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T12:36:33.364327Z&quot;}" data-papermill="{&quot;duration&quot;:0.366902,&quot;end_time&quot;:&quot;2023-01-07T12:36:33.367902&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T12:36:33.001000&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="20">
<div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><span class="im" style="color: #00769E;">from</span> tensorflow.keras.optimizers <span class="im" style="color: #00769E;">import</span> Adam</span>
<span id="cb34-2"><span class="im" style="color: #00769E;">from</span> tensorflow.keras.callbacks <span class="im" style="color: #00769E;">import</span> EarlyStopping</span>
<span id="cb34-3">opt <span class="op" style="color: #5E5E5E;">=</span> Adam(learning_rate<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.001</span>)</span>
<span id="cb34-4">model.<span class="bu" style="color: null;">compile</span>(loss<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'categorical_crossentropy'</span>, </span>
<span id="cb34-5">              optimizer<span class="op" style="color: #5E5E5E;">=</span>opt, </span>
<span id="cb34-6">              metrics<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">'accuracy'</span>])</span></code></pre></div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T12:36:33.938057Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T12:36:33.937490Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T13:06:34.195009Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T13:06:34.194053Z&quot;}" data-papermill="{&quot;duration&quot;:1800.546907,&quot;end_time&quot;:&quot;2023-01-07T13:06:34.197419&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T12:36:33.650512&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="21">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1">epochs<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span></span>
<span id="cb35-2">callbacks<span class="op" style="color: #5E5E5E;">=</span>[EarlyStopping(monitor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'loss'</span>, patience<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>), ]</span>
<span id="cb35-3"><span class="cf" style="color: #003B4F;">if</span> use_wandb: callbacks.append(WandbCallback())</span>
<span id="cb35-4"></span>
<span id="cb35-5">history <span class="op" style="color: #5E5E5E;">=</span> model.fit_generator(</span>
<span id="cb35-6">    train_generator,</span>
<span id="cb35-7">    epochs<span class="op" style="color: #5E5E5E;">=</span>epochs,</span>
<span id="cb35-8">    validation_data<span class="op" style="color: #5E5E5E;">=</span>validation_generator,</span>
<span id="cb35-9">    callbacks<span class="op" style="color: #5E5E5E;">=</span>callbacks,</span>
<span id="cb35-10">)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>wandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&amp;B files and the SavedModel as W&amp;B Artifacts.
/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.
  warnings.warn('`Model.fit_generator` is deprecated and '
2023-01-07 12:36:46.761375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 12:36:46.761921: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 1
2023-01-07 12:36:46.762185: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session
2023-01-07 12:36:46.765305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 12:36:46.765841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 12:36:46.766331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 12:36:46.766954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 12:36:46.767464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-01-07 12:36:46.767852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -&gt; device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0
2023-01-07 12:36:46.787714: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize
  function_optimizer: function_optimizer did nothing. time = 0.01ms.
  function_optimizer: function_optimizer did nothing. time = 0.002ms.
</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/5
313/313 [==============================] - 350s 1s/step - loss: 0.1106 - accuracy: 0.9637 - val_loss: 0.0460 - val_accuracy: 0.9844</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
  category=CustomMaskWarning)</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/5
313/313 [==============================] - 335s 1s/step - loss: 0.0764 - accuracy: 0.9768 - val_loss: 0.0479 - val_accuracy: 0.9828
Epoch 3/5
313/313 [==============================] - 334s 1s/step - loss: 0.0690 - accuracy: 0.9771 - val_loss: 0.0380 - val_accuracy: 0.9844
Epoch 4/5
313/313 [==============================] - 331s 1s/step - loss: 0.0626 - accuracy: 0.9792 - val_loss: 0.0399 - val_accuracy: 0.9854
Epoch 5/5
313/313 [==============================] - 337s 1s/step - loss: 0.0629 - accuracy: 0.9777 - val_loss: 0.0380 - val_accuracy: 0.9836</code></pre>
</div>
</div>
</section>
<section id="massive-improvement-using-transfer-learning" class="level2">
<h2 class="anchored" data-anchor-id="massive-improvement-using-transfer-learning">Massive Improvement Using Transfer Learning</h2>
<p>When we defined a custom model earlier, the best validation accuracy we got after 10 epochs was <strong>85%</strong>.</p>
<p>Here, by using a pre-trained model, our validation accuracy after 1 epoch is as high as <strong>98%</strong>!</p>
<p>As we can see, using a pre-trained model can really boost our performance with minimal training efforts.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T13:06:35.271404Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T13:06:35.271028Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T13:06:35.565654Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T13:06:35.564802Z&quot;}" data-papermill="{&quot;duration&quot;:0.561439,&quot;end_time&quot;:&quot;2023-01-07T13:06:35.567660&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T13:06:35.006221&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="22">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">plot_loss(history)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://shindeshu.github.io/posts/tutorials/keras_cnns_intro_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-01-07T13:06:36.099744Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-01-07T13:06:36.099399Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-01-07T13:06:46.136502Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-01-07T13:06:46.135704Z&quot;}" data-papermill="{&quot;duration&quot;:10.306872,&quot;end_time&quot;:&quot;2023-01-07T13:06:46.141786&quot;,&quot;exception&quot;:false,&quot;start_time&quot;:&quot;2023-01-07T13:06:35.834914&quot;,&quot;status&quot;:&quot;completed&quot;}" data-tags="[]" data-execution_count="23">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">visualize_predictions(test_generator, model)</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://shindeshu.github.io/posts/tutorials/keras_cnns_intro_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>

 ]]></description>
  <category>getting-started</category>
  <category>deep-learning</category>
  <guid>https://shindeshu.github.io/posts/tutorials/keras_cnns_intro.html</guid>
  <pubDate>Sat, 14 Jan 2023 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/tutorials/assets/computer_vision.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Creating AI Art of Myself With Stable Diffusion + Dreambooth</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/stable_diff/getting_started_sd.html</link>
  <description><![CDATA[ 



<section id="what-is-stable-diffusion" class="level2">
<h2 class="anchored" data-anchor-id="what-is-stable-diffusion">What is Stable Diffusion</h2>
<p>For years, style transfer and image generation was the domain of an architecture called GANs. It produced results that were, well, not enough to impress a person not into the topic. And it look a lot of time, compute, and didn’t always converge. That is history anyway, so we do not need to trouble ourselves.</p>
<p>This year changed everything in generative AI. Three models, particularly Stable Diffusion took the world of art by storm. The text-to-image model could produce images with high fidelity, coherence, and able to produce images with so many styles. It was a 4GB checkpoint that contained in itself majority of the visual world created by man. With a bunch of text it could bring imagination to reality.</p>
</section>
<section id="the-magic" class="level2">
<h2 class="anchored" data-anchor-id="the-magic">The Magic</h2>
<p>All of these images are generated by AI completely on its own. There are some sites like <a href="lexica.art">lexica.art</a> where you can scroll through tons of AI-generated art like this, as well as the prompts that led to them.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/art_ex3.webp" style="height:40.0%" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Some Examples of AI generated art</figcaption><p></p>
</figure>
</div>
<p>You can try it for yourself at any one of the many huggingface spaces:</p>
<ul>
<li><a href="https://huggingface.co/spaces/runwayml/stable-diffusion-v1-5">App for Stability Diffusion 1.5</a></li>
<li><a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">App for Stability Diffusion 2, recently released</a></li>
<li><a href="https://huggingface.co/spaces/anzorq/finetuned_diffusion">App that runs models for many styles</a></li>
</ul>
<p>It can take 10-20s to generate an image.</p>
<p>If you prefer a more hands-on approach, use this colab notebook - <a href="https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_AUTOMATIC1111.ipynb#scrollTo=PjzwxTkPSPHf">fast_stable_diffusion_AUTOMATIC1111.ipynb</a>, in this notebook, give the path to model hosted on huggingface (I used <code>runwayml/stable-diffusion-v1-5</code> for my base model). The notebook has all the code for installing dependencies, and will launch a GUI which you can use for experimentation.</p>
</section>
<section id="dreambooth" class="level2">
<h2 class="anchored" data-anchor-id="dreambooth">Dreambooth</h2>
<p>However, the publicly released stable diffusion model includes only those concepts, people, artstyles that were present in the training dataset of it. What if you want to add new people/concepts/styles, and use them to generate art?</p>
<p>That’s where dreambooth comes in. This technique allows you to add a person/concept to the stable diffusion model using only ~20 images of it, and takes only ~1 hour on free colab to train! Because of dreambooth, fine-tuning the stable diffusion model on a given style or person is a breeze.</p>
</section>
<section id="finetuning-approach" class="level2">
<h2 class="anchored" data-anchor-id="finetuning-approach">Finetuning Approach</h2>
<p>My primary resource for fine-tuning, i.e.&nbsp;adding myself to the stable diffusion space was this blog post: <a href="https://bytexd.com/how-to-use-dreambooth-to-fine-tune-stable-diffusion-colab/">How to Use DreamBooth to Fine-Tune Stable Diffusion (Colab)</a>. Which points to a beautiful colab notebook - <a href="https://colab.research.google.com/github/TheLastBen/fast-stable-diffusion/blob/main/fast-DreamBooth.ipynb#scrollTo=iAZGngFcI8hq">TheLastBen/fast-stable-diffusion</a>, which has it all ready- you only need to give the links to the base model, huggingface token (you need to signup to this website), and upload your images, and hit train.</p>
<p>Follow the advice in the blogpost and the notebook, and you’ll have a gradio app in your browser. Now you can add any text prompt you want in the text box, and generate results!</p>
</section>
<section id="finding-the-right-prompts" class="level2">
<h2 class="anchored" data-anchor-id="finding-the-right-prompts">Finding the Right Prompts</h2>
<p>Yeah, except, <em>any</em> prompt will not give you the best results. Prompt Engineering is a big deal here. You have to construct a prompt, add labels and styles in order to direct the model to get an acceptable image. You also can tinker with the hyperparameters- adjust the CFG scale, sampling steps, to get better results.</p>
<p>My experience- prompt engineering sucks. It’s more of an art that engineering, and there’s no other way but trial and error. Fortunately there are resources on the internet that can help you.</p>
<p>For me the process looked like:</p>
<ol type="1">
<li>Go to a resource like <a href="lexica.art">lexica.art</a>, and find an image you like</li>
<li>Copy the prompt and settings, and modify the prompt to include your token used during training.</li>
</ol>
<p>This will generally give you good results, and how I got most of my good images. However tuning of hyperparameters requireds trial and error, and hence time. I didn’t spend too much time on this, so I fixed my sampling method at euler-a and steps at 40-50. I only tinked with CFG scale.</p>
</section>
<section id="results-on-myself" class="level2">
<h2 class="anchored" data-anchor-id="results-on-myself">Results on Myself</h2>
<p>Finally, the meat of the post- let’s see the actual results on myself.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/port1.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/port2.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/port3.png" class="img-fluid"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/port6.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">As a Naruto(?) Character</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/port5.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Style of Japanese Woodblock</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/shubham_pixar.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Me As a Pixar Character</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p>Some Portraits of Me</p>
</div>
</div>
</div>
<p>Some generated art of my wife and me.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/ss1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">As a Disney Princess</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/ss2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">As a Studio Ghibli Character</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/ss3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">As Pixar Characters</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/ss4.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Anime Character</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/ss5.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Stylized Portrait</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/stable_diff/assets/ss6.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">As a Japanese Woodblock</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>


</section>

 ]]></description>
  <category>ai-art</category>
  <guid>https://shindeshu.github.io/posts/stable_diff/getting_started_sd.html</guid>
  <pubDate>Tue, 29 Nov 2022 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/stable_diff/assets/art_ex3.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Implementing a Graph Neural Network from Scratch</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/gnns/gnn_from_scratch.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/gnns/graph_cat.jpg" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">Let’s Build a GNN</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://colab.research.google.com/github/shindeshu/gnn_from_scratch.ipynb"><img src="https://shindeshu.github.io/posts/gnns/https:/colab.research.google.com/assets/colab-badge.svg" class="img-fluid figure-img"></a></p>
<p></p><figcaption class="figure-caption">Open In Colab</figcaption><p></p>
</figure>
</div>
<p>If you are unfamiliar with GNNs in general, please go through my small <a href="https://shindeshu.github.io/posts/gnns/intro_gnn.html">intro blogpost</a>. Message Passing is one of the more popular concepts in GNNs, and that is what we’ll try to implement here. Specifically we are implementing the Graph Convolutional Layer/Network proposed by Kipf et al in 2016. You can go through a detailed <a href="https://tkipf.github.io/graph-convolutional-networks/">blogpost</a> of his or the <a href="https://arxiv.org/abs/1609.02907">original paper</a>.</p>
<section id="representing-a-graph" class="level2">
<h2 class="anchored" data-anchor-id="representing-a-graph">Representing a Graph</h2>
<p>Before we start on to Graph convolutions, let’s first present it out on how do we represent a graph in code. Mathematically, a graph is defined as a tuple of a set of nodes/vertices <img src="https://shindeshu.github.io/posts/gnns/https:/render.githubusercontent.com/render/math?math=V">, and a set of edges/links <img src="https://shindeshu.github.io/posts/gnns/https:/render.githubusercontent.com/render/math?math=E:/mathcal{G}=(V,E)">. Further, each edge is a pair of two vertices, and represents a connection between them.</p>
<p>Visually, a graph would look something like this:</p>
<center width="100%" style="padding:10px">
<img src="https://shindeshu.github.io/posts/gnns/https:/www.researchgate.net/profile/Wei-Dong-51/publication/235973074/figure/fig1/AS:393538677297153@1470838340530/A-small-example-graph-representation-of-a-network-with-8-nodes-and-14-undirected-edges.png" width="350px">
</center>
<p>The vertices are <img src="https://shindeshu.github.io/posts/gnns/https:/render.githubusercontent.com/render/math?math=V=/{1,2,3,4,5,6,7,8/}">, and edges <img src="https://shindeshu.github.io/posts/gnns/https:/render.githubusercontent.com/render/math?math=E=/{(1,5), (2,1), (2,8), (3,4), .../}">.</p>
<p>There are many ways to represent graphs in memory- two of them include “adjacency matrix” (<img src="https://latex.codecogs.com/png.latex?a">) and “edge list”. If the number of nodes is <img src="https://latex.codecogs.com/png.latex?n">, the adjacency matrix is <img src="https://latex.codecogs.com/png.latex?n%20x%20n">. If there’s an edge from node <img src="https://latex.codecogs.com/png.latex?n_i"> to <img src="https://latex.codecogs.com/png.latex?n_j">, the element <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is equal to 1. Likewise, the other elements of <img src="https://latex.codecogs.com/png.latex?a"> are populated.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">[[ <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">0</span> ]</span>
<span id="cb1-2"> [ <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">1</span> ]</span>
<span id="cb1-3"> [ <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> ]</span>
<span id="cb1-4"> [ <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">0</span> ]]</span></code></pre></div>
<p>Working with adjacency matrix for graph operations is easier, although they have their limitations. While established libraries like <code>dgl</code> or <code>pytorch-geometric</code> use edge-list format of data, here we are working with an adjacency matrix.</p>
</section>
<section id="graph-convolutions" class="level2">
<h2 class="anchored" data-anchor-id="graph-convolutions">Graph Convolutions</h2>
<p>Graph convolutions are somewhat similar to image convolutions, in that they take their neighbourhood information and aggregate to get a richer understanding of their position. Also, the “parameters” of the filters are shared across the entire image, which is analogous to a graph convolution as well, where the parameters are shared across the graph.</p>
<p>GCNs rely on the message passing paradigm. Each node has a feature vector associated with it. For a given node u, each of its neighbouring nodes <img src="https://latex.codecogs.com/png.latex?v_i"> send a message derived from its feature vector to it. All these messages are aggregated alongwith its own feature vector, and this is used to update this node <img src="https://latex.codecogs.com/png.latex?u"> to get the final feature vector (or embedding).</p>
</section>
<section id="current-implementation" class="level2">
<h2 class="anchored" data-anchor-id="current-implementation">Current Implementation</h2>
<p>Each node has a feature vector. This feature will be projected using a linear layer, output of which will be the message that each node passes. We will represent the graph as an adjacency matrix, and multiply by the node features (projected) to perform the message passing. This will be divided by the number of neighbours for normalizing, which will give us the output of our first graph convolution layer.</p>
<p>Importing all our libraries. We are not using libraries like <code>dgl</code> or <code>pytorch-geometric</code>, we will be using plain pytorch. We are also using <code>networkx</code> for manipulating graph.</p>
<p>We will be a creating a random matrix as an adjacency matrix. Creating a matrix with uniform_ method and the bernoulli method.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">nodes <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb2-2">node_features_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4</span></span>
<span id="cb2-3"></span>
<span id="cb2-4">adj <span class="op" style="color: #5E5E5E;">=</span> torch.empty(nodes, nodes).uniform_(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>).bernoulli()</span></code></pre></div>
</div>
<p>Visualizing the graph we created with networkx library</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">graph <span class="op" style="color: #5E5E5E;">=</span> nx.from_numpy_matrix(adj.numpy())</span>
<span id="cb3-2">graph.remove_edges_from(nx.selfloop_edges(graph))</span>
<span id="cb3-3"></span>
<span id="cb3-4">pos <span class="op" style="color: #5E5E5E;">=</span> nx.kamada_kawai_layout(graph)</span>
<span id="cb3-5">nx.draw(graph, pos, with_labels<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/gnns/gnn_from_scratch_files/gnn_from_scratch_12_0.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">png</figcaption><p></p>
</figure>
</div>
<p>Creating random features for our nodes. These features will go through a dense layer and then act as our messages.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">node_features <span class="op" style="color: #5E5E5E;">=</span> torch.empty(nodes, node_features_size).uniform_(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>).bernoulli()<span class="co" style="color: #5E5E5E;">#.view(1, nodes, node_features_size)</span></span>
<span id="cb4-2">node_features</span></code></pre></div>
</div>
<p>The features will pass through a linear layer to create our messages</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">projector <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(node_features_size, <span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb5-2"></span>
<span id="cb5-3">node_feat_proj <span class="op" style="color: #5E5E5E;">=</span> projector(node_features)</span>
<span id="cb5-4"></span>
<span id="cb5-5">num_neighbours <span class="op" style="color: #5E5E5E;">=</span> adj.<span class="bu" style="color: null;">sum</span>(dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>, keepdims<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb5-6"></span>
<span id="cb5-7">torch.matmul(adj, node_feat_proj)<span class="op" style="color: #5E5E5E;">/</span>num_neighbours</span></code></pre></div>
</div>
<pre><code>tensor([[-0.5067, -0.2463, -0.0555,  0.2188,  0.4031],
            [-0.8397,  0.0945,  0.5124,  0.1179, -0.0296],
            [-0.6457,  0.2369,  0.5048, -0.0216,  0.1531],
            [-0.9893,  0.4223,  0.7235,  0.3212, -0.1165],
            [-0.5876,  0.2246,  0.5227, -0.1519,  0.1979],
            [-0.6133, -0.0359,  0.2532,  0.0760,  0.2250],
            [-0.7740,  0.2055,  0.5252,  0.1075,  0.0174],
            [-0.7827,  0.1653,  0.5654,  0.0135, -0.0155],
            [-0.8635,  0.3189,  0.6940,  0.0758, -0.0423],
            [-0.9374,  0.2670,  0.6672,  0.1805, -0.1292]], grad_fn=&lt;DivBackward0&gt;)</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">adj.shape, node_feat_proj.shape</span></code></pre></div>
</div>
<pre><code>(torch.Size([10, 10]), torch.Size([10, 5]))</code></pre>
</section>
<section id="a-note-on-above-multiplication-operation" class="level2">
<h2 class="anchored" data-anchor-id="a-note-on-above-multiplication-operation">A Note on Above Multiplication Operation</h2>
<p>How it does achieve our objective, i.e.&nbsp;summing up of messages from neighbouring nodes of a particular node?</p>
<p>For simplicity, lets take an example where the adj matrix is $ 7 $ and the message matrix is $ 7 $.</p>
<p>Consider a single row from the adjacency matrix, that corresponds to a node <img src="https://latex.codecogs.com/png.latex?n_i">. It might look something like <img src="https://latex.codecogs.com/png.latex?%0AA%20=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%200%20&amp;%201%20&amp;%200%20&amp;%200%20&amp;%201%20&amp;%200%20&amp;%201%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>And the message matrix is <img src="https://latex.codecogs.com/png.latex?7%20%5Ctimes%205">. (seven rows, five columns).</p>
<p>For this node, we can observe there are edges existent only for nodes <img src="https://latex.codecogs.com/png.latex?%7B2,%205,%207%7D">. When we multiple the above matrix with the message/feature matrix, we will get the elements corresponding to those indexes summed up (since others are multiplied by zero), along the second axis of the feature matrix i.e.&nbsp;we will get a <img src="https://latex.codecogs.com/png.latex?1%20%5Ctimes%205"> size vector.</p>
<p>Here, you can see that only the neighbouring nodes’ features have been summed up to get the final d-length vector.</p>
</section>
<section id="putting-it-all-together" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together">Putting It All Together</h2>
<p>Now that we’ve done it step-by-step, let us aggregate the operations together in proper functions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;">class</span> GCNLayer(nn.Module):</span>
<span id="cb9-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, in_feat, out_feat):</span>
<span id="cb9-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb9-4">        <span class="va" style="color: #111111;">self</span>.projector <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(in_feat, out_feat)</span>
<span id="cb9-5"></span>
<span id="cb9-6">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, node_features, adj):</span>
<span id="cb9-7">        num_neighbours <span class="op" style="color: #5E5E5E;">=</span> adj.<span class="bu" style="color: null;">sum</span>(dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>, keepdims<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb9-8">        node_features <span class="op" style="color: #5E5E5E;">=</span> torch.relu(<span class="va" style="color: #111111;">self</span>.projector(node_features))</span>
<span id="cb9-9">        node_features <span class="op" style="color: #5E5E5E;">=</span> torch.matmul(adj, node_features)</span>
<span id="cb9-10">        node_features <span class="op" style="color: #5E5E5E;">=</span> node_features <span class="op" style="color: #5E5E5E;">/</span> num_neighbours</span>
<span id="cb9-11">        node_features <span class="op" style="color: #5E5E5E;">=</span> torch.relu(node_features)</span>
<span id="cb9-12">        <span class="cf" style="color: #003B4F;">return</span> node_features</span>
<span id="cb9-13">layer1 <span class="op" style="color: #5E5E5E;">=</span> GCNLayer(node_features_size, <span class="dv" style="color: #AD0000;">8</span>)</span>
<span id="cb9-14">layer1(node_features, adj).shape</span></code></pre></div>
</div>
<pre><code>torch.Size([10, 8])</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">layer2 <span class="op" style="color: #5E5E5E;">=</span> GCNLayer(<span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb11-2">layer2(layer1(node_features, adj), adj)</span></code></pre></div>
</div>
<pre><code>tensor([[0.4279, 0.4171],
            [0.4724, 0.4304],
            [0.4318, 0.3761],
            [0.4315, 0.3860],
            [0.4520, 0.4132],
            [0.4449, 0.4049],
            [0.4346, 0.3827],
            [0.4614, 0.4176],
            [0.4446, 0.3860],
            [0.4068, 0.3582]], grad_fn=&lt;ReluBackward0&gt;)</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="kw" style="color: #003B4F;">class</span> GCNmodel(nn.Module):</span>
<span id="cb13-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, in_feat, hid_feat, out_feat):</span>
<span id="cb13-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb13-4">        <span class="va" style="color: #111111;">self</span>.gcn_layer1 <span class="op" style="color: #5E5E5E;">=</span> GCNLayer(in_feat, hid_feat)</span>
<span id="cb13-5">        <span class="va" style="color: #111111;">self</span>.gcn_layer2 <span class="op" style="color: #5E5E5E;">=</span> GCNLayer(hid_feat, out_feat)</span>
<span id="cb13-6"></span>
<span id="cb13-7">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, node_features, adj):</span>
<span id="cb13-8">        h <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.gcn_layer1(node_features, adj)</span>
<span id="cb13-9">        h <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.gcn_layer2(h, adj)</span>
<span id="cb13-10">        <span class="cf" style="color: #003B4F;">return</span> h</span>
<span id="cb13-11">model <span class="op" style="color: #5E5E5E;">=</span> GCNmodel(node_features_size, <span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</div>
</section>
<section id="solving-a-real-problem" class="level2">
<h2 class="anchored" data-anchor-id="solving-a-real-problem">Solving a Real Problem</h2>
<p>Now that we are able to play around with random data, lets us get to work on some real datasets that we can do basic classification problems on. We will be using the zachary’s karate club dataset, which is a small dataset of 34 people and the edges include their observed interactions with each other. Our objective: predict which group will each of the people go to once their club is bisected.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;">def</span> build_karate_club_graph():</span>
<span id="cb14-2">    g <span class="op" style="color: #5E5E5E;">=</span> nx.Graph()</span>
<span id="cb14-3">    edge_list <span class="op" style="color: #5E5E5E;">=</span> [(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb14-4">        (<span class="dv" style="color: #AD0000;">4</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">6</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">6</span>, <span class="dv" style="color: #AD0000;">4</span>), (<span class="dv" style="color: #AD0000;">6</span>, <span class="dv" style="color: #AD0000;">5</span>), (<span class="dv" style="color: #AD0000;">7</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">7</span>, <span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb14-5">        (<span class="dv" style="color: #AD0000;">7</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">7</span>, <span class="dv" style="color: #AD0000;">3</span>), (<span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">9</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">4</span>),</span>
<span id="cb14-6">        (<span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">5</span>), (<span class="dv" style="color: #AD0000;">11</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">3</span>), (<span class="dv" style="color: #AD0000;">13</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">13</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">13</span>, <span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb14-7">        (<span class="dv" style="color: #AD0000;">13</span>, <span class="dv" style="color: #AD0000;">3</span>), (<span class="dv" style="color: #AD0000;">16</span>, <span class="dv" style="color: #AD0000;">5</span>), (<span class="dv" style="color: #AD0000;">16</span>, <span class="dv" style="color: #AD0000;">6</span>), (<span class="dv" style="color: #AD0000;">17</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">17</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">19</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">19</span>, <span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb14-8">        (<span class="dv" style="color: #AD0000;">21</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">21</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">25</span>, <span class="dv" style="color: #AD0000;">23</span>), (<span class="dv" style="color: #AD0000;">25</span>, <span class="dv" style="color: #AD0000;">24</span>), (<span class="dv" style="color: #AD0000;">27</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">27</span>, <span class="dv" style="color: #AD0000;">23</span>),</span>
<span id="cb14-9">        (<span class="dv" style="color: #AD0000;">27</span>, <span class="dv" style="color: #AD0000;">24</span>), (<span class="dv" style="color: #AD0000;">28</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">29</span>, <span class="dv" style="color: #AD0000;">23</span>), (<span class="dv" style="color: #AD0000;">29</span>, <span class="dv" style="color: #AD0000;">26</span>), (<span class="dv" style="color: #AD0000;">30</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">30</span>, <span class="dv" style="color: #AD0000;">8</span>),</span>
<span id="cb14-10">        (<span class="dv" style="color: #AD0000;">31</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">31</span>, <span class="dv" style="color: #AD0000;">24</span>), (<span class="dv" style="color: #AD0000;">31</span>, <span class="dv" style="color: #AD0000;">25</span>), (<span class="dv" style="color: #AD0000;">31</span>, <span class="dv" style="color: #AD0000;">28</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">8</span>),</span>
<span id="cb14-11">        (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">14</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">15</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">18</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">20</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">22</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">23</span>),</span>
<span id="cb14-12">        (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">29</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">30</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">31</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">8</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">9</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">13</span>),</span>
<span id="cb14-13">        (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">14</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">15</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">18</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">19</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">20</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">22</span>),</span>
<span id="cb14-14">        (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">23</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">26</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">27</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">28</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">29</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">30</span>),</span>
<span id="cb14-15">        (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">31</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">32</span>)]</span>
<span id="cb14-16">    g.add_edges_from(edge_list)</span>
<span id="cb14-17">    <span class="cf" style="color: #003B4F;">return</span> g</span>
<span id="cb14-18"></span>
<span id="cb14-19">g <span class="op" style="color: #5E5E5E;">=</span> build_karate_club_graph()</span></code></pre></div>
</div>
<p>Visualizing our karate club graph:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">pos <span class="op" style="color: #5E5E5E;">=</span> nx.kamada_kawai_layout(g)</span>
<span id="cb15-2">nx.draw(g, pos, with_labels<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/gnns/gnn_from_scratch_files/gnn_from_scratch_35_0.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">png</figcaption><p></p>
</figure>
</div>
<p>We don’t have any node features. So here we’re creating a one-hot vector for each node based on its id. Together, it’d be a single identity matrix for the graph.</p>
<p>At the beginning, only the instructor and president nodes are labelled. Later on each person will join one of the groups headed by these two. So it’s a binary classification, and the only labeled nodes we have are two.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">node_features <span class="op" style="color: #5E5E5E;">=</span>  torch.eye(<span class="dv" style="color: #AD0000;">34</span>) </span>
<span id="cb16-2">labeled_nodes <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">33</span>])  <span class="co" style="color: #5E5E5E;"># only the instructor and the president nodes are labeled</span></span>
<span id="cb16-3">labels <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb16-4"></span>
<span id="cb16-5"><span class="co" style="color: #5E5E5E;"># since our code only works on adjacency matrix and not on edge-list</span></span>
<span id="cb16-6"></span>
<span id="cb16-7">adj_matrix <span class="op" style="color: #5E5E5E;">=</span> torch.from_numpy(nx.adjacency_matrix(g).todense()).<span class="bu" style="color: null;">float</span>()</span>
<span id="cb16-8"></span>
<span id="cb16-9"><span class="co" style="color: #5E5E5E;"># define our gcn model</span></span>
<span id="cb16-10"></span>
<span id="cb16-11">model <span class="op" style="color: #5E5E5E;">=</span> GCNmodel(<span class="dv" style="color: #AD0000;">34</span>, <span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb16-12"></span>
<span id="cb16-13"><span class="co" style="color: #5E5E5E;"># do a single pass just for a check</span></span>
<span id="cb16-14"></span>
<span id="cb16-15">model(node_features, adj_matrix)</span></code></pre></div>
</div>
<p>Lets get to the meat of it: time to train our model. We create the usual pytorch pipeline. If you’ve worked with pytorch before, this is familiar to you. Even if not, you can get a certain idea if you know some basics of neural networks / backprop.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">optimizer <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb17-2">all_logits <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb17-3"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">100</span>):</span>
<span id="cb17-4">    logits <span class="op" style="color: #5E5E5E;">=</span> model(node_features, adj_matrix)</span>
<span id="cb17-5">    <span class="co" style="color: #5E5E5E;"># we save the logits for visualization later</span></span>
<span id="cb17-6">    all_logits.append(logits.detach())</span>
<span id="cb17-7">    logp <span class="op" style="color: #5E5E5E;">=</span> F.log_softmax(logits, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb17-8">    <span class="co" style="color: #5E5E5E;"># we only compute loss for labeled nodes</span></span>
<span id="cb17-9">    loss <span class="op" style="color: #5E5E5E;">=</span> F.nll_loss(logp[labeled_nodes], labels)</span>
<span id="cb17-10"></span>
<span id="cb17-11">    optimizer.zero_grad()</span>
<span id="cb17-12">    loss.backward()</span>
<span id="cb17-13">    optimizer.step()</span>
<span id="cb17-14"></span>
<span id="cb17-15">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">'Epoch </span><span class="sc" style="color: #5E5E5E;">%d</span><span class="st" style="color: #20794D;"> | Loss: </span><span class="sc" style="color: #5E5E5E;">%.4f</span><span class="st" style="color: #20794D;">'</span> <span class="op" style="color: #5E5E5E;">%</span> (epoch, loss.item()))</span></code></pre></div>
</div>
<pre><code>    Epoch 0 | Loss: 0.6887
    Epoch 1 | Loss: 0.6823
    Epoch 2 | Loss: 0.6756
    Epoch 3 | Loss: 0.6704
    Epoch 4 | Loss: 0.6653
    Epoch 5 | Loss: 0.6592
    Epoch 6 | Loss: 0.6529
    Epoch 7 | Loss: 0.6465
    Epoch 8 | Loss: 0.6396
    Epoch 9 | Loss: 0.6320
    Epoch 10 | Loss: 0.6239
    Epoch 11 | Loss: 0.6151
    Epoch 12 | Loss: 0.6064
    Epoch 13 | Loss: 0.5973
    Epoch 14 | Loss: 0.5878
    Epoch 15 | Loss: 0.5783
    Epoch 16 | Loss: 0.5686
    Epoch 17 | Loss: 0.5585
    Epoch 18 | Loss: 0.5482
    Epoch 19 | Loss: 0.5382
    Epoch 20 | Loss: 0.5281
    Epoch 21 | Loss: 0.5182
    Epoch 22 | Loss: 0.5085
    Epoch 23 | Loss: 0.4990
    Epoch 24 | Loss: 0.4899
    Epoch 25 | Loss: 0.4810
    Epoch 26 | Loss: 0.4725
    Epoch 27 | Loss: 0.4642
    Epoch 28 | Loss: 0.4560
    Epoch 29 | Loss: 0.4477
    Epoch 30 | Loss: 0.4397
    Epoch 31 | Loss: 0.4331
    Epoch 32 | Loss: 0.4267
    Epoch 33 | Loss: 0.4204
    Epoch 34 | Loss: 0.4143
    Epoch 35 | Loss: 0.4082
    Epoch 36 | Loss: 0.4037
    Epoch 37 | Loss: 0.3994
    Epoch 38 | Loss: 0.3952
    Epoch 39 | Loss: 0.3911
    Epoch 40 | Loss: 0.3873
    Epoch 41 | Loss: 0.3837
    Epoch 42 | Loss: 0.3802
    Epoch 43 | Loss: 0.3767
    Epoch 44 | Loss: 0.3733
    Epoch 45 | Loss: 0.3698
    Epoch 46 | Loss: 0.3670
    Epoch 47 | Loss: 0.3655
    Epoch 48 | Loss: 0.3638
    Epoch 49 | Loss: 0.3620
    Epoch 50 | Loss: 0.3602
    Epoch 51 | Loss: 0.3586
    Epoch 52 | Loss: 0.3571
    Epoch 53 | Loss: 0.3573
    Epoch 54 | Loss: 0.3564
    Epoch 55 | Loss: 0.3544
    Epoch 56 | Loss: 0.3542
    Epoch 57 | Loss: 0.3539
    Epoch 58 | Loss: 0.3536
    Epoch 59 | Loss: 0.3533
    Epoch 60 | Loss: 0.3529
    Epoch 61 | Loss: 0.3525
    Epoch 62 | Loss: 0.3522
    Epoch 63 | Loss: 0.3518
    Epoch 64 | Loss: 0.3514
    Epoch 65 | Loss: 0.3511
    Epoch 66 | Loss: 0.3508
    Epoch 67 | Loss: 0.3505
    Epoch 68 | Loss: 0.3502
    Epoch 69 | Loss: 0.3504
    Epoch 70 | Loss: 0.3498
    Epoch 71 | Loss: 0.3497
    Epoch 72 | Loss: 0.3439
    Epoch 73 | Loss: 0.3194
    Epoch 74 | Loss: 0.2869
    Epoch 75 | Loss: 0.2505
    Epoch 76 | Loss: 0.2138
    Epoch 77 | Loss: 0.1789
    Epoch 78 | Loss: 0.1476
    Epoch 79 | Loss: 0.1206
    Epoch 80 | Loss: 0.0984
    Epoch 81 | Loss: 0.0811
    Epoch 82 | Loss: 0.0682
    Epoch 83 | Loss: 0.0587
    Epoch 84 | Loss: 0.0516
    Epoch 85 | Loss: 0.0459
    Epoch 86 | Loss: 0.0407
    Epoch 87 | Loss: 0.0356
    Epoch 88 | Loss: 0.0307
    Epoch 89 | Loss: 0.0262
    Epoch 90 | Loss: 0.0223
    Epoch 91 | Loss: 0.0191
    Epoch 92 | Loss: 0.0164
    Epoch 93 | Loss: 0.0142
    Epoch 94 | Loss: 0.0124
    Epoch 95 | Loss: 0.0111
    Epoch 96 | Loss: 0.0101
    Epoch 97 | Loss: 0.0093
    Epoch 98 | Loss: 0.0087
    Epoch 99 | Loss: 0.0081</code></pre>
<p>We can see the loss converging. This dataset doesn’t really have a valid set or anything, so there are no metrics to be presented here. But we can visualize them directly which can be fun to see. Here, we can create an animation of the results of each epoch, and watch them fluctuate as the model converges.</p>
<p>This vis code was taken from <a href="https://docs.dgl.ai/en/0.2.x/tutorials/basics/1_first.html">dgl documentation</a>. The dgl docs are a great place to start learning about graph neural networks!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;">import</span> matplotlib.animation <span class="im" style="color: #00769E;">as</span> animation</span>
<span id="cb19-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb19-3"></span>
<span id="cb19-4"><span class="kw" style="color: #003B4F;">def</span> draw(i):</span>
<span id="cb19-5">    cls1color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'#00FFFF'</span></span>
<span id="cb19-6">    cls2color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'#FF00FF'</span></span>
<span id="cb19-7">    pos <span class="op" style="color: #5E5E5E;">=</span> {}</span>
<span id="cb19-8">    colors <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb19-9">    <span class="cf" style="color: #003B4F;">for</span> v <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">34</span>):</span>
<span id="cb19-10">        pos[v] <span class="op" style="color: #5E5E5E;">=</span> all_logits[i][v].numpy()</span>
<span id="cb19-11">        cls <span class="op" style="color: #5E5E5E;">=</span> pos[v].argmax()</span>
<span id="cb19-12">        colors.append(cls1color <span class="cf" style="color: #003B4F;">if</span> cls <span class="cf" style="color: #003B4F;">else</span> cls2color)</span>
<span id="cb19-13">    ax.cla()</span>
<span id="cb19-14">    ax.axis(<span class="st" style="color: #20794D;">'off'</span>)</span>
<span id="cb19-15">    ax.set_title(<span class="st" style="color: #20794D;">'Epoch: </span><span class="sc" style="color: #5E5E5E;">%d</span><span class="st" style="color: #20794D;">'</span> <span class="op" style="color: #5E5E5E;">%</span> i)</span>
<span id="cb19-16">    pos <span class="op" style="color: #5E5E5E;">=</span> nx.kamada_kawai_layout(g)</span>
<span id="cb19-17">    nx.draw_networkx(g.to_undirected(), pos, node_color<span class="op" style="color: #5E5E5E;">=</span>colors,</span>
<span id="cb19-18">            with_labels<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, node_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">300</span>, ax<span class="op" style="color: #5E5E5E;">=</span>ax)</span>
<span id="cb19-19"></span>
<span id="cb19-20">fig <span class="op" style="color: #5E5E5E;">=</span> plt.figure(dpi<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">150</span>)</span>
<span id="cb19-21">fig.clf()</span>
<span id="cb19-22">ax <span class="op" style="color: #5E5E5E;">=</span> fig.subplots()</span>
<span id="cb19-23">draw(<span class="dv" style="color: #AD0000;">0</span>)  <span class="co" style="color: #5E5E5E;"># draw the prediction of the first epoch</span></span>
<span id="cb19-24">plt.close()</span>
<span id="cb19-25"></span>
<span id="cb19-26">ani <span class="op" style="color: #5E5E5E;">=</span> animation.FuncAnimation(fig, draw, frames<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">len</span>(all_logits), interval<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">200</span>)</span>
<span id="cb19-27"></span>
<span id="cb19-28">ani.save(<span class="st" style="color: #20794D;">"karate.gif"</span>, writer<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pillow"</span>)</span></code></pre></div>
</div>


</section>

 ]]></description>
  <category>deep-learning</category>
  <category>gnn</category>
  <guid>https://shindeshu.github.io/posts/gnns/gnn_from_scratch.html</guid>
  <pubDate>Mon, 24 Jan 2022 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/gnns/graph_cat.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A Brief Intro To Graph Neural Networks</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/gnns/intro_to_gnn.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/gnns/graph_img.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">graph-img</figcaption><p></p>
</figure>
</div>
<p>Deep Learning has revolutionized machine learning on all types of tasks ranging from computer vision to natural language processing or sequence modeling. Most of these applications however involve mostly euclidean data that are constrained to some fixed dimensions.</p>
<p>What happens when your data is of non-euclidean nature? Graphs are one way to represent such non-euclidean data, which represent it in form of objects linked with each other through relationships. Machine learning using graphs has always been around, however with the advances in deep learning, recently there have been some exciting developments for learning on graphs.</p>
<p><img src="https://shindeshu.github.io/posts/gnns/https:/shindeshu.github.io/assets/images/euclidean.png" width="600"></p>
<p>What is a graph, you say? Graph is a set of vertices / nodes (our objects of interest), with edges (relationships between our objects). For example in a social media graph, an account would be a node, and them following someone could be an edge. Numerically, a graph can be represented as a matrix (adjacency), or as a list (of edges).</p>
<p>What data can be represented in the form of graphs? A lot of it! Interactions on a social media site, financial transactions, citation networks, molecules, all these can be represented in the form of graphs and can then be leveraged for machine learning.</p>
<p>Graph representation learning: when we do have a graph (i.e.&nbsp;our nodes, their features, their edges, <em>their</em> features), our objective is to learn embeddings for each node, such that two “similar” nodes will have their embeddings closer in space. This embedding for a node should bake into itself its relationships and its neighbourhood and their features (apart from its own). This embedding vector can then be used for our downstream tasks.</p>
<p><img src="https://shindeshu.github.io/posts/gnns/https:/shindeshu.github.io/assets/images/node_rep_learning.png" width="600"></p>
<p>Learning the embedding: while there are many ways to skin this particular cat, the one that’s hot right now is called “message passing” or a graph convolution layer. The core concept is pretty simple. Lets say our current node of interest, has three neighbours. Each one of these will pass a “message” to our node, this message being the current state of the node. These messages will be aggregated together with our node’s current state, and this will be used to update the node’s state to next state. After covering for all nodes, you’d get a complete pass over the entire graph, for a single graph convolution layer. Different frameworks will have different ways of passing messages, or updating them, but the underlying principle is pretty same.</p>
<p>The details of message passing, we’ll go over in another post- since this is supposed to be a “brief” introduction.</p>



 ]]></description>
  <category>deep-learning</category>
  <category>gnn</category>
  <guid>https://shindeshu.github.io/posts/gnns/intro_to_gnn.html</guid>
  <pubDate>Mon, 24 Jan 2022 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/gnns/graph_img.jpeg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
