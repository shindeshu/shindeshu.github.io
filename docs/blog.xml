<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Shubham Shinde</title>
<link>https://shindeshu.github.io/blog.html</link>
<atom:link href="https://shindeshu.github.io/blog.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.1.251</generator>
<lastBuildDate>Mon, 24 Jan 2022 18:30:00 GMT</lastBuildDate>
<item>
  <title>Implementing a Graph Neural Network from Scratch</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/gnns/gnn_from_scratch.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/gnns/graph_img.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">graph-img</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://colab.research.google.com/github/shindeshu/gnn_from_scratch.ipynb"><img src="https://shindeshu.github.io/posts/gnns/https:/colab.research.google.com/assets/colab-badge.svg" class="img-fluid figure-img"></a></p>
<p></p><figcaption class="figure-caption">Open In Colab</figcaption><p></p>
</figure>
</div>
<p>If you are unfamiliar with GNNs in general, please go through my small <a href="https://shindeshu.github.io/posts/gnns/intro_gnn.html">intro blogpost</a>. Message Passing is one of the more popular concepts in GNNs, and that is what we’ll try to implement here. Specifically we are implementing the Graph Convolutional Layer/Network proposed by Kipf et al in 2016. You can go through a detailed <a href="https://tkipf.github.io/graph-convolutional-networks/">blogpost</a> of his or the <a href="https://arxiv.org/abs/1609.02907">original paper</a>.</p>
<section id="representing-a-graph" class="level2">
<h2 class="anchored" data-anchor-id="representing-a-graph">Representing a Graph</h2>
<p>Before we start on to Graph convolutions, let’s first present it out on how do we represent a graph in code. Mathematically, a graph is defined as a tuple of a set of nodes/vertices <img src="https://shindeshu.github.io/posts/gnns/https:/render.githubusercontent.com/render/math?math=V">, and a set of edges/links <img src="https://shindeshu.github.io/posts/gnns/https:/render.githubusercontent.com/render/math?math=E:/mathcal{G}=(V,E)">. Further, each edge is a pair of two vertices, and represents a connection between them.</p>
<p>Visually, a graph would look something like this:</p>
<center width="100%" style="padding:10px">
<img src="https://shindeshu.github.io/posts/gnns/https:/www.researchgate.net/profile/Wei-Dong-51/publication/235973074/figure/fig1/AS:393538677297153@1470838340530/A-small-example-graph-representation-of-a-network-with-8-nodes-and-14-undirected-edges.png" width="350px">
</center>
<p>The vertices are <img src="https://shindeshu.github.io/posts/gnns/https:/render.githubusercontent.com/render/math?math=V=/{1,2,3,4,5,6,7,8/}">, and edges <img src="https://shindeshu.github.io/posts/gnns/https:/render.githubusercontent.com/render/math?math=E=/{(1,5), (2,1), (2,8), (3,4), .../}">.</p>
<p>There are many ways to represent graphs in memory- two of them include “adjacency matrix” (<img src="https://latex.codecogs.com/png.latex?a">) and “edge list”. If the number of nodes is <img src="https://latex.codecogs.com/png.latex?n">, the adjacency matrix is <img src="https://latex.codecogs.com/png.latex?n%20x%20n">. If there’s an edge from node <img src="https://latex.codecogs.com/png.latex?n_i"> to <img src="https://latex.codecogs.com/png.latex?n_j">, the element <img src="https://latex.codecogs.com/png.latex?a_%7Bij%7D"> is equal to 1. Likewise, the other elements of <img src="https://latex.codecogs.com/png.latex?a"> are populated.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">[[ <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">0</span> ]</span>
<span id="cb1-2"> [ <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">1</span> ]</span>
<span id="cb1-3"> [ <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> ]</span>
<span id="cb1-4"> [ <span class="dv" style="color: #AD0000;">0</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">1</span> <span class="dv" style="color: #AD0000;">0</span> ]]</span></code></pre></div>
<p>Working with adjacency matrix for graph operations is easier, although they have their limitations. While established libraries like <code>dgl</code> or <code>pytorch-geometric</code> use edge-list format of data, here we are working with an adjacency matrix.</p>
</section>
<section id="graph-convolutions" class="level2">
<h2 class="anchored" data-anchor-id="graph-convolutions">Graph Convolutions</h2>
<p>Graph convolutions are somewhat similar to image convolutions, in that they take their neighbourhood information and aggregate to get a richer understanding of their position. Also, the “parameters” of the filters are shared across the entire image, which is analogous to a graph convolution as well, where the parameters are shared across the graph.</p>
<p>GCNs rely on the message passing paradigm. Each node has a feature vector associated with it. For a given node u, each of its neighbouring nodes <img src="https://latex.codecogs.com/png.latex?v_i"> send a message derived from its feature vector to it. All these messages are aggregated alongwith its own feature vector, and this is used to update this node <img src="https://latex.codecogs.com/png.latex?u"> to get the final feature vector (or embedding).</p>
</section>
<section id="current-implementation" class="level2">
<h2 class="anchored" data-anchor-id="current-implementation">Current Implementation</h2>
<p>Each node has a feature vector. This feature will be projected using a linear layer, output of which will be the message that each node passes. We will represent the graph as an adjacency matrix, and multiply by the node features (projected) to perform the message passing. This will be divided by the number of neighbours for normalizing, which will give us the output of our first graph convolution layer.</p>
<p>Importing all our libraries. We are not using libraries like <code>dgl</code> or <code>pytorch-geometric</code>, we will be using plain pytorch. We are also using <code>networkx</code> for manipulating graph.</p>
<p>We will be a creating a random matrix as an adjacency matrix. Creating a matrix with uniform_ method and the bernoulli method.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">nodes <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span></span>
<span id="cb2-2">node_features_size <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">4</span></span>
<span id="cb2-3"></span>
<span id="cb2-4">adj <span class="op" style="color: #5E5E5E;">=</span> torch.empty(nodes, nodes).uniform_(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>).bernoulli()</span></code></pre></div>
</div>
<p>Visualizing the graph we created with networkx library</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">graph <span class="op" style="color: #5E5E5E;">=</span> nx.from_numpy_matrix(adj.numpy())</span>
<span id="cb3-2">graph.remove_edges_from(nx.selfloop_edges(graph))</span>
<span id="cb3-3"></span>
<span id="cb3-4">pos <span class="op" style="color: #5E5E5E;">=</span> nx.kamada_kawai_layout(graph)</span>
<span id="cb3-5">nx.draw(graph, pos, with_labels<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/gnns/gnn_from_scratch_files/gnn_from_scratch_12_0.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">png</figcaption><p></p>
</figure>
</div>
<p>Creating random features for our nodes. These features will go through a dense layer and then act as our messages.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">node_features <span class="op" style="color: #5E5E5E;">=</span> torch.empty(nodes, node_features_size).uniform_(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>).bernoulli()<span class="co" style="color: #5E5E5E;">#.view(1, nodes, node_features_size)</span></span>
<span id="cb4-2">node_features</span></code></pre></div>
</div>
<p>The features will pass through a linear layer to create our messages</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">projector <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(node_features_size, <span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb5-2"></span>
<span id="cb5-3">node_feat_proj <span class="op" style="color: #5E5E5E;">=</span> projector(node_features)</span>
<span id="cb5-4"></span>
<span id="cb5-5">num_neighbours <span class="op" style="color: #5E5E5E;">=</span> adj.<span class="bu" style="color: null;">sum</span>(dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>, keepdims<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb5-6"></span>
<span id="cb5-7">torch.matmul(adj, node_feat_proj)<span class="op" style="color: #5E5E5E;">/</span>num_neighbours</span></code></pre></div>
</div>
<pre><code>tensor([[-0.5067, -0.2463, -0.0555,  0.2188,  0.4031],
            [-0.8397,  0.0945,  0.5124,  0.1179, -0.0296],
            [-0.6457,  0.2369,  0.5048, -0.0216,  0.1531],
            [-0.9893,  0.4223,  0.7235,  0.3212, -0.1165],
            [-0.5876,  0.2246,  0.5227, -0.1519,  0.1979],
            [-0.6133, -0.0359,  0.2532,  0.0760,  0.2250],
            [-0.7740,  0.2055,  0.5252,  0.1075,  0.0174],
            [-0.7827,  0.1653,  0.5654,  0.0135, -0.0155],
            [-0.8635,  0.3189,  0.6940,  0.0758, -0.0423],
            [-0.9374,  0.2670,  0.6672,  0.1805, -0.1292]], grad_fn=&lt;DivBackward0&gt;)</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">adj.shape, node_feat_proj.shape</span></code></pre></div>
</div>
<pre><code>(torch.Size([10, 10]), torch.Size([10, 5]))</code></pre>
</section>
<section id="a-note-on-above-multiplication-operation" class="level2">
<h2 class="anchored" data-anchor-id="a-note-on-above-multiplication-operation">A Note on Above Multiplication Operation</h2>
<p>How it does achieve our objective, i.e.&nbsp;summing up of messages from neighbouring nodes of a particular node?</p>
<p>For simplicity, lets take an example where the adj matrix is $ 7 $ and the message matrix is $ 7 $.</p>
<p>Consider a single row from the adjacency matrix, that corresponds to a node <img src="https://latex.codecogs.com/png.latex?n_i">. It might look something like <img src="https://latex.codecogs.com/png.latex?%0AA%20=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%200%20&amp;%201%20&amp;%200%20&amp;%200%20&amp;%201%20&amp;%200%20&amp;%201%5C%5C%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>And the message matrix is <img src="https://latex.codecogs.com/png.latex?7%20%5Ctimes%205">. (seven rows, five columns).</p>
<p>For this node, we can observe there are edges existent only for nodes <img src="https://latex.codecogs.com/png.latex?%7B2,%205,%207%7D">. When we multiple the above matrix with the message/feature matrix, we will get the elements corresponding to those indexes summed up (since others are multiplied by zero), along the second axis of the feature matrix i.e.&nbsp;we will get a <img src="https://latex.codecogs.com/png.latex?1%20%5Ctimes%205"> size vector.</p>
<p>Here, you can see that only the neighbouring nodes’ features have been summed up to get the final d-length vector.</p>
</section>
<section id="putting-it-all-together" class="level2">
<h2 class="anchored" data-anchor-id="putting-it-all-together">Putting It All Together</h2>
<p>Now that we’ve done it step-by-step, let us aggregate the operations together in proper functions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="kw" style="color: #003B4F;">class</span> GCNLayer(nn.Module):</span>
<span id="cb9-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, in_feat, out_feat):</span>
<span id="cb9-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb9-4">        <span class="va" style="color: #111111;">self</span>.projector <span class="op" style="color: #5E5E5E;">=</span> nn.Linear(in_feat, out_feat)</span>
<span id="cb9-5"></span>
<span id="cb9-6">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, node_features, adj):</span>
<span id="cb9-7">        num_neighbours <span class="op" style="color: #5E5E5E;">=</span> adj.<span class="bu" style="color: null;">sum</span>(dim<span class="op" style="color: #5E5E5E;">=-</span><span class="dv" style="color: #AD0000;">1</span>, keepdims<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb9-8">        node_features <span class="op" style="color: #5E5E5E;">=</span> torch.relu(<span class="va" style="color: #111111;">self</span>.projector(node_features))</span>
<span id="cb9-9">        node_features <span class="op" style="color: #5E5E5E;">=</span> torch.matmul(adj, node_features)</span>
<span id="cb9-10">        node_features <span class="op" style="color: #5E5E5E;">=</span> node_features <span class="op" style="color: #5E5E5E;">/</span> num_neighbours</span>
<span id="cb9-11">        node_features <span class="op" style="color: #5E5E5E;">=</span> torch.relu(node_features)</span>
<span id="cb9-12">        <span class="cf" style="color: #003B4F;">return</span> node_features</span>
<span id="cb9-13">layer1 <span class="op" style="color: #5E5E5E;">=</span> GCNLayer(node_features_size, <span class="dv" style="color: #AD0000;">8</span>)</span>
<span id="cb9-14">layer1(node_features, adj).shape</span></code></pre></div>
</div>
<pre><code>torch.Size([10, 8])</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">layer2 <span class="op" style="color: #5E5E5E;">=</span> GCNLayer(<span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb11-2">layer2(layer1(node_features, adj), adj)</span></code></pre></div>
</div>
<pre><code>tensor([[0.4279, 0.4171],
            [0.4724, 0.4304],
            [0.4318, 0.3761],
            [0.4315, 0.3860],
            [0.4520, 0.4132],
            [0.4449, 0.4049],
            [0.4346, 0.3827],
            [0.4614, 0.4176],
            [0.4446, 0.3860],
            [0.4068, 0.3582]], grad_fn=&lt;ReluBackward0&gt;)</code></pre>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="kw" style="color: #003B4F;">class</span> GCNmodel(nn.Module):</span>
<span id="cb13-2">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, in_feat, hid_feat, out_feat):</span>
<span id="cb13-3">        <span class="bu" style="color: null;">super</span>().<span class="fu" style="color: #4758AB;">__init__</span>()</span>
<span id="cb13-4">        <span class="va" style="color: #111111;">self</span>.gcn_layer1 <span class="op" style="color: #5E5E5E;">=</span> GCNLayer(in_feat, hid_feat)</span>
<span id="cb13-5">        <span class="va" style="color: #111111;">self</span>.gcn_layer2 <span class="op" style="color: #5E5E5E;">=</span> GCNLayer(hid_feat, out_feat)</span>
<span id="cb13-6"></span>
<span id="cb13-7">    <span class="kw" style="color: #003B4F;">def</span> forward(<span class="va" style="color: #111111;">self</span>, node_features, adj):</span>
<span id="cb13-8">        h <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.gcn_layer1(node_features, adj)</span>
<span id="cb13-9">        h <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.gcn_layer2(h, adj)</span>
<span id="cb13-10">        <span class="cf" style="color: #003B4F;">return</span> h</span>
<span id="cb13-11">model <span class="op" style="color: #5E5E5E;">=</span> GCNmodel(node_features_size, <span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
</div>
</section>
<section id="solving-a-real-problem" class="level2">
<h2 class="anchored" data-anchor-id="solving-a-real-problem">Solving a Real Problem</h2>
<p>Now that we are able to play around with random data, lets us get to work on some real datasets that we can do basic classification problems on. We will be using the zachary’s karate club dataset, which is a small dataset of 34 people and the edges include their observed interactions with each other. Our objective: predict which group will each of the people go to once their club is bisected.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="kw" style="color: #003B4F;">def</span> build_karate_club_graph():</span>
<span id="cb14-2">    g <span class="op" style="color: #5E5E5E;">=</span> nx.Graph()</span>
<span id="cb14-3">    edge_list <span class="op" style="color: #5E5E5E;">=</span> [(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">3</span>, <span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb14-4">        (<span class="dv" style="color: #AD0000;">4</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">6</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">6</span>, <span class="dv" style="color: #AD0000;">4</span>), (<span class="dv" style="color: #AD0000;">6</span>, <span class="dv" style="color: #AD0000;">5</span>), (<span class="dv" style="color: #AD0000;">7</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">7</span>, <span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb14-5">        (<span class="dv" style="color: #AD0000;">7</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">7</span>, <span class="dv" style="color: #AD0000;">3</span>), (<span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">9</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">4</span>),</span>
<span id="cb14-6">        (<span class="dv" style="color: #AD0000;">10</span>, <span class="dv" style="color: #AD0000;">5</span>), (<span class="dv" style="color: #AD0000;">11</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">12</span>, <span class="dv" style="color: #AD0000;">3</span>), (<span class="dv" style="color: #AD0000;">13</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">13</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">13</span>, <span class="dv" style="color: #AD0000;">2</span>),</span>
<span id="cb14-7">        (<span class="dv" style="color: #AD0000;">13</span>, <span class="dv" style="color: #AD0000;">3</span>), (<span class="dv" style="color: #AD0000;">16</span>, <span class="dv" style="color: #AD0000;">5</span>), (<span class="dv" style="color: #AD0000;">16</span>, <span class="dv" style="color: #AD0000;">6</span>), (<span class="dv" style="color: #AD0000;">17</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">17</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">19</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">19</span>, <span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb14-8">        (<span class="dv" style="color: #AD0000;">21</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">21</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">25</span>, <span class="dv" style="color: #AD0000;">23</span>), (<span class="dv" style="color: #AD0000;">25</span>, <span class="dv" style="color: #AD0000;">24</span>), (<span class="dv" style="color: #AD0000;">27</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">27</span>, <span class="dv" style="color: #AD0000;">23</span>),</span>
<span id="cb14-9">        (<span class="dv" style="color: #AD0000;">27</span>, <span class="dv" style="color: #AD0000;">24</span>), (<span class="dv" style="color: #AD0000;">28</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">29</span>, <span class="dv" style="color: #AD0000;">23</span>), (<span class="dv" style="color: #AD0000;">29</span>, <span class="dv" style="color: #AD0000;">26</span>), (<span class="dv" style="color: #AD0000;">30</span>, <span class="dv" style="color: #AD0000;">1</span>), (<span class="dv" style="color: #AD0000;">30</span>, <span class="dv" style="color: #AD0000;">8</span>),</span>
<span id="cb14-10">        (<span class="dv" style="color: #AD0000;">31</span>, <span class="dv" style="color: #AD0000;">0</span>), (<span class="dv" style="color: #AD0000;">31</span>, <span class="dv" style="color: #AD0000;">24</span>), (<span class="dv" style="color: #AD0000;">31</span>, <span class="dv" style="color: #AD0000;">25</span>), (<span class="dv" style="color: #AD0000;">31</span>, <span class="dv" style="color: #AD0000;">28</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">2</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">8</span>),</span>
<span id="cb14-11">        (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">14</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">15</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">18</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">20</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">22</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">23</span>),</span>
<span id="cb14-12">        (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">29</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">30</span>), (<span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">31</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">8</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">9</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">13</span>),</span>
<span id="cb14-13">        (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">14</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">15</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">18</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">19</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">20</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">22</span>),</span>
<span id="cb14-14">        (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">23</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">26</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">27</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">28</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">29</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">30</span>),</span>
<span id="cb14-15">        (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">31</span>), (<span class="dv" style="color: #AD0000;">33</span>, <span class="dv" style="color: #AD0000;">32</span>)]</span>
<span id="cb14-16">    g.add_edges_from(edge_list)</span>
<span id="cb14-17">    <span class="cf" style="color: #003B4F;">return</span> g</span>
<span id="cb14-18"></span>
<span id="cb14-19">g <span class="op" style="color: #5E5E5E;">=</span> build_karate_club_graph()</span></code></pre></div>
</div>
<p>Visualizing our karate club graph:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">pos <span class="op" style="color: #5E5E5E;">=</span> nx.kamada_kawai_layout(g)</span>
<span id="cb15-2">nx.draw(g, pos, with_labels<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span></code></pre></div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/gnns/gnn_from_scratch_files/gnn_from_scratch_35_0.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">png</figcaption><p></p>
</figure>
</div>
<p>We don’t have any node features. So here we’re creating a one-hot vector for each node based on its id. Together, it’d be a single identity matrix for the graph.</p>
<p>At the beginning, only the instructor and president nodes are labelled. Later on each person will join one of the groups headed by these two. So it’s a binary classification, and the only labeled nodes we have are two.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">node_features <span class="op" style="color: #5E5E5E;">=</span>  torch.eye(<span class="dv" style="color: #AD0000;">34</span>) </span>
<span id="cb16-2">labeled_nodes <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">33</span>])  <span class="co" style="color: #5E5E5E;"># only the instructor and the president nodes are labeled</span></span>
<span id="cb16-3">labels <span class="op" style="color: #5E5E5E;">=</span> torch.tensor([<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb16-4"></span>
<span id="cb16-5"><span class="co" style="color: #5E5E5E;"># since our code only works on adjacency matrix and not on edge-list</span></span>
<span id="cb16-6"></span>
<span id="cb16-7">adj_matrix <span class="op" style="color: #5E5E5E;">=</span> torch.from_numpy(nx.adjacency_matrix(g).todense()).<span class="bu" style="color: null;">float</span>()</span>
<span id="cb16-8"></span>
<span id="cb16-9"><span class="co" style="color: #5E5E5E;"># define our gcn model</span></span>
<span id="cb16-10"></span>
<span id="cb16-11">model <span class="op" style="color: #5E5E5E;">=</span> GCNmodel(<span class="dv" style="color: #AD0000;">34</span>, <span class="dv" style="color: #AD0000;">32</span>, <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb16-12"></span>
<span id="cb16-13"><span class="co" style="color: #5E5E5E;"># do a single pass just for a check</span></span>
<span id="cb16-14"></span>
<span id="cb16-15">model(node_features, adj_matrix)</span></code></pre></div>
</div>
<p>Lets get to the meat of it: time to train our model. We create the usual pytorch pipeline. If you’ve worked with pytorch before, this is familiar to you. Even if not, you can get a certain idea if you know some basics of neural networks / backprop.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">optimizer <span class="op" style="color: #5E5E5E;">=</span> torch.optim.Adam(model.parameters(), lr<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb17-2">all_logits <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb17-3"><span class="cf" style="color: #003B4F;">for</span> epoch <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">100</span>):</span>
<span id="cb17-4">    logits <span class="op" style="color: #5E5E5E;">=</span> model(node_features, adj_matrix)</span>
<span id="cb17-5">    <span class="co" style="color: #5E5E5E;"># we save the logits for visualization later</span></span>
<span id="cb17-6">    all_logits.append(logits.detach())</span>
<span id="cb17-7">    logp <span class="op" style="color: #5E5E5E;">=</span> F.log_softmax(logits, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb17-8">    <span class="co" style="color: #5E5E5E;"># we only compute loss for labeled nodes</span></span>
<span id="cb17-9">    loss <span class="op" style="color: #5E5E5E;">=</span> F.nll_loss(logp[labeled_nodes], labels)</span>
<span id="cb17-10"></span>
<span id="cb17-11">    optimizer.zero_grad()</span>
<span id="cb17-12">    loss.backward()</span>
<span id="cb17-13">    optimizer.step()</span>
<span id="cb17-14"></span>
<span id="cb17-15">    <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">'Epoch </span><span class="sc" style="color: #5E5E5E;">%d</span><span class="st" style="color: #20794D;"> | Loss: </span><span class="sc" style="color: #5E5E5E;">%.4f</span><span class="st" style="color: #20794D;">'</span> <span class="op" style="color: #5E5E5E;">%</span> (epoch, loss.item()))</span></code></pre></div>
</div>
<pre><code>    Epoch 0 | Loss: 0.6887
    Epoch 1 | Loss: 0.6823
    Epoch 2 | Loss: 0.6756
    Epoch 3 | Loss: 0.6704
    Epoch 4 | Loss: 0.6653
    Epoch 5 | Loss: 0.6592
    Epoch 6 | Loss: 0.6529
    Epoch 7 | Loss: 0.6465
    Epoch 8 | Loss: 0.6396
    Epoch 9 | Loss: 0.6320
    Epoch 10 | Loss: 0.6239
    Epoch 11 | Loss: 0.6151
    Epoch 12 | Loss: 0.6064
    Epoch 13 | Loss: 0.5973
    Epoch 14 | Loss: 0.5878
    Epoch 15 | Loss: 0.5783
    Epoch 16 | Loss: 0.5686
    Epoch 17 | Loss: 0.5585
    Epoch 18 | Loss: 0.5482
    Epoch 19 | Loss: 0.5382
    Epoch 20 | Loss: 0.5281
    Epoch 21 | Loss: 0.5182
    Epoch 22 | Loss: 0.5085
    Epoch 23 | Loss: 0.4990
    Epoch 24 | Loss: 0.4899
    Epoch 25 | Loss: 0.4810
    Epoch 26 | Loss: 0.4725
    Epoch 27 | Loss: 0.4642
    Epoch 28 | Loss: 0.4560
    Epoch 29 | Loss: 0.4477
    Epoch 30 | Loss: 0.4397
    Epoch 31 | Loss: 0.4331
    Epoch 32 | Loss: 0.4267
    Epoch 33 | Loss: 0.4204
    Epoch 34 | Loss: 0.4143
    Epoch 35 | Loss: 0.4082
    Epoch 36 | Loss: 0.4037
    Epoch 37 | Loss: 0.3994
    Epoch 38 | Loss: 0.3952
    Epoch 39 | Loss: 0.3911
    Epoch 40 | Loss: 0.3873
    Epoch 41 | Loss: 0.3837
    Epoch 42 | Loss: 0.3802
    Epoch 43 | Loss: 0.3767
    Epoch 44 | Loss: 0.3733
    Epoch 45 | Loss: 0.3698
    Epoch 46 | Loss: 0.3670
    Epoch 47 | Loss: 0.3655
    Epoch 48 | Loss: 0.3638
    Epoch 49 | Loss: 0.3620
    Epoch 50 | Loss: 0.3602
    Epoch 51 | Loss: 0.3586
    Epoch 52 | Loss: 0.3571
    Epoch 53 | Loss: 0.3573
    Epoch 54 | Loss: 0.3564
    Epoch 55 | Loss: 0.3544
    Epoch 56 | Loss: 0.3542
    Epoch 57 | Loss: 0.3539
    Epoch 58 | Loss: 0.3536
    Epoch 59 | Loss: 0.3533
    Epoch 60 | Loss: 0.3529
    Epoch 61 | Loss: 0.3525
    Epoch 62 | Loss: 0.3522
    Epoch 63 | Loss: 0.3518
    Epoch 64 | Loss: 0.3514
    Epoch 65 | Loss: 0.3511
    Epoch 66 | Loss: 0.3508
    Epoch 67 | Loss: 0.3505
    Epoch 68 | Loss: 0.3502
    Epoch 69 | Loss: 0.3504
    Epoch 70 | Loss: 0.3498
    Epoch 71 | Loss: 0.3497
    Epoch 72 | Loss: 0.3439
    Epoch 73 | Loss: 0.3194
    Epoch 74 | Loss: 0.2869
    Epoch 75 | Loss: 0.2505
    Epoch 76 | Loss: 0.2138
    Epoch 77 | Loss: 0.1789
    Epoch 78 | Loss: 0.1476
    Epoch 79 | Loss: 0.1206
    Epoch 80 | Loss: 0.0984
    Epoch 81 | Loss: 0.0811
    Epoch 82 | Loss: 0.0682
    Epoch 83 | Loss: 0.0587
    Epoch 84 | Loss: 0.0516
    Epoch 85 | Loss: 0.0459
    Epoch 86 | Loss: 0.0407
    Epoch 87 | Loss: 0.0356
    Epoch 88 | Loss: 0.0307
    Epoch 89 | Loss: 0.0262
    Epoch 90 | Loss: 0.0223
    Epoch 91 | Loss: 0.0191
    Epoch 92 | Loss: 0.0164
    Epoch 93 | Loss: 0.0142
    Epoch 94 | Loss: 0.0124
    Epoch 95 | Loss: 0.0111
    Epoch 96 | Loss: 0.0101
    Epoch 97 | Loss: 0.0093
    Epoch 98 | Loss: 0.0087
    Epoch 99 | Loss: 0.0081</code></pre>
<p>We can see the loss converging. This dataset doesn’t really have a valid set or anything, so there are no metrics to be presented here. But we can visualize them directly which can be fun to see. Here, we can create an animation of the results of each epoch, and watch them fluctuate as the model converges.</p>
<p>This vis code was taken from <a href="https://docs.dgl.ai/en/0.2.x/tutorials/basics/1_first.html">dgl documentation</a>. The dgl docs are a great place to start learning about graph neural networks!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;">import</span> matplotlib.animation <span class="im" style="color: #00769E;">as</span> animation</span>
<span id="cb19-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb19-3"></span>
<span id="cb19-4"><span class="kw" style="color: #003B4F;">def</span> draw(i):</span>
<span id="cb19-5">    cls1color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'#00FFFF'</span></span>
<span id="cb19-6">    cls2color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'#FF00FF'</span></span>
<span id="cb19-7">    pos <span class="op" style="color: #5E5E5E;">=</span> {}</span>
<span id="cb19-8">    colors <span class="op" style="color: #5E5E5E;">=</span> []</span>
<span id="cb19-9">    <span class="cf" style="color: #003B4F;">for</span> v <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="dv" style="color: #AD0000;">34</span>):</span>
<span id="cb19-10">        pos[v] <span class="op" style="color: #5E5E5E;">=</span> all_logits[i][v].numpy()</span>
<span id="cb19-11">        cls <span class="op" style="color: #5E5E5E;">=</span> pos[v].argmax()</span>
<span id="cb19-12">        colors.append(cls1color <span class="cf" style="color: #003B4F;">if</span> cls <span class="cf" style="color: #003B4F;">else</span> cls2color)</span>
<span id="cb19-13">    ax.cla()</span>
<span id="cb19-14">    ax.axis(<span class="st" style="color: #20794D;">'off'</span>)</span>
<span id="cb19-15">    ax.set_title(<span class="st" style="color: #20794D;">'Epoch: </span><span class="sc" style="color: #5E5E5E;">%d</span><span class="st" style="color: #20794D;">'</span> <span class="op" style="color: #5E5E5E;">%</span> i)</span>
<span id="cb19-16">    pos <span class="op" style="color: #5E5E5E;">=</span> nx.kamada_kawai_layout(g)</span>
<span id="cb19-17">    nx.draw_networkx(g.to_undirected(), pos, node_color<span class="op" style="color: #5E5E5E;">=</span>colors,</span>
<span id="cb19-18">            with_labels<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, node_size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">300</span>, ax<span class="op" style="color: #5E5E5E;">=</span>ax)</span>
<span id="cb19-19"></span>
<span id="cb19-20">fig <span class="op" style="color: #5E5E5E;">=</span> plt.figure(dpi<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">150</span>)</span>
<span id="cb19-21">fig.clf()</span>
<span id="cb19-22">ax <span class="op" style="color: #5E5E5E;">=</span> fig.subplots()</span>
<span id="cb19-23">draw(<span class="dv" style="color: #AD0000;">0</span>)  <span class="co" style="color: #5E5E5E;"># draw the prediction of the first epoch</span></span>
<span id="cb19-24">plt.close()</span>
<span id="cb19-25"></span>
<span id="cb19-26">ani <span class="op" style="color: #5E5E5E;">=</span> animation.FuncAnimation(fig, draw, frames<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">len</span>(all_logits), interval<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">200</span>)</span>
<span id="cb19-27"></span>
<span id="cb19-28">ani.save(<span class="st" style="color: #20794D;">"karate.gif"</span>, writer<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"pillow"</span>)</span></code></pre></div>
</div>


</section>

 ]]></description>
  <category>gnn</category>
  <guid>https://shindeshu.github.io/posts/gnns/gnn_from_scratch.html</guid>
  <pubDate>Mon, 24 Jan 2022 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/gnns/graph_img.jpeg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A Brief Intro To Graph Neural Networks</title>
  <dc:creator>Shubham Shinde</dc:creator>
  <link>https://shindeshu.github.io/posts/gnns/intro_to_gnn.html</link>
  <description><![CDATA[ 



<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://shindeshu.github.io/posts/gnns/graph_img.jpeg" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">graph-img</figcaption><p></p>
</figure>
</div>
<p>Deep Learning has revolutionized machine learning on all types of tasks ranging from computer vision to natural language processing or sequence modeling. Most of these applications however involve mostly euclidean data that are constrained to some fixed dimensions.</p>
<p>What happens when your data is of non-euclidean nature? Graphs are one way to represent such non-euclidean data, which represent it in form of objects linked with each other through relationships. Machine learning using graphs has always been around, however with the advances in deep learning, recently there have been some exciting developments for learning on graphs.</p>
<p><img src="https://shindeshu.github.io/posts/gnns/https:/shindeshu.github.io/assets/images/euclidean.png" width="600"></p>
<p>What is a graph, you say? Graph is a set of vertices / nodes (our objects of interest), with edges (relationships between our objects). For example in a social media graph, an account would be a node, and them following someone could be an edge. Numerically, a graph can be represented as a matrix (adjacency), or as a list (of edges).</p>
<p>What data can be represented in the form of graphs? A lot of it! Interactions on a social media site, financial transactions, citation networks, molecules, all these can be represented in the form of graphs and can then be leveraged for machine learning.</p>
<p>Graph representation learning: when we do have a graph (i.e.&nbsp;our nodes, their features, their edges, <em>their</em> features), our objective is to learn embeddings for each node, such that two “similar” nodes will have their embeddings closer in space. This embedding for a node should bake into itself its relationships and its neighbourhood and their features (apart from its own). This embedding vector can then be used for our downstream tasks.</p>
<p><img src="https://shindeshu.github.io/posts/gnns/https:/shindeshu.github.io/assets/images/node_rep_learning.png" width="600"></p>
<p>Learning the embedding: while there are many ways to skin this particular cat, the one that’s hot right now is called “message passing” or a graph convolution layer. The core concept is pretty simple. Lets say our current node of interest, has three neighbours. Each one of these will pass a “message” to our node, this message being the current state of the node. These messages will be aggregated together with our node’s current state, and this will be used to update the node’s state to next state. After covering for all nodes, you’d get a complete pass over the entire graph, for a single graph convolution layer. Different frameworks will have different ways of passing messages, or updating them, but the underlying principle is pretty same.</p>
<p>The details of message passing, we’ll go over in another post- since this is supposed to be a “brief” introduction.</p>



 ]]></description>
  <category>gnn</category>
  <guid>https://shindeshu.github.io/posts/gnns/intro_to_gnn.html</guid>
  <pubDate>Mon, 24 Jan 2022 18:30:00 GMT</pubDate>
  <media:content url="https://shindeshu.github.io/posts/gnns/graph_img.jpeg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
