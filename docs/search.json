[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "getting-started\n\n\ndata-science\n\n\n\n\nHow should newcomers into data science and ML should navigate the waters\n\n\n\n\n\n\nJan 15, 2023\n\n\nShubham Shinde\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngetting-started\n\n\ndeep-learning\n\n\n\n\nA basic intro to CNNs, a code-only tutorial using keras\n\n\n\n\n\n\nJan 15, 2023\n\n\nShubham Shinde\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nai-art\n\n\n\n\nI tried my hands on stable diffusion, trying to create styled images of myself. Here‚Äôs what I found.\n\n\n\n\n\n\nNov 30, 2022\n\n\nShubham Shinde\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngnn\n\n\n\n\nIn this notebook we‚Äôll try to implement a simple message passing neural network (Graph Convolution Layer) from scratch, and a step-by-step introduction to the topic.\n\n\n\n\n\n\nJan 25, 2022\n\n\nShubham Shinde\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngnn\n\n\n\n\nA brief, very brief, introduction to the hot field of graph neural networks.\n\n\n\n\n\n\nJan 25, 2022\n\n\nShubham Shinde\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shubham Shinde",
    "section": "",
    "text": "Hi there!\nI am a data scientist who has been around for a while. I‚Äôve built üõ†Ô∏è solutions for sectors ranging from ‚úàÔ∏è aerospace to üè• insurance. I‚Äôve worked in areas like insurance fraud detection, satellite image segmentation, engine failure prediction, among others. Checkout my resume for more!\nI am passionate about AI, and the way it is changing our world. This is why I love to keep on top of the latest advances - Stable Diffusion, Transformers, RecSys, MLOps, etc. Some of it I try to share back through my blog, where I write everything about AI that is churning in my brain.\nWhen I‚Äôm not doing tech, I love to sing üé§, and read üìô, and scribble ‚úèÔ∏è once in a while. I love history and the classics as well.\n\n\nClick here to check out my blog.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 15, 2023\n\n\nA Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras\n\n\nShubham Shinde\n\n\n\n\nJan 15, 2023\n\n\nA Roadmap for Getting into Data Science\n\n\nShubham Shinde\n\n\n\n\nNov 30, 2022\n\n\nCreating AI Art of Myself With Stable Diffusion + Dreambooth\n\n\nShubham Shinde\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html",
    "href": "posts/gnns/gnn_from_scratch.html",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "",
    "text": "Let‚Äôs Build a GNN\nIf you are unfamiliar with GNNs in general, please go through my small intro blogpost. Message Passing is one of the more popular concepts in GNNs, and that is what we‚Äôll try to implement here. Specifically we are implementing the Graph Convolutional Layer/Network proposed by Kipf et al in 2016. You can go through a detailed blogpost of his or the original paper."
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#representing-a-graph",
    "href": "posts/gnns/gnn_from_scratch.html#representing-a-graph",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Representing a Graph",
    "text": "Representing a Graph\nBefore we start on to Graph convolutions, let‚Äôs first present it out on how do we represent a graph in code. Mathematically, a graph is defined as a tuple of a set of nodes/vertices , and a set of edges/links . Further, each edge is a pair of two vertices, and represents a connection between them.\nVisually, a graph would look something like this:\n\n\n\nThe vertices are , and edges .\nThere are many ways to represent graphs in memory- two of them include ‚Äúadjacency matrix‚Äù (\\(a\\)) and ‚Äúedge list‚Äù. If the number of nodes is \\(n\\), the adjacency matrix is \\(n x n\\). If there‚Äôs an edge from node \\(n_i\\) to \\(n_j\\), the element \\(a_{ij}\\) is equal to 1. Likewise, the other elements of \\(a\\) are populated.\n[[ 0 1 0 0 ]\n [ 1 0 1 1 ]\n [ 0 1 0 1 ]\n [ 0 1 1 0 ]]\nWorking with adjacency matrix for graph operations is easier, although they have their limitations. While established libraries like dgl or pytorch-geometric use edge-list format of data, here we are working with an adjacency matrix."
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#graph-convolutions",
    "href": "posts/gnns/gnn_from_scratch.html#graph-convolutions",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Graph Convolutions",
    "text": "Graph Convolutions\nGraph convolutions are somewhat similar to image convolutions, in that they take their neighbourhood information and aggregate to get a richer understanding of their position. Also, the ‚Äúparameters‚Äù of the filters are shared across the entire image, which is analogous to a graph convolution as well, where the parameters are shared across the graph.\nGCNs rely on the message passing paradigm. Each node has a feature vector associated with it. For a given node u, each of its neighbouring nodes \\(v_i\\) send a message derived from its feature vector to it. All these messages are aggregated alongwith its own feature vector, and this is used to update this node \\(u\\) to get the final feature vector (or embedding)."
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#current-implementation",
    "href": "posts/gnns/gnn_from_scratch.html#current-implementation",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Current Implementation",
    "text": "Current Implementation\nEach node has a feature vector. This feature will be projected using a linear layer, output of which will be the message that each node passes. We will represent the graph as an adjacency matrix, and multiply by the node features (projected) to perform the message passing. This will be divided by the number of neighbours for normalizing, which will give us the output of our first graph convolution layer.\nImporting all our libraries. We are not using libraries like dgl or pytorch-geometric, we will be using plain pytorch. We are also using networkx for manipulating graph.\nWe will be a creating a random matrix as an adjacency matrix. Creating a matrix with uniform_ method and the bernoulli method.\n\nnodes = 10\nnode_features_size = 4\n\nadj = torch.empty(nodes, nodes).uniform_(0, 1).bernoulli()\n\nVisualizing the graph we created with networkx library\n\ngraph = nx.from_numpy_matrix(adj.numpy())\ngraph.remove_edges_from(nx.selfloop_edges(graph))\n\npos = nx.kamada_kawai_layout(graph)\nnx.draw(graph, pos, with_labels=True)\n\n\n\n\npng\n\n\nCreating random features for our nodes. These features will go through a dense layer and then act as our messages.\n\nnode_features = torch.empty(nodes, node_features_size).uniform_(0, 1).bernoulli()#.view(1, nodes, node_features_size)\nnode_features\n\nThe features will pass through a linear layer to create our messages\n\nprojector = nn.Linear(node_features_size, 5)\n\nnode_feat_proj = projector(node_features)\n\nnum_neighbours = adj.sum(dim=-1, keepdims=True)\n\ntorch.matmul(adj, node_feat_proj)/num_neighbours\n\ntensor([[-0.5067, -0.2463, -0.0555,  0.2188,  0.4031],\n            [-0.8397,  0.0945,  0.5124,  0.1179, -0.0296],\n            [-0.6457,  0.2369,  0.5048, -0.0216,  0.1531],\n            [-0.9893,  0.4223,  0.7235,  0.3212, -0.1165],\n            [-0.5876,  0.2246,  0.5227, -0.1519,  0.1979],\n            [-0.6133, -0.0359,  0.2532,  0.0760,  0.2250],\n            [-0.7740,  0.2055,  0.5252,  0.1075,  0.0174],\n            [-0.7827,  0.1653,  0.5654,  0.0135, -0.0155],\n            [-0.8635,  0.3189,  0.6940,  0.0758, -0.0423],\n            [-0.9374,  0.2670,  0.6672,  0.1805, -0.1292]], grad_fn=<DivBackward0>)\n\nadj.shape, node_feat_proj.shape\n\n(torch.Size([10, 10]), torch.Size([10, 5]))"
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#a-note-on-above-multiplication-operation",
    "href": "posts/gnns/gnn_from_scratch.html#a-note-on-above-multiplication-operation",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "A Note on Above Multiplication Operation",
    "text": "A Note on Above Multiplication Operation\nHow it does achieve our objective, i.e.¬†summing up of messages from neighbouring nodes of a particular node?\nFor simplicity, lets take an example where the adj matrix is $ 7 $ and the message matrix is $ 7 $.\nConsider a single row from the adjacency matrix, that corresponds to a node \\(n_i\\). It might look something like \\[\nA = \\begin{bmatrix}\n    0 & 1 & 0 & 0 & 1 & 0 & 1\\\\\n\\end{bmatrix}\n\\]\nAnd the message matrix is \\(7 \\times 5\\). (seven rows, five columns).\nFor this node, we can observe there are edges existent only for nodes \\({2, 5, 7}\\). When we multiple the above matrix with the message/feature matrix, we will get the elements corresponding to those indexes summed up (since others are multiplied by zero), along the second axis of the feature matrix i.e.¬†we will get a \\(1 \\times 5\\) size vector.\nHere, you can see that only the neighbouring nodes‚Äô features have been summed up to get the final d-length vector."
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#putting-it-all-together",
    "href": "posts/gnns/gnn_from_scratch.html#putting-it-all-together",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nNow that we‚Äôve done it step-by-step, let us aggregate the operations together in proper functions.\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_feat, out_feat):\n        super().__init__()\n        self.projector = nn.Linear(in_feat, out_feat)\n\n    def forward(self, node_features, adj):\n        num_neighbours = adj.sum(dim=-1, keepdims=True)\n        node_features = torch.relu(self.projector(node_features))\n        node_features = torch.matmul(adj, node_features)\n        node_features = node_features / num_neighbours\n        node_features = torch.relu(node_features)\n        return node_features\nlayer1 = GCNLayer(node_features_size, 8)\nlayer1(node_features, adj).shape\n\ntorch.Size([10, 8])\n\nlayer2 = GCNLayer(8, 2)\nlayer2(layer1(node_features, adj), adj)\n\ntensor([[0.4279, 0.4171],\n            [0.4724, 0.4304],\n            [0.4318, 0.3761],\n            [0.4315, 0.3860],\n            [0.4520, 0.4132],\n            [0.4449, 0.4049],\n            [0.4346, 0.3827],\n            [0.4614, 0.4176],\n            [0.4446, 0.3860],\n            [0.4068, 0.3582]], grad_fn=<ReluBackward0>)\n\nclass GCNmodel(nn.Module):\n    def __init__(self, in_feat, hid_feat, out_feat):\n        super().__init__()\n        self.gcn_layer1 = GCNLayer(in_feat, hid_feat)\n        self.gcn_layer2 = GCNLayer(hid_feat, out_feat)\n\n    def forward(self, node_features, adj):\n        h = self.gcn_layer1(node_features, adj)\n        h = self.gcn_layer2(h, adj)\n        return h\nmodel = GCNmodel(node_features_size, 12, 2)"
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#solving-a-real-problem",
    "href": "posts/gnns/gnn_from_scratch.html#solving-a-real-problem",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Solving a Real Problem",
    "text": "Solving a Real Problem\nNow that we are able to play around with random data, lets us get to work on some real datasets that we can do basic classification problems on. We will be using the zachary‚Äôs karate club dataset, which is a small dataset of 34 people and the edges include their observed interactions with each other. Our objective: predict which group will each of the people go to once their club is bisected.\n\ndef build_karate_club_graph():\n    g = nx.Graph()\n    edge_list = [(1, 0), (2, 0), (2, 1), (3, 0), (3, 1), (3, 2),\n        (4, 0), (5, 0), (6, 0), (6, 4), (6, 5), (7, 0), (7, 1),\n        (7, 2), (7, 3), (8, 0), (8, 2), (9, 2), (10, 0), (10, 4),\n        (10, 5), (11, 0), (12, 0), (12, 3), (13, 0), (13, 1), (13, 2),\n        (13, 3), (16, 5), (16, 6), (17, 0), (17, 1), (19, 0), (19, 1),\n        (21, 0), (21, 1), (25, 23), (25, 24), (27, 2), (27, 23),\n        (27, 24), (28, 2), (29, 23), (29, 26), (30, 1), (30, 8),\n        (31, 0), (31, 24), (31, 25), (31, 28), (32, 2), (32, 8),\n        (32, 14), (32, 15), (32, 18), (32, 20), (32, 22), (32, 23),\n        (32, 29), (32, 30), (32, 31), (33, 8), (33, 9), (33, 13),\n        (33, 14), (33, 15), (33, 18), (33, 19), (33, 20), (33, 22),\n        (33, 23), (33, 26), (33, 27), (33, 28), (33, 29), (33, 30),\n        (33, 31), (33, 32)]\n    g.add_edges_from(edge_list)\n    return g\n\ng = build_karate_club_graph()\n\nVisualizing our karate club graph:\n\npos = nx.kamada_kawai_layout(g)\nnx.draw(g, pos, with_labels=True)\n\n\n\n\npng\n\n\nWe don‚Äôt have any node features. So here we‚Äôre creating a one-hot vector for each node based on its id. Together, it‚Äôd be a single identity matrix for the graph.\nAt the beginning, only the instructor and president nodes are labelled. Later on each person will join one of the groups headed by these two. So it‚Äôs a binary classification, and the only labeled nodes we have are two.\n\nnode_features =  torch.eye(34) \nlabeled_nodes = torch.tensor([0, 33])  # only the instructor and the president nodes are labeled\nlabels = torch.tensor([0, 1])\n\n# since our code only works on adjacency matrix and not on edge-list\n\nadj_matrix = torch.from_numpy(nx.adjacency_matrix(g).todense()).float()\n\n# define our gcn model\n\nmodel = GCNmodel(34, 32, 2)\n\n# do a single pass just for a check\n\nmodel(node_features, adj_matrix)\n\nLets get to the meat of it: time to train our model. We create the usual pytorch pipeline. If you‚Äôve worked with pytorch before, this is familiar to you. Even if not, you can get a certain idea if you know some basics of neural networks / backprop.\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nall_logits = []\nfor epoch in range(100):\n    logits = model(node_features, adj_matrix)\n    # we save the logits for visualization later\n    all_logits.append(logits.detach())\n    logp = F.log_softmax(logits, 1)\n    # we only compute loss for labeled nodes\n    loss = F.nll_loss(logp[labeled_nodes], labels)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))\n\n    Epoch 0 | Loss: 0.6887\n    Epoch 1 | Loss: 0.6823\n    Epoch 2 | Loss: 0.6756\n    Epoch 3 | Loss: 0.6704\n    Epoch 4 | Loss: 0.6653\n    Epoch 5 | Loss: 0.6592\n    Epoch 6 | Loss: 0.6529\n    Epoch 7 | Loss: 0.6465\n    Epoch 8 | Loss: 0.6396\n    Epoch 9 | Loss: 0.6320\n    Epoch 10 | Loss: 0.6239\n    Epoch 11 | Loss: 0.6151\n    Epoch 12 | Loss: 0.6064\n    Epoch 13 | Loss: 0.5973\n    Epoch 14 | Loss: 0.5878\n    Epoch 15 | Loss: 0.5783\n    Epoch 16 | Loss: 0.5686\n    Epoch 17 | Loss: 0.5585\n    Epoch 18 | Loss: 0.5482\n    Epoch 19 | Loss: 0.5382\n    Epoch 20 | Loss: 0.5281\n    Epoch 21 | Loss: 0.5182\n    Epoch 22 | Loss: 0.5085\n    Epoch 23 | Loss: 0.4990\n    Epoch 24 | Loss: 0.4899\n    Epoch 25 | Loss: 0.4810\n    Epoch 26 | Loss: 0.4725\n    Epoch 27 | Loss: 0.4642\n    Epoch 28 | Loss: 0.4560\n    Epoch 29 | Loss: 0.4477\n    Epoch 30 | Loss: 0.4397\n    Epoch 31 | Loss: 0.4331\n    Epoch 32 | Loss: 0.4267\n    Epoch 33 | Loss: 0.4204\n    Epoch 34 | Loss: 0.4143\n    Epoch 35 | Loss: 0.4082\n    Epoch 36 | Loss: 0.4037\n    Epoch 37 | Loss: 0.3994\n    Epoch 38 | Loss: 0.3952\n    Epoch 39 | Loss: 0.3911\n    Epoch 40 | Loss: 0.3873\n    Epoch 41 | Loss: 0.3837\n    Epoch 42 | Loss: 0.3802\n    Epoch 43 | Loss: 0.3767\n    Epoch 44 | Loss: 0.3733\n    Epoch 45 | Loss: 0.3698\n    Epoch 46 | Loss: 0.3670\n    Epoch 47 | Loss: 0.3655\n    Epoch 48 | Loss: 0.3638\n    Epoch 49 | Loss: 0.3620\n    Epoch 50 | Loss: 0.3602\n    Epoch 51 | Loss: 0.3586\n    Epoch 52 | Loss: 0.3571\n    Epoch 53 | Loss: 0.3573\n    Epoch 54 | Loss: 0.3564\n    Epoch 55 | Loss: 0.3544\n    Epoch 56 | Loss: 0.3542\n    Epoch 57 | Loss: 0.3539\n    Epoch 58 | Loss: 0.3536\n    Epoch 59 | Loss: 0.3533\n    Epoch 60 | Loss: 0.3529\n    Epoch 61 | Loss: 0.3525\n    Epoch 62 | Loss: 0.3522\n    Epoch 63 | Loss: 0.3518\n    Epoch 64 | Loss: 0.3514\n    Epoch 65 | Loss: 0.3511\n    Epoch 66 | Loss: 0.3508\n    Epoch 67 | Loss: 0.3505\n    Epoch 68 | Loss: 0.3502\n    Epoch 69 | Loss: 0.3504\n    Epoch 70 | Loss: 0.3498\n    Epoch 71 | Loss: 0.3497\n    Epoch 72 | Loss: 0.3439\n    Epoch 73 | Loss: 0.3194\n    Epoch 74 | Loss: 0.2869\n    Epoch 75 | Loss: 0.2505\n    Epoch 76 | Loss: 0.2138\n    Epoch 77 | Loss: 0.1789\n    Epoch 78 | Loss: 0.1476\n    Epoch 79 | Loss: 0.1206\n    Epoch 80 | Loss: 0.0984\n    Epoch 81 | Loss: 0.0811\n    Epoch 82 | Loss: 0.0682\n    Epoch 83 | Loss: 0.0587\n    Epoch 84 | Loss: 0.0516\n    Epoch 85 | Loss: 0.0459\n    Epoch 86 | Loss: 0.0407\n    Epoch 87 | Loss: 0.0356\n    Epoch 88 | Loss: 0.0307\n    Epoch 89 | Loss: 0.0262\n    Epoch 90 | Loss: 0.0223\n    Epoch 91 | Loss: 0.0191\n    Epoch 92 | Loss: 0.0164\n    Epoch 93 | Loss: 0.0142\n    Epoch 94 | Loss: 0.0124\n    Epoch 95 | Loss: 0.0111\n    Epoch 96 | Loss: 0.0101\n    Epoch 97 | Loss: 0.0093\n    Epoch 98 | Loss: 0.0087\n    Epoch 99 | Loss: 0.0081\nWe can see the loss converging. This dataset doesn‚Äôt really have a valid set or anything, so there are no metrics to be presented here. But we can visualize them directly which can be fun to see. Here, we can create an animation of the results of each epoch, and watch them fluctuate as the model converges.\nThis vis code was taken from dgl documentation. The dgl docs are a great place to start learning about graph neural networks!\n\nimport matplotlib.animation as animation\nimport matplotlib.pyplot as plt\n\ndef draw(i):\n    cls1color = '#00FFFF'\n    cls2color = '#FF00FF'\n    pos = {}\n    colors = []\n    for v in range(34):\n        pos[v] = all_logits[i][v].numpy()\n        cls = pos[v].argmax()\n        colors.append(cls1color if cls else cls2color)\n    ax.cla()\n    ax.axis('off')\n    ax.set_title('Epoch: %d' % i)\n    pos = nx.kamada_kawai_layout(g)\n    nx.draw_networkx(g.to_undirected(), pos, node_color=colors,\n            with_labels=True, node_size=300, ax=ax)\n\nfig = plt.figure(dpi=150)\nfig.clf()\nax = fig.subplots()\ndraw(0)  # draw the prediction of the first epoch\nplt.close()\n\nani = animation.FuncAnimation(fig, draw, frames=len(all_logits), interval=200)\n\nani.save(\"karate.gif\", writer=\"pillow\")"
  },
  {
    "objectID": "posts/gnns/intro_to_gnn.html",
    "href": "posts/gnns/intro_to_gnn.html",
    "title": "A Brief Intro To Graph Neural Networks",
    "section": "",
    "text": "Deep Learning has revolutionized machine learning on all types of tasks ranging from computer vision to natural language processing or sequence modeling. Most of these applications however involve mostly euclidean data that are constrained to some fixed dimensions.\nWhat happens when your data is of non-euclidean nature? Graphs are one way to represent such non-euclidean data, which represent it in form of objects linked with each other through relationships. Machine learning using graphs has always been around, however with the advances in deep learning, recently there have been some exciting developments for learning on graphs.\n\nWhat is a graph, you say? Graph is a set of vertices / nodes (our objects of interest), with edges (relationships between our objects). For example in a social media graph, an account would be a node, and them following someone could be an edge. Numerically, a graph can be represented as a matrix (adjacency), or as a list (of edges).\nWhat data can be represented in the form of graphs? A lot of it! Interactions on a social media site, financial transactions, citation networks, molecules, all these can be represented in the form of graphs and can then be leveraged for machine learning.\nGraph representation learning: when we do have a graph (i.e.¬†our nodes, their features, their edges, their features), our objective is to learn embeddings for each node, such that two ‚Äúsimilar‚Äù nodes will have their embeddings closer in space. This embedding for a node should bake into itself its relationships and its neighbourhood and their features (apart from its own). This embedding vector can then be used for our downstream tasks.\n\nLearning the embedding: while there are many ways to skin this particular cat, the one that‚Äôs hot right now is called ‚Äúmessage passing‚Äù or a graph convolution layer. The core concept is pretty simple. Lets say our current node of interest, has three neighbours. Each one of these will pass a ‚Äúmessage‚Äù to our node, this message being the current state of the node. These messages will be aggregated together with our node‚Äôs current state, and this will be used to update the node‚Äôs state to next state. After covering for all nodes, you‚Äôd get a complete pass over the entire graph, for a single graph convolution layer. Different frameworks will have different ways of passing messages, or updating them, but the underlying principle is pretty same.\nThe details of message passing, we‚Äôll go over in another post- since this is supposed to be a ‚Äúbrief‚Äù introduction."
  },
  {
    "objectID": "posts/roadmap/start.html",
    "href": "posts/roadmap/start.html",
    "title": "A Roadmap for Getting into Data Science",
    "section": "",
    "text": "It‚Äôs no news nowadays that folks are flocking towards data science- there are quite many aspiring data scientists around who want to do AI things. I wrote this post as a starter for those beginner data scientists who want to break in. This post does not discuss whether you should, or about the quantum of opportunity in this space, that‚Äôs a different topic.\nThis post also does not give you a big list of links. We try to create a structure of our learning process, and in the end give a few resources.\n\n\n\nImages generated using Stable Diffusion"
  },
  {
    "objectID": "posts/roadmap/start.html#what-is-dsmlai",
    "href": "posts/roadmap/start.html#what-is-dsmlai",
    "title": "A Roadmap for Getting into Data Science",
    "section": "What is DS/ML/AI?",
    "text": "What is DS/ML/AI?\nDefinitions are of no help here. There aren‚Äôt well-defined boundaries around these terms, no self-evident ground truths that distinguish ML from AI or DS.\nBroadly, the field is about learning patterns from data to solve various business problems. Take for instance, price of a house. Traditionally, you could make up basic if-else rules that incorporate business logic to get a value for a house price. With data science you could utilize algorithms to learn these rules and predict these values given the attributes of a house. Similarly, you could\n\nPredict when a given machine part could fail\nFlag a Credit Card Fraud\nAssess Credit-Worthiness given financial attributes of an individual\n\nAbility to identify complex patterns in a data is useful in almost every domain that has any significant data, and underlying structure in it.\n\nSome Real Use Cases of Data Science\n\nRead how Aviation Industry Uses DS for Optimizing their Business Activities\nRead how Pratt and Whitney Uses DS for Predictive Maintenance of Engines\n\n\n\nTypes of Data - Tabular, Image and Text\nThe examples cited above are of tabular data. As the name explains, it is data that‚Äôs stored in a tabular format. This is the most common data format, and easier to work on as well. But there‚Äôs also image data and text data, both are also increasingly forming the proportion of useful industrial models.\n\nOn Pinterest, the images that get recommended to you are based on ML models.\nFacebook, Twitter recommending feeds to you are recommended using ML models as well.\nFace Recognition softwares are used worldwide.\nChatbots, Translation all use text-based ML models underneath them."
  },
  {
    "objectID": "posts/roadmap/start.html#an-overview-of-requirements",
    "href": "posts/roadmap/start.html#an-overview-of-requirements",
    "title": "A Roadmap for Getting into Data Science",
    "section": "An Overview of Requirements",
    "text": "An Overview of Requirements\nWhat do you need to become a data scientist, and what do you do on a day-to-day basis on your job?\nI‚Äôd divide this into four topics - Technical Skills, Theoretical Knowledge, Domain and Business Knowledge, and Project Management. For a beginner, we‚Äôd be only interested in the first two.\n\nTechnical Skills\nThese are divided into parts.\n\nSoftware Engineering: You should know how to code.\nTools Specific to Data Science: Know thy tools!\n\n\n\nTheoretical Knowledge\n\nTheory of Data Science: Learn from the wisdom of ancients!\nMathematics: Basics will do fine."
  },
  {
    "objectID": "posts/roadmap/start.html#structure-your-machine-learning-journey",
    "href": "posts/roadmap/start.html#structure-your-machine-learning-journey",
    "title": "A Roadmap for Getting into Data Science",
    "section": "Structure Your Machine Learning Journey",
    "text": "Structure Your Machine Learning Journey\n\n\n\n.\n\n\nHere, we‚Äôll expand on what are these skills, and how do you go about acquiring them, and in what order.\nPython: For a beginner, the first step should be to learn how to code. Python is the most used language in this space, and it is great language for a first-time coder to learn. In fact, the ease of coding is exactly the reason why python came to dominate the machine learning landscape. This is a good free resource to start learning.\nBasics of the language are enough to get started- you don‚Äôt need to leetcode yourself. You do need to get a hang of jupyter notebooks, an IDE like VS-Code, and be able to install packages.\nTabular vs Deep Learning: As a newcomer, you should aim to get proficient in tabular data before you move to tackling image or text datasets.\nTools: Knowing pandas (library that handles tabular data) and matplotlib/seaborn (data visualization library) is another starting step. Knowing basic methods is enough at this stage.\nNow that we have warmed up, we‚Äôll step into the main process and discuss the sources to learn from.\n\nThe Problem Of Plenty\nWhere to learn from? There is just so much to choose, so many courses, books, websites, twitter influencers, linkedin influencers, an overflowing bookmarks folder, pdfs of unread papers. The progress is overwhelming too- What‚Äôs a standard practice a year ago may not be the same today.\nThis can be overwhelming for a veteran, let alone a beginner. Separating the noise from the signal can be daunting, as well as finding the best possible source to learn from.\nSpeaking from experience, what works best for me is learning on the job. Whatever projects I‚Äôve worked on, that knowledge stays with me forever. Courses and Books are tougher to retain. So if you‚Äôre employed, give your best in your own project, it‚Äôs the best way to upskill.\nHowever, if you‚Äôre not employed, or your job is narrow in its scope and you want to upskill, the problem persists. There should be a process to identify and follow the materials. (My personal recommendations are at the end of this post.)\nCourses: Instead of sampling and browsing dozens of courses, it is better to simply pick one and get started. The time you waste in finding the perfect course can be better utilized in completing an okay course. Most important thing is to get moving and not be stagnant.\nBooks: There are some books that are structured as tutorials. Don‚Äôt delve deep into theory at this stage, if a book contains code snippets, is a latest copy, and is easy to read - that‚Äôs the one.\nWebsites and Blogs: Keeping browsing medium, towardsdatascience, kdnuggets, etc. Keep it light."
  },
  {
    "objectID": "posts/roadmap/start.html#blind-men-and-the-elephant",
    "href": "posts/roadmap/start.html#blind-men-and-the-elephant",
    "title": "A Roadmap for Getting into Data Science",
    "section": "Blind Men and the Elephant",
    "text": "Blind Men and the Elephant\nDue to the problem of plenty, it is important to keep things simple. Don‚Äôt follow too many threads, pick a few sources and stick with it. Course- pick any one and complete it without distraction, but remember that this is not everything, and it has only shown you a part of the elephant. Much like the parable of the blind men and the elephant, where each of the men describe the elephant differently since they‚Äôre feeling a different part of the elephant.\nDoing an online course could show you one part of this elephant- the theory behind the many algorithms like linear regression, random forest. There are many helpful books too that can do this. To cover other topics you‚Äôll need other things.\n\n\n\nUnderstanding the Elephant\n\n\n\nIt Can Get Overwhelming\nI know that‚Äôs a lot of stuff, right? Data science can be overwhelming for a veteran, let alone one who‚Äôs just starting their journey. There is so much to learn, from the theory to the latest tools, the latest research, etc. Which is why it is important to remember that it is not necessary to learn everything."
  },
  {
    "objectID": "posts/roadmap/start.html#list-of-topics",
    "href": "posts/roadmap/start.html#list-of-topics",
    "title": "A Roadmap for Getting into Data Science",
    "section": "List of Topics",
    "text": "List of Topics\nThis is a rough outline of how your learning journey could look like. Not exhaustive.\n\nPython, SQL (Basics)\nPandas and Matplotlib (Basics)\n\nReading and basic operations on dataframes\nBasic plotting\n\nData Science Theory\n\nSupervised vs Unsupervised Learning\nDifferent ways of representing data (encoding, binning)\nsupervised learning algorithms (linear regression, trees)\nunsupervised learning algorithms (clustering, PCA)\nmodel evaluation and metrics\noverfitting, underfitting\nensembles, correlation, feature engineering, outliers, regularization\n\nPractice\n\nuse datasets to create models\ndata visualization and EDA\n\nFull-Fledged Projects\n\nThis is a rough outline of the process. My recommended resource to start with is the book Introduction to Machine Learning using Python by Andreas Muller. It is a very easy-to-read book that contains code snippets and gradually introduces all topics. You can also complete the Kaggle Courses at here, they‚Äôre quite friendly and brief tutorials, plus you can run them in your browser itself.\nThis outline doesn‚Äôt include deep learning (computer vision or natural language processing), or model deployment."
  },
  {
    "objectID": "posts/roadmap/start.html#a-practice-first-discipline",
    "href": "posts/roadmap/start.html#a-practice-first-discipline",
    "title": "A Roadmap for Getting into Data Science",
    "section": "A Practice-First Discipline",
    "text": "A Practice-First Discipline\n\n\n\nLearn by Doing! Image by mastervector on freepik\n\n\nThe first two blocks are in place, the coding environment and the theory of data science. DS is a very practice-oriented discipline. Theoretical knowledge constitutes a small proportion of the data scientist mindspace. It is in the dirty trenches of the notebooks where the real are proven.\nIf your resume contains just a bunch of courses, it does not give an interviewer any idea of your prowess. But having a project, notebooks, repos is a sign that you have been there and done those things that matter the most. What things, you ask?\nReading data, understanding, cleaning, processing, preparing is the single most important aspect of data science that is often not given the necessary important even by those long in the game. These skills cannot be taught in any course or book, but be learned by doing.\nExploratory Data Analysis (EDA) is a term you‚Äôll come across quite often, and it is an art more than a science, another skill that you‚Äôll learn through practice. Understanding the contents of your data, asking it the right questions and making it answer, this is our bread and butter.\nThen there are feature engineering, dealing with imbalanced data, model training, model evaluation, all skills which are practical in nature. Fortunately for us, there is a single place to learn all of the above- Kaggle.\nKaggle is a website where you can train machine learning models for free using many datasets available there. (It also hosts competitions, but at this point we are focussed on learning). Simply copy-and-editing someone else‚Äôs publicly available notebooks can be incredibly helpful to understand various work flows. Spend your time creating a chimaera of a dozen code blocks from dozen notebooks in a single task, and it will be exponentially helpful from a learning perspective.\n\nDon‚Äôt Heed the Kaggle Haters\nThere has been criticism about Kaggle that it is not helpful for a data science job, since you always get a clean dataset in hand which is never the case in real life. While it is true that real life data is much messier than Kaggle, practicing in Kaggle has much more returns than handicap. You learn to max out your feature engineering skills, data visualization and EDA skills, and are also able to learn how to rapid iterations. Kaggle is helpful without a doubt.\nYou also get to learn about the latest techniques, libraries, models, you observe empirical evidence on what methods work and what don‚Äôt."
  },
  {
    "objectID": "posts/roadmap/start.html#projects",
    "href": "posts/roadmap/start.html#projects",
    "title": "A Roadmap for Getting into Data Science",
    "section": "Projects",
    "text": "Projects\nOnce you are able to write a kaggle notebook from start to end, you have pretty much achieved the baseline of what you set out to do. So if you‚Äôre at this stage, you should sit back and congratulate yourself.\nIf you want to graduate to the next level, you should start doing full fledged projects that solve a meaningful problem. Here, you can use data that‚Äôs not from kaggle for a challenge.\nThis includes some advanced topics that can be skipped by a beginner, but if you are employed as a data scientist, these would be quite important.\nHow would this differ from a kaggle project\n\nwork with .py files instead of a notebook\nwrite comments, docstrings in order to be helpful to the reader\nuse sklearn pipelines in order to save the pre-processing steps, making it easier for inference.\nwrite modular, readable code.\nlearn about code-writing guidelines. PEP8. Use black, isort, etc. tools.\nwrite tests."
  },
  {
    "objectID": "posts/roadmap/start.html#domain-knowledge",
    "href": "posts/roadmap/start.html#domain-knowledge",
    "title": "A Roadmap for Getting into Data Science",
    "section": "Domain Knowledge",
    "text": "Domain Knowledge\nThis section is not applicable for a complete beginner.\nNobody pays you to fit a model to a csv. The purpose of data scientists, at the end of the day, is to use their skills to add value to the business. This purpose, and that of understanding the data, formulating problems to solve, and evaluating solutions, all require you to have knowledge of the business systems that you work in. This includes industry knowledge, how processes flow inside your own firm, the meaning behind the data, and more. Acquiring this knowledge needs communication skills, and ability to ask the right questions.\nUnlike specialists, data scientists also need to rotate over different businesses. If today you are building a recommender system for ecommerce, tomorrow you could be building a failure prediction model for aircraft engines. The variety of domains we operate in is vast, so the ability to quickly onboard is important too."
  },
  {
    "objectID": "posts/roadmap/start.html#deep-learning",
    "href": "posts/roadmap/start.html#deep-learning",
    "title": "A Roadmap for Getting into Data Science",
    "section": "Deep Learning",
    "text": "Deep Learning\nWell, we only learned about tabular data! Where‚Äôs the cool stuff about neural networks, chat-GPT? This post was supposed to include deep learning topics as well, however the length has gotten too long hence they‚Äôll be covered in a separate post.\nHowever if you need a quick tutorial on them, you can follow the kaggle courses.\nSecond place to grokk deep learning is reading and running public kernels on kaggle. Third place is the official documentation of libraries like keras.\nDeep learning is a very empirical field where practice and theory go hand-in-hand. The traditional serial approach of reading up theory and then taking up examples will not work for deep learning, they have to go together. It is best if, as a beginner, you don‚Äôt go too deep into theoretical aspects of deep learning before getting hands-on experience with their applications."
  },
  {
    "objectID": "posts/roadmap/start.html#resources",
    "href": "posts/roadmap/start.html#resources",
    "title": "A Roadmap for Getting into Data Science",
    "section": "Resources",
    "text": "Resources\nIn order to avoid the problem of plenty, I will keep this list as short as possible.\n\nBook - Introduction to Machine Learning by Andreas Muller. This covers theory and practice at the same time while being beginner friendly.\nCourse - Most courses like that of Andrew Ng are quite heavy on mathematics and the ‚Äúbackend‚Äù of ML algorithms. I personally think this is not a good place to start for a newcomer. However if you have a background in STEM, you might like this approach of thinking from first principles.\nKaggle Learn - Most people know kaggle for building models and competitions. But kaggle also has a courses section where there are easy-to-follow and brief tutorials on various topics like pandas, data visualization, etc. They are fantastic for a beginner.\nKaggle Notebooks - Spending time here will teach you about almost everything about model building.\n\nThis is all for this beginner‚Äôs notebook. Good luck to your machine learning journey!"
  },
  {
    "objectID": "posts/stable_diff/getting_started_sd.html",
    "href": "posts/stable_diff/getting_started_sd.html",
    "title": "Creating AI Art of Myself With Stable Diffusion + Dreambooth",
    "section": "",
    "text": "For years, style transfer and image generation was the domain of an architecture called GANs. It produced results that were, well, not enough to impress a person not into the topic. And it look a lot of time, compute, and didn‚Äôt always converge. That is history anyway, so we do not need to trouble ourselves.\nThis year changed everything in generative AI. Three models, particularly Stable Diffusion took the world of art by storm. The text-to-image model could produce images with high fidelity, coherence, and able to produce images with so many styles. It was a 4GB checkpoint that contained in itself majority of the visual world created by man. With a bunch of text it could bring imagination to reality."
  },
  {
    "objectID": "posts/stable_diff/getting_started_sd.html#the-magic",
    "href": "posts/stable_diff/getting_started_sd.html#the-magic",
    "title": "Creating AI Art of Myself With Stable Diffusion + Dreambooth",
    "section": "The Magic",
    "text": "The Magic\nAll of these images are generated by AI completely on its own. There are some sites like lexica.art where you can scroll through tons of AI-generated art like this, as well as the prompts that led to them.\n\n\n\nSome Examples of AI generated art\n\n\nYou can try it for yourself at any one of the many huggingface spaces:\n\nApp for Stability Diffusion 1.5\nApp for Stability Diffusion 2, recently released\nApp that runs models for many styles\n\nIt can take 10-20s to generate an image.\nIf you prefer a more hands-on approach, use this colab notebook - fast_stable_diffusion_AUTOMATIC1111.ipynb, in this notebook, give the path to model hosted on huggingface (I used runwayml/stable-diffusion-v1-5 for my base model). The notebook has all the code for installing dependencies, and will launch a GUI which you can use for experimentation."
  },
  {
    "objectID": "posts/stable_diff/getting_started_sd.html#dreambooth",
    "href": "posts/stable_diff/getting_started_sd.html#dreambooth",
    "title": "Creating AI Art of Myself With Stable Diffusion + Dreambooth",
    "section": "Dreambooth",
    "text": "Dreambooth\nHowever, the publicly released stable diffusion model includes only those concepts, people, artstyles that were present in the training dataset of it. What if you want to add new people/concepts/styles, and use them to generate art?\nThat‚Äôs where dreambooth comes in. This technique allows you to add a person/concept to the stable diffusion model using only ~20 images of it, and takes only ~1 hour on free colab to train! Because of dreambooth, fine-tuning the stable diffusion model on a given style or person is a breeze."
  },
  {
    "objectID": "posts/stable_diff/getting_started_sd.html#finetuning-approach",
    "href": "posts/stable_diff/getting_started_sd.html#finetuning-approach",
    "title": "Creating AI Art of Myself With Stable Diffusion + Dreambooth",
    "section": "Finetuning Approach",
    "text": "Finetuning Approach\nMy primary resource for fine-tuning, i.e.¬†adding myself to the stable diffusion space was this blog post: How to Use DreamBooth to Fine-Tune Stable Diffusion (Colab). Which points to a beautiful colab notebook - TheLastBen/fast-stable-diffusion, which has it all ready- you only need to give the links to the base model, huggingface token (you need to signup to this website), and upload your images, and hit train.\nFollow the advice in the blogpost and the notebook, and you‚Äôll have a gradio app in your browser. Now you can add any text prompt you want in the text box, and generate results!"
  },
  {
    "objectID": "posts/stable_diff/getting_started_sd.html#finding-the-right-prompts",
    "href": "posts/stable_diff/getting_started_sd.html#finding-the-right-prompts",
    "title": "Creating AI Art of Myself With Stable Diffusion + Dreambooth",
    "section": "Finding the Right Prompts",
    "text": "Finding the Right Prompts\nYeah, except, any prompt will not give you the best results. Prompt Engineering is a big deal here. You have to construct a prompt, add labels and styles in order to direct the model to get an acceptable image. You also can tinker with the hyperparameters- adjust the CFG scale, sampling steps, to get better results.\nMy experience- prompt engineering sucks. It‚Äôs more of an art that engineering, and there‚Äôs no other way but trial and error. Fortunately there are resources on the internet that can help you.\nFor me the process looked like:\n\nGo to a resource like lexica.art, and find an image you like\nCopy the prompt and settings, and modify the prompt to include your token used during training.\n\nThis will generally give you good results, and how I got most of my good images. However tuning of hyperparameters requireds trial and error, and hence time. I didn‚Äôt spend too much time on this, so I fixed my sampling method at euler-a and steps at 40-50. I only tinked with CFG scale."
  },
  {
    "objectID": "posts/stable_diff/getting_started_sd.html#results-on-myself",
    "href": "posts/stable_diff/getting_started_sd.html#results-on-myself",
    "title": "Creating AI Art of Myself With Stable Diffusion + Dreambooth",
    "section": "Results on Myself",
    "text": "Results on Myself\nFinally, the meat of the post- let‚Äôs see the actual results on myself.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs a Naruto(?) Character\n\n\n\n\n\n\n\nStyle of Japanese Woodblock\n\n\n\n\n\n\n\nMe As a Pixar Character\n\n\n\n\n\n\nSome Portraits of Me\n\n\n\nSome generated art of my wife and me.\n\n\n\n\n\n\nAs a Disney Princess\n\n\n\n\n\n\n\nAs a Studio Ghibli Character\n\n\n\n\n\n\n\nAs Pixar Characters\n\n\n\n\n\n\n\n\n\nAnime Character\n\n\n\n\n\n\n\nStylized Portrait\n\n\n\n\n\n\n\nAs a Japanese Woodblock"
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html",
    "href": "posts/tutorials/keras_cnns_intro.html",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "",
    "text": "Computer Vision\n\n\nThis was originally a  notebook.\nIn a nutshell, CNNs are special deep learning architectures that have revolutionized the field of computer vision. Computer Vision is a field that is concerned about deriving information from images using computers. Some examples of computer vision include- Identifying whether a given image contains an item, like identifying a pedestrian in traffic, identifying cracks in an industrial machine, identifying if an X-Ray is abnormal. Self-Driving cars rely on computer vision algorithms, which are often CNNs. Filters on Instagram, face recognition systems, all use deep learning under the hood.\nBut some general types of computer vision problems are:\n\nImage Classification (Is this image of a cat?)\nObject Detection (Is there a cat in this image, and WHERE exactly is it?)\nSegmentation (give me the exact outline of the cat in this image, if it exists)\n\n\n\n\nComputer Vision Tasks\n\n\n\n\nIf you‚Äôre here, it probably means you‚Äôre familiar with basic machine learning concepts- like training data, predictions, feature engineering, etc. on tabular data. However image data is different from tabular data due to having a different structure, hence traditional algorithms like random forest cannot be used for classifying images.\n\n\n\nThis is where CNNs made the breakthrough, and achieved tremendous results on image data. Before CNNs, image analytics required a lot of feature engineering and pre-processing (tons of hand-made filters). CNNs outperformed all the traditional methods without requiring such feature engineering. CNNs learnt the features and filters by itself. All you had to do was feed a lot of data to the model."
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#deep-learning-and-neural-networks",
    "href": "posts/tutorials/keras_cnns_intro.html#deep-learning-and-neural-networks",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "Deep Learning and Neural Networks",
    "text": "Deep Learning and Neural Networks\nCNN, that is, Convolutional Neural Networks are a subfield of neural networks, a family of algorithms. A neural network is a collection of nodes or neurons, where each neuron has a weight*. These weights are learnt during the training process such that the model is able to predict the output when input is given. When a lot of such neurons are stacked together, we get a neural network. A neural network with a lot of layers would be called deep neural network, a phenomenon which has driven majority of the AI success in the last decade.\nIn CNNs, the neurons are arranged and stacked in a manner suitable for images.\n\nConvolutional Neural Networks\nIn CNN, we have filters (which are tiny 3x3 matrices) which ‚Äúconvolve‚Äù an image to get a transformed matrix. We won‚Äôt worry about the theory and filters here. All you need to know, that filters transform the image to a new matrix. This matrix is made smaller by a method called Pooling. These two operations create one Convolution Layer, and several such Layers create a CNN. This order isn‚Äôt mandatory, as we‚Äôll see later.\nThis is a nice animation that showcases the convolution operation.\n\n\n\n\n\n\nTransfer Learning\nThere‚Äôs a neat trick in deep learning called transfer learning- which is covered at the end of the notebook in case you make it.\nThat‚Äôs quite a lot of theory, on to the problem at hand."
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#problem-at-hand",
    "href": "posts/tutorials/keras_cnns_intro.html#problem-at-hand",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "Problem At Hand",
    "text": "Problem At Hand\nThe task at hand is an image classification task. You‚Äôre given a ton of images that are either a cat image or a dog image. Now, if you give a new image, you should be able to predict if it‚Äôs of a dog or a cat.\nWe are going to train a CNN to do this. Using the keras library."
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#typical-workflow",
    "href": "posts/tutorials/keras_cnns_intro.html#typical-workflow",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "Typical Workflow",
    "text": "Typical Workflow\nTypically when you work on a CNN task, this is how your notebook flow will look like: Whatever time you spend with CNNs, it will be in one of these sections.\n\nGet the Images\n\n(collecting images itself can be either a herculean task or sometimes ready-made data is available, time and effort varies with dataset)\nDifficulty Level: Varies Time Needed: Varies\n\nLook at the Images and the Targets\n\n(see how the images actually look like, what are the classes, how many of them.)\n\nCreate a Data Loader\n\n(in most libraries you need a guy that reads the images and feeds to the model, and does the intermediate work- batching, augmentation, split, multiprocessing, etc. configuring this step will be a good chunk of your time )\n\nDefine Your Model\n\n(how many CNN layers? How many filters, the optimizer, the loss function? this could be as easy as downloading/pasting an existing model in ten minutes, or the experiments could go on forever)\n\nTrain the Model\n\n(now throw the dataloader function on the model and let it train. sit back and sip coffee.)\n\nGet The Predictions\n\n(here you actually use the model. for some task, or just to check if it‚Äôs doing good. evaluating whether the model is giving good predictions can also be challenging in some use cases.)\n\nDebugging\n\nIf you keep it simple, load pre-built modules, the model will work. But there could be many possible problems that might arise in the task. These will be covered at the end.\n::: {.cell _cell_guid=‚Äòb1076dfc-b9ad-4769-8c92-a6c4dae69d19‚Äô _uuid=‚Äò8f2839f25d086af736a60e9eeb907d3b93b6e0e5‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2023-01-07T11:37:46.774008Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2023-01-07T11:37:46.773068Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2023-01-07T11:37:52.956681Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2023-01-07T11:37:52.955647Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:6.19509,‚Äúend_time‚Äù:‚Äú2023-01-07T11:37:52.959543‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2023-01-07T11:37:46.764453‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[]‚Äô execution_count=1}\nimport numpy as np\nimport pandas as pd \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport random\nimport os\nfrom PIL import Image\nimport glob\n:::\n\n!ls ../input/dogs-vs-cats/\n\n# the images are in a zip file in this folder\n\nsampleSubmission.csv  test1.zip  train.zip"
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#get-the-images",
    "href": "posts/tutorials/keras_cnns_intro.html#get-the-images",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "1. Get the Images",
    "text": "1. Get the Images\nExtract the images from the zip file. Now there are two folders: train/ and test1/\n\nimport zipfile\n\nwith zipfile.ZipFile(\"../input/dogs-vs-cats/train.zip\",\"r\") as z:\n    z.extractall(\".\")\n    \nwith zipfile.ZipFile(\"../input/dogs-vs-cats/test1.zip\",\"r\") as z:\n    z.extractall(\".\")\n!ls /kaggle/working/\n\n__notebook__.ipynb  test1  train\n\n\n\n# let's look at any five files in the train folder\nglob.glob(\"/kaggle/working/train/*.jpg\")[:5]\n\n['/kaggle/working/train/dog.8701.jpg',\n '/kaggle/working/train/dog.9481.jpg',\n '/kaggle/working/train/cat.104.jpg',\n '/kaggle/working/train/cat.8800.jpg',\n '/kaggle/working/train/dog.6199.jpg']\n\n\nYou now notice that the label is encoded in the filename itself- cat or dog! We need to extract that to be able to train the model.\nWe are collecting each filename, and the corresponding label in a pandas dataframe (needed later)\n\ntrain_files = glob.glob(\"/kaggle/working/train/*.jpg\")\ntrain_labels = [i.strip('/kaggle/working/train/')[:3] for i in train_files]\ntrain_df = pd.DataFrame({'filename': train_files, 'class': train_labels})\ntrain_df.head()\n\n\n\n\n\n  \n    \n      \n      filename\n      class\n    \n  \n  \n    \n      0\n      /kaggle/working/train/dog.8701.jpg\n      dog\n    \n    \n      1\n      /kaggle/working/train/dog.9481.jpg\n      dog\n    \n    \n      2\n      /kaggle/working/train/cat.104.jpg\n      cat\n    \n    \n      3\n      /kaggle/working/train/cat.8800.jpg\n      cat\n    \n    \n      4\n      /kaggle/working/train/dog.6199.jpg\n      dog\n    \n  \n\n\n\n\nFirst step is done- we have the images, and we have the labels. Let‚Äôs move to the second step."
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#look-at-them",
    "href": "posts/tutorials/keras_cnns_intro.html#look-at-them",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "2. Look at Them!",
    "text": "2. Look at Them!\nWe now observe what the images look like. We‚Äôll look at four random images from the data.\n\nfig, axs = plt.subplots(2, 2, figsize=(10,10))\naxs = axs.ravel()\nfor i in range(0,4):\n    idx = random.choice(train_df.index)\n    axs[i].imshow(Image.open(train_df['filename'][idx]))\n    axs[i].set_title(train_df['class'][idx])\n\n\n\n\nThis is a pretty clean dataset- that‚Äôs good. The subjects of the image are in center, occupy a majority of the image, no blurriness or anything. (Ideally you‚Äôd look at more images than a dozen tho,:) )\nYou might‚Äôve noticed that the dimensions of the images are not constant, they vary a lot. This will be a problem for the CNN- it expects images of a fixed size! How do we take care of it? We don‚Äôt, our data loader guy will do it for us.\n(Well, sometimes we might want to do it ourselves, if we think the resizing needs extra attention- but in this case, we‚Äôll let it be automated.)"
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#release-the-loader",
    "href": "posts/tutorials/keras_cnns_intro.html#release-the-loader",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "3. Release the Loader!",
    "text": "3. Release the Loader!\nDataloaders are the unsung heroes of the CNN world- they take care of a lot of nasty work that would be a nightmare if we had to do it by hand. What they essentially do is simple- read the data from the disk, and feed to the model. But under the hood they take care of many things, like‚Ä¶\n\nResizing, as we discussed. Ensuring that each image is of a fixed size.\nBatching. Feeding images one-by-one to the model is tedious, would take a lot of time. It‚Äôs better to feed a large number of them at once (as much your computer will allow)\nLabel Encoding. Computers don‚Äôt understand string like ‚Äòcat‚Äô or ‚Äòdog‚Äô, you have to convert them to numbers like 0 or 1.\nData Augmentation. Create more images by slightly modifying an image (flipping it, rotating it teeny bit, adding some spots, etc.)\nValidation Split. Keras now has support for validation splitting.\nSometimes the data is too large to fit into memory (10+ GB, say), then loaders can iterate through the dataset on disk chunk-by-chunk instead of loading everything at once.\n\nWe are using keras‚Äôs ImageDataGenerator to create our training data loader.\nTwo steps:\n\nDefine a ImageDataGenerator instance, and specify the augmentation strategies.\nCreate a generator from this instance by specifying the image file paths and labels. Pass this generator to the model for training.\n\nIn pytorch there‚Äôs torch.utils.data.Dataset andtorch.utils.data.DataLoader. Sometimes you may need to define a custom dataloader, but the default is good enough for most use cases.\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range=5,\n    rescale=1./255,\n    horizontal_flip=True,\n    shear_range=0.2,\n    zoom_range=0.2,\n    validation_split=0.2)\n\n# create a image data generator object. \n# all these are data augmentation parameters.\n\n# now let's specify the image size to which each image will be resized to\nimg_height, img_width = 224, 224\nbatch_size = 64\n\nThere are two ways to create data generators/loaders from above instance. I recommend going through the ImageDataGenerator API page, but the two methods are:\n\nFlow from Dataframe (here, you can contain the filenames and labels in a pandas dataframe, and pass the dataframe), we are using this, remember the dataframe we created earlier?\nFlow from Directory (here, you can pass the path of a directory. This directory should contain subfolders corresponding to each class. You will have to rearrange your directory so that it looks like this.\n\nTrain/\n---| Dog/\n   ---| Dog1.jpg\n   ---| Dog1.jpg\n---| Cat/\n   ---| Cat1.jpg\n   ---| Cat2.jpg\nVal/\n---| Dog/\n   ---| Dog5.jpg\n   ---| Dog6.jpg\n---| Cat/\n   ---| Cat7.jpg\n   ---| Cat8.jpg\nSince we are using method1, we will not be rearranging the folders.\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    train_df,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training') # set as training data\n\n# remember we put 0.2 validation split while defining ImageDataGenerator?\nvalidation_generator = train_datagen.flow_from_dataframe(\n    train_df, # same directory as training data\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation') # set as validation data\n\nprint(\"Class Indices:\", train_generator.class_indices)\n\nFound 20000 validated image filenames belonging to 2 classes.\nFound 5000 validated image filenames belonging to 2 classes.\nClass Indices: {'cat': 0, 'dog': 1}"
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#define-a-model",
    "href": "posts/tutorials/keras_cnns_intro.html#define-a-model",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "4. Define A Model",
    "text": "4. Define A Model\nNow we come to the meaty part- defining the CNN network, the engine of our application. As we discussed earlier, we are going to define a series of Convolution Layers, and each convolution layer consists of a convolution operation followed by a max pool layer.\nThis order is not mandatory- you can have two Convolution operations followed by one max pool, or three, or an average pool- that‚Äôs the neat part of neural networks, they‚Äôre so adaptable and malleable, and the best configurations are often found out by trial and error. In this case, we are going with the wisdom of our elders, and go by this order.\n\nHead of A CNN\nAs we apply more Conv Layers, you will get a transformed matrix of somesize x somesize. But what good is a matrix to us? We need a simple answer- 0 or 1! In order to get this answer, we ‚Äúflatten‚Äù the final matrix to a single vector of size somesize-times-2 x 1. Then we pass it through more neural network neurons to get a single neuron at the end. This neuron‚Äôs output is constrained between 0 and 1. This is our final probability! If it‚Äôs greater than 0.5, the prediction is 1, if not, it‚Äôs 0.\nIf you have more than 2 classes, like predicting a digit. In this case, there would be ten neurons at the end. Each of their output would be the probability of that class.\n\n\nModel Hyperparameters\nApart from number of Conv layers, there are other design choices while designing a CNN- they include selecting the optimizer, the learning rate, the loss function, the number of filters. For an introductory notebook, discussion on those is not necessary.\nWhat‚Äôs important to note: passing the input shape to the first layer. Ensuring that the last layer corresponds to the number of classes.\nTry tinkering with this configuration to see how the results change. Try using only one Conv Layer, reducing number of filters, increasing number of filters, etc.\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', \n                 input_shape=(img_width, img_height, 3)))\n# 32 here means this layer will contain 32 filters of size 3x3 being learnt\nmodel.add(BatchNormalization())\n# batchnorm is a useful layer that helps in convergence\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# maxpooling will reduce the size of the image\nmodel.add(Dropout(0.25))\n# dropout is used for regularization, ensuring that model doesn't overfit\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# convolutional block is complete. now on to defining the \"head\"\n\n# first flatten the matrix to get a single array\nmodel.add(Flatten())\n# adding a dense hidden layer of neurons\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n# finally the output layer with neurons=number of classes and softmax activation\nmodel.add(Dense(2, activation='softmax')) # 2 because we have cat and dog classes\n\n2023-01-07 11:38:12.212718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 11:38:12.342711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 11:38:12.343593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 11:38:12.345511: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-07 11:38:12.345912: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 11:38:12.346654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 11:38:12.347302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 11:38:14.495879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 11:38:14.496731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 11:38:14.497497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 11:38:14.498130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n\n\nLet‚Äôs visualize how our network looks like, and what the shapes of input and output at each layer.\nThe shapes of input and output can be useful for debugging. If there‚Äôs a mismatch between output of one layer and input of next, model will throw up error.\n\nplot_model(model, show_shapes=True, show_layer_names=False, dpi=60)\n\n\n\n\n\n# compile the model while defining a loss, optimizer, and metrics to track, \n# and add callbacks if necessary\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])"
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#train-the-model",
    "href": "posts/tutorials/keras_cnns_intro.html#train-the-model",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "5. Train the Model",
    "text": "5. Train the Model\n\nepochs=10\nhistory = model.fit(\n    train_generator, \n    epochs=epochs,\n    validation_data=validation_generator,\n)\n\n2023-01-07 11:38:16.953970: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n\n\nEpoch 1/10\n\n\n2023-01-07 11:38:20.190526: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n\n\n313/313 [==============================] - 334s 1s/step - loss: 0.7747 - accuracy: 0.6353 - val_loss: 1.8673 - val_accuracy: 0.4876\nEpoch 2/10\n313/313 [==============================] - 324s 1s/step - loss: 0.5450 - accuracy: 0.7254 - val_loss: 0.5757 - val_accuracy: 0.7110\nEpoch 3/10\n313/313 [==============================] - 325s 1s/step - loss: 0.4821 - accuracy: 0.7671 - val_loss: 0.4726 - val_accuracy: 0.7732\nEpoch 4/10\n313/313 [==============================] - 326s 1s/step - loss: 0.4332 - accuracy: 0.7987 - val_loss: 0.4627 - val_accuracy: 0.7800\nEpoch 5/10\n313/313 [==============================] - 323s 1s/step - loss: 0.3989 - accuracy: 0.8184 - val_loss: 0.4141 - val_accuracy: 0.8128\nEpoch 6/10\n313/313 [==============================] - 322s 1s/step - loss: 0.3702 - accuracy: 0.8349 - val_loss: 0.4598 - val_accuracy: 0.7822\nEpoch 7/10\n313/313 [==============================] - 323s 1s/step - loss: 0.3409 - accuracy: 0.8484 - val_loss: 0.3489 - val_accuracy: 0.8442\nEpoch 8/10\n313/313 [==============================] - 324s 1s/step - loss: 0.3169 - accuracy: 0.8641 - val_loss: 0.3765 - val_accuracy: 0.8478\nEpoch 9/10\n313/313 [==============================] - 323s 1s/step - loss: 0.3124 - accuracy: 0.8629 - val_loss: 0.4741 - val_accuracy: 0.7906\nEpoch 10/10\n313/313 [==============================] - 324s 1s/step - loss: 0.2925 - accuracy: 0.8721 - val_loss: 0.6446 - val_accuracy: 0.7484\n\n\n\ndef plot_loss(history):\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 6))\n    ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n    ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n    ax1.set_xticks(np.arange(1, epochs, 1))\n\n    ax2.plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n    ax2.plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n    ax2.set_xticks(np.arange(1, epochs, 1))\n\n    legend = plt.legend(loc='best', shadow=True)\n    plt.tight_layout()\n    plt.show()\nplot_loss(history)"
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#get-the-predictions",
    "href": "posts/tutorials/keras_cnns_intro.html#get-the-predictions",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "6. Get the Predictions",
    "text": "6. Get the Predictions\nNow that the model is trained, let‚Äôs check if the model is giving us good predictions, by trying it out on the test data.\n\ntest_files = glob.glob('/kaggle/working/test1/*.jpg')\ntest_df = pd.DataFrame({'filename': test_files})\ntest_gen = ImageDataGenerator(rescale=1./255)\ntest_generator = test_gen.flow_from_dataframe(\n    test_df, \n    x_col='filename',\n    y_col=None,\n    class_mode=None,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    shuffle=False)\n\nFound 12500 validated image filenames.\n\n\n\ndef visualize_predictions(test_generator, model):\n    plt.figure(figsize=(12, 12))\n    for i in range(0, 15):\n        plt.subplot(5, 3, i+1)\n        for X_batch in test_generator:\n            prediction = model.predict(X_batch)[0]\n            image = X_batch[0]\n            plt.imshow(image)\n            plt.title('cat' if np.argmax(prediction)==0 else \"dog\")\n            break\n    plt.tight_layout()\n    plt.show()\nvisualize_predictions(test_generator, model)"
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#transfer-learning-1",
    "href": "posts/tutorials/keras_cnns_intro.html#transfer-learning-1",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nIf you made it here, we‚Äôll talk about deep learning‚Äôs most important tricks- transfer learning!\nNeural networks are notoriously data hungry- they can eat millions of images and digest them to be able to generalize upon their features. In this case, what if you don‚Äôt have millions of images?\nIn this case, you use a model that has been trained on millions of images. And take it as your starting point. And train your model from them. Those massive datasets don‚Äôt necessarily have to be related to your image classes.\nThere are many publicly available models like resnet, xception, convnext (particular architectures of CNNs) trained on ImageNet dataset (a very large image dataset with 100+ different classes). You can simply download them, and use it for your task (classifying dogs), and it will work much better than defining a model from new.\nWe‚Äôll implement a model using transfer learning below.\n\nfrom tensorflow.keras.applications import ResNet50, Xception\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\n\n# ResNet50 is our \"backbone\" of sorts, a CNN architecture \n# pretrained on the imagenet dataset\n# we are only taking the CNN portion of it (include_top = False)\n# and dropping the dense layer, we'll initialize a dense network of our own\n\nbasemodel = Xception(include_top = False, \n                   weights = 'imagenet',\n                  input_shape=(img_height, img_width, 3))\n\nbasemodel.trainable = False\n\n# use the output of the baseModel to create a \"head\"\nheadModel = basemodel.output\nheadModel = MaxPooling2D(pool_size=(5, 5))(headModel)\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(128, activation=\"relu\")(headModel)\nheadModel = Dropout(0.8)(headModel)\n# headModel = Dense(32, activation=\"relu\")(headModel)\n# headModel = Dropout(0.5)(headModel)\nheadModel = Dense(2, activation=\"softmax\")(headModel)\n# at the end, we'll have two neurons, for two of the classes\n\n# we're \"disabling\" the backbone, and only training the head for this task\n# we're assuming that the backbone is already sufficiently trained to generate\n# features from images like ours.\n# we can also \"disable\" all CNN layers except last 4\n\n# create a model object\nmodel = Model(inputs=basemodel.input, outputs=headModel)\n\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n83689472/83683744 [==============================] - 1s 0us/step\n83697664/83683744 [==============================] - 1s 0us/step\n\n\n\n# plot_model(model, show_shapes=True, show_layer_names=False, dpi=60)"
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#experiment-tracking",
    "href": "posts/tutorials/keras_cnns_intro.html#experiment-tracking",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "Experiment Tracking",
    "text": "Experiment Tracking\nIn earlier case, we plotted our losses and metrics after the training was done, manually using matplotlib. But there are tools available using which we can observe real-time how our training is progressing. They also log system metrics like GPU usage, and can keep track of multiple experiments, hyperparameters etc. One such tool is wandb.ai, using which you can track your model even on phone as it‚Äôs running in background.\nUsing it is very simple, signup on wandb.ai, and add only few lines of code. Get the API token, and go through this discussion on how to add it as a kaggle secret.\n\nuse_wandb = True # set to false if you don't want to use wandb for tracking\nif use_wandb:\n    from kaggle_secrets import UserSecretsClient\n    import wandb\n    from wandb.keras import WandbCallback\n    user_secrets = UserSecretsClient()\n    wandb_api = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=wandb_api)\n\nwandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\nwandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\nwandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\nwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\n\n\nif use_wandb:\n    wandb.init(project=\"keras_cats_and_dogs\", config={\"batch_size\": batch_size})\n\nwandb: Currently logged in as: shindeshu. Use `wandb login --relogin` to force relogin\n\n\nwandb version 0.13.7 is available!  To upgrade, please run:\n $ pip install wandb --upgrade\n\n\nTracking run with wandb version 0.12.21\n\n\nRun data is saved locally in /kaggle/working/wandb/run-20230107_123626-2bw5qpbs\n\n\nSyncing run fancy-plant-6 to Weights & Biases (docs)\n\n\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nopt = Adam(learning_rate=0.001)\nmodel.compile(loss='categorical_crossentropy', \n              optimizer=opt, \n              metrics=['accuracy'])\n\n\nepochs=5\ncallbacks=[EarlyStopping(monitor='loss', patience=2), ]\nif use_wandb: callbacks.append(WandbCallback())\n\nhistory = model.fit_generator(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    callbacks=callbacks,\n)\n\nwandb: WARNING The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n  warnings.warn('`Model.fit_generator` is deprecated and '\n2023-01-07 12:36:46.761375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 12:36:46.761921: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n2023-01-07 12:36:46.762185: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n2023-01-07 12:36:46.765305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 12:36:46.765841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 12:36:46.766331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 12:36:46.766954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 12:36:46.767464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-07 12:36:46.767852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n2023-01-07 12:36:46.787714: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n  function_optimizer: function_optimizer did nothing. time = 0.01ms.\n  function_optimizer: function_optimizer did nothing. time = 0.002ms.\n\n\n\nEpoch 1/5\n313/313 [==============================] - 350s 1s/step - loss: 0.1106 - accuracy: 0.9637 - val_loss: 0.0460 - val_accuracy: 0.9844\n\n\n/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  category=CustomMaskWarning)\n\n\nEpoch 2/5\n313/313 [==============================] - 335s 1s/step - loss: 0.0764 - accuracy: 0.9768 - val_loss: 0.0479 - val_accuracy: 0.9828\nEpoch 3/5\n313/313 [==============================] - 334s 1s/step - loss: 0.0690 - accuracy: 0.9771 - val_loss: 0.0380 - val_accuracy: 0.9844\nEpoch 4/5\n313/313 [==============================] - 331s 1s/step - loss: 0.0626 - accuracy: 0.9792 - val_loss: 0.0399 - val_accuracy: 0.9854\nEpoch 5/5\n313/313 [==============================] - 337s 1s/step - loss: 0.0629 - accuracy: 0.9777 - val_loss: 0.0380 - val_accuracy: 0.9836"
  },
  {
    "objectID": "posts/tutorials/keras_cnns_intro.html#massive-improvement-using-transfer-learning",
    "href": "posts/tutorials/keras_cnns_intro.html#massive-improvement-using-transfer-learning",
    "title": "A Friendly ‚ò∫Ô∏è Introduction to CNNs with Keras",
    "section": "Massive Improvement Using Transfer Learning",
    "text": "Massive Improvement Using Transfer Learning\nWhen we defined a custom model earlier, the best validation accuracy we got after 10 epochs was 85%.\nHere, by using a pre-trained model, our validation accuracy after 1 epoch is as high as 98%!\nAs we can see, using a pre-trained model can really boost our performance with minimal training efforts.\n\nplot_loss(history)\n\n\n\n\n\nvisualize_predictions(test_generator, model)"
  }
]