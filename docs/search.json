[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "gnn\n\n\n\n\nIn this notebook we‚Äôll try to implement a simple message passing neural network (Graph Convolution Layer) from scratch, and a step-by-step introduction to the topic.\n\n\n\n\n\n\nJan 25, 2022\n\n\nShubham Shinde\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ngnn\n\n\n\n\nA brief, very brief, introduction to the hot field of graph neural networks.\n\n\n\n\n\n\nJan 25, 2022\n\n\nShubham Shinde\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shubham Shinde",
    "section": "",
    "text": "Hi there!\nI am a data scientist who has been around for a while. I‚Äôve built üõ†Ô∏è solutions for sectors ranging from ‚úàÔ∏è aerospace to üè• insurance. I‚Äôve worked in areas like insurance fraud detection, satellite image segmentation, engine failure prediction, among others. Checkout my resume for more!\nI am passionate about AI, and the way it is changing our world. This is why I love to keep on top of the latest advances - Stable Diffusion, Transformers, RecSys, MLOps, etc. Some of it I try to share back through my blog, where I write everything about AI that is churning in my brain.\nWhen I‚Äôm not doing tech, I love to sing üé§, and read üìô, and scribble ‚úèÔ∏è once in a while. I love history and the classics as well."
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html",
    "href": "posts/gnns/gnn_from_scratch.html",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "",
    "text": "graph-img\nIf you are unfamiliar with GNNs in general, please go through my small intro blogpost. Message Passing is one of the more popular concepts in GNNs, and that is what we‚Äôll try to implement here. Specifically we are implementing the Graph Convolutional Layer/Network proposed by Kipf et al in 2016. You can go through a detailed blogpost of his or the original paper."
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#representing-a-graph",
    "href": "posts/gnns/gnn_from_scratch.html#representing-a-graph",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Representing a Graph",
    "text": "Representing a Graph\nBefore we start on to Graph convolutions, let‚Äôs first present it out on how do we represent a graph in code. Mathematically, a graph is defined as a tuple of a set of nodes/vertices , and a set of edges/links . Further, each edge is a pair of two vertices, and represents a connection between them.\nVisually, a graph would look something like this:\n\n\n\nThe vertices are , and edges .\nThere are many ways to represent graphs in memory- two of them include ‚Äúadjacency matrix‚Äù (\\(a\\)) and ‚Äúedge list‚Äù. If the number of nodes is \\(n\\), the adjacency matrix is \\(n x n\\). If there‚Äôs an edge from node \\(n_i\\) to \\(n_j\\), the element \\(a_{ij}\\) is equal to 1. Likewise, the other elements of \\(a\\) are populated.\n[[ 0 1 0 0 ]\n [ 1 0 1 1 ]\n [ 0 1 0 1 ]\n [ 0 1 1 0 ]]\nWorking with adjacency matrix for graph operations is easier, although they have their limitations. While established libraries like dgl or pytorch-geometric use edge-list format of data, here we are working with an adjacency matrix."
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#graph-convolutions",
    "href": "posts/gnns/gnn_from_scratch.html#graph-convolutions",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Graph Convolutions",
    "text": "Graph Convolutions\nGraph convolutions are somewhat similar to image convolutions, in that they take their neighbourhood information and aggregate to get a richer understanding of their position. Also, the ‚Äúparameters‚Äù of the filters are shared across the entire image, which is analogous to a graph convolution as well, where the parameters are shared across the graph.\nGCNs rely on the message passing paradigm. Each node has a feature vector associated with it. For a given node u, each of its neighbouring nodes \\(v_i\\) send a message derived from its feature vector to it. All these messages are aggregated alongwith its own feature vector, and this is used to update this node \\(u\\) to get the final feature vector (or embedding)."
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#current-implementation",
    "href": "posts/gnns/gnn_from_scratch.html#current-implementation",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Current Implementation",
    "text": "Current Implementation\nEach node has a feature vector. This feature will be projected using a linear layer, output of which will be the message that each node passes. We will represent the graph as an adjacency matrix, and multiply by the node features (projected) to perform the message passing. This will be divided by the number of neighbours for normalizing, which will give us the output of our first graph convolution layer.\nImporting all our libraries. We are not using libraries like dgl or pytorch-geometric, we will be using plain pytorch. We are also using networkx for manipulating graph.\nWe will be a creating a random matrix as an adjacency matrix. Creating a matrix with uniform_ method and the bernoulli method.\n\nnodes = 10\nnode_features_size = 4\n\nadj = torch.empty(nodes, nodes).uniform_(0, 1).bernoulli()\n\nVisualizing the graph we created with networkx library\n\ngraph = nx.from_numpy_matrix(adj.numpy())\ngraph.remove_edges_from(nx.selfloop_edges(graph))\n\npos = nx.kamada_kawai_layout(graph)\nnx.draw(graph, pos, with_labels=True)\n\n\n\n\npng\n\n\nCreating random features for our nodes. These features will go through a dense layer and then act as our messages.\n\nnode_features = torch.empty(nodes, node_features_size).uniform_(0, 1).bernoulli()#.view(1, nodes, node_features_size)\nnode_features\n\nThe features will pass through a linear layer to create our messages\n\nprojector = nn.Linear(node_features_size, 5)\n\nnode_feat_proj = projector(node_features)\n\nnum_neighbours = adj.sum(dim=-1, keepdims=True)\n\ntorch.matmul(adj, node_feat_proj)/num_neighbours\n\ntensor([[-0.5067, -0.2463, -0.0555,  0.2188,  0.4031],\n            [-0.8397,  0.0945,  0.5124,  0.1179, -0.0296],\n            [-0.6457,  0.2369,  0.5048, -0.0216,  0.1531],\n            [-0.9893,  0.4223,  0.7235,  0.3212, -0.1165],\n            [-0.5876,  0.2246,  0.5227, -0.1519,  0.1979],\n            [-0.6133, -0.0359,  0.2532,  0.0760,  0.2250],\n            [-0.7740,  0.2055,  0.5252,  0.1075,  0.0174],\n            [-0.7827,  0.1653,  0.5654,  0.0135, -0.0155],\n            [-0.8635,  0.3189,  0.6940,  0.0758, -0.0423],\n            [-0.9374,  0.2670,  0.6672,  0.1805, -0.1292]], grad_fn=<DivBackward0>)\n\nadj.shape, node_feat_proj.shape\n\n(torch.Size([10, 10]), torch.Size([10, 5]))"
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#a-note-on-above-multiplication-operation",
    "href": "posts/gnns/gnn_from_scratch.html#a-note-on-above-multiplication-operation",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "A Note on Above Multiplication Operation",
    "text": "A Note on Above Multiplication Operation\nHow it does achieve our objective, i.e.¬†summing up of messages from neighbouring nodes of a particular node?\nFor simplicity, lets take an example where the adj matrix is $ 7 $ and the message matrix is $ 7 $.\nConsider a single row from the adjacency matrix, that corresponds to a node \\(n_i\\). It might look something like \\[\nA = \\begin{bmatrix}\n    0 & 1 & 0 & 0 & 1 & 0 & 1\\\\\n\\end{bmatrix}\n\\]\nAnd the message matrix is \\(7 \\times 5\\). (seven rows, five columns).\nFor this node, we can observe there are edges existent only for nodes \\({2, 5, 7}\\). When we multiple the above matrix with the message/feature matrix, we will get the elements corresponding to those indexes summed up (since others are multiplied by zero), along the second axis of the feature matrix i.e.¬†we will get a \\(1 \\times 5\\) size vector.\nHere, you can see that only the neighbouring nodes‚Äô features have been summed up to get the final d-length vector."
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#putting-it-all-together",
    "href": "posts/gnns/gnn_from_scratch.html#putting-it-all-together",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Putting It All Together",
    "text": "Putting It All Together\nNow that we‚Äôve done it step-by-step, let us aggregate the operations together in proper functions.\n\nclass GCNLayer(nn.Module):\n    def __init__(self, in_feat, out_feat):\n        super().__init__()\n        self.projector = nn.Linear(in_feat, out_feat)\n\n    def forward(self, node_features, adj):\n        num_neighbours = adj.sum(dim=-1, keepdims=True)\n        node_features = torch.relu(self.projector(node_features))\n        node_features = torch.matmul(adj, node_features)\n        node_features = node_features / num_neighbours\n        node_features = torch.relu(node_features)\n        return node_features\nlayer1 = GCNLayer(node_features_size, 8)\nlayer1(node_features, adj).shape\n\ntorch.Size([10, 8])\n\nlayer2 = GCNLayer(8, 2)\nlayer2(layer1(node_features, adj), adj)\n\ntensor([[0.4279, 0.4171],\n            [0.4724, 0.4304],\n            [0.4318, 0.3761],\n            [0.4315, 0.3860],\n            [0.4520, 0.4132],\n            [0.4449, 0.4049],\n            [0.4346, 0.3827],\n            [0.4614, 0.4176],\n            [0.4446, 0.3860],\n            [0.4068, 0.3582]], grad_fn=<ReluBackward0>)\n\nclass GCNmodel(nn.Module):\n    def __init__(self, in_feat, hid_feat, out_feat):\n        super().__init__()\n        self.gcn_layer1 = GCNLayer(in_feat, hid_feat)\n        self.gcn_layer2 = GCNLayer(hid_feat, out_feat)\n\n    def forward(self, node_features, adj):\n        h = self.gcn_layer1(node_features, adj)\n        h = self.gcn_layer2(h, adj)\n        return h\nmodel = GCNmodel(node_features_size, 12, 2)"
  },
  {
    "objectID": "posts/gnns/gnn_from_scratch.html#solving-a-real-problem",
    "href": "posts/gnns/gnn_from_scratch.html#solving-a-real-problem",
    "title": "Implementing a Graph Neural Network from Scratch",
    "section": "Solving a Real Problem",
    "text": "Solving a Real Problem\nNow that we are able to play around with random data, lets us get to work on some real datasets that we can do basic classification problems on. We will be using the zachary‚Äôs karate club dataset, which is a small dataset of 34 people and the edges include their observed interactions with each other. Our objective: predict which group will each of the people go to once their club is bisected.\n\ndef build_karate_club_graph():\n    g = nx.Graph()\n    edge_list = [(1, 0), (2, 0), (2, 1), (3, 0), (3, 1), (3, 2),\n        (4, 0), (5, 0), (6, 0), (6, 4), (6, 5), (7, 0), (7, 1),\n        (7, 2), (7, 3), (8, 0), (8, 2), (9, 2), (10, 0), (10, 4),\n        (10, 5), (11, 0), (12, 0), (12, 3), (13, 0), (13, 1), (13, 2),\n        (13, 3), (16, 5), (16, 6), (17, 0), (17, 1), (19, 0), (19, 1),\n        (21, 0), (21, 1), (25, 23), (25, 24), (27, 2), (27, 23),\n        (27, 24), (28, 2), (29, 23), (29, 26), (30, 1), (30, 8),\n        (31, 0), (31, 24), (31, 25), (31, 28), (32, 2), (32, 8),\n        (32, 14), (32, 15), (32, 18), (32, 20), (32, 22), (32, 23),\n        (32, 29), (32, 30), (32, 31), (33, 8), (33, 9), (33, 13),\n        (33, 14), (33, 15), (33, 18), (33, 19), (33, 20), (33, 22),\n        (33, 23), (33, 26), (33, 27), (33, 28), (33, 29), (33, 30),\n        (33, 31), (33, 32)]\n    g.add_edges_from(edge_list)\n    return g\n\ng = build_karate_club_graph()\n\nVisualizing our karate club graph:\n\npos = nx.kamada_kawai_layout(g)\nnx.draw(g, pos, with_labels=True)\n\n\n\n\npng\n\n\nWe don‚Äôt have any node features. So here we‚Äôre creating a one-hot vector for each node based on its id. Together, it‚Äôd be a single identity matrix for the graph.\nAt the beginning, only the instructor and president nodes are labelled. Later on each person will join one of the groups headed by these two. So it‚Äôs a binary classification, and the only labeled nodes we have are two.\n\nnode_features =  torch.eye(34) \nlabeled_nodes = torch.tensor([0, 33])  # only the instructor and the president nodes are labeled\nlabels = torch.tensor([0, 1])\n\n# since our code only works on adjacency matrix and not on edge-list\n\nadj_matrix = torch.from_numpy(nx.adjacency_matrix(g).todense()).float()\n\n# define our gcn model\n\nmodel = GCNmodel(34, 32, 2)\n\n# do a single pass just for a check\n\nmodel(node_features, adj_matrix)\n\nLets get to the meat of it: time to train our model. We create the usual pytorch pipeline. If you‚Äôve worked with pytorch before, this is familiar to you. Even if not, you can get a certain idea if you know some basics of neural networks / backprop.\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\nall_logits = []\nfor epoch in range(100):\n    logits = model(node_features, adj_matrix)\n    # we save the logits for visualization later\n    all_logits.append(logits.detach())\n    logp = F.log_softmax(logits, 1)\n    # we only compute loss for labeled nodes\n    loss = F.nll_loss(logp[labeled_nodes], labels)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))\n\n    Epoch 0 | Loss: 0.6887\n    Epoch 1 | Loss: 0.6823\n    Epoch 2 | Loss: 0.6756\n    Epoch 3 | Loss: 0.6704\n    Epoch 4 | Loss: 0.6653\n    Epoch 5 | Loss: 0.6592\n    Epoch 6 | Loss: 0.6529\n    Epoch 7 | Loss: 0.6465\n    Epoch 8 | Loss: 0.6396\n    Epoch 9 | Loss: 0.6320\n    Epoch 10 | Loss: 0.6239\n    Epoch 11 | Loss: 0.6151\n    Epoch 12 | Loss: 0.6064\n    Epoch 13 | Loss: 0.5973\n    Epoch 14 | Loss: 0.5878\n    Epoch 15 | Loss: 0.5783\n    Epoch 16 | Loss: 0.5686\n    Epoch 17 | Loss: 0.5585\n    Epoch 18 | Loss: 0.5482\n    Epoch 19 | Loss: 0.5382\n    Epoch 20 | Loss: 0.5281\n    Epoch 21 | Loss: 0.5182\n    Epoch 22 | Loss: 0.5085\n    Epoch 23 | Loss: 0.4990\n    Epoch 24 | Loss: 0.4899\n    Epoch 25 | Loss: 0.4810\n    Epoch 26 | Loss: 0.4725\n    Epoch 27 | Loss: 0.4642\n    Epoch 28 | Loss: 0.4560\n    Epoch 29 | Loss: 0.4477\n    Epoch 30 | Loss: 0.4397\n    Epoch 31 | Loss: 0.4331\n    Epoch 32 | Loss: 0.4267\n    Epoch 33 | Loss: 0.4204\n    Epoch 34 | Loss: 0.4143\n    Epoch 35 | Loss: 0.4082\n    Epoch 36 | Loss: 0.4037\n    Epoch 37 | Loss: 0.3994\n    Epoch 38 | Loss: 0.3952\n    Epoch 39 | Loss: 0.3911\n    Epoch 40 | Loss: 0.3873\n    Epoch 41 | Loss: 0.3837\n    Epoch 42 | Loss: 0.3802\n    Epoch 43 | Loss: 0.3767\n    Epoch 44 | Loss: 0.3733\n    Epoch 45 | Loss: 0.3698\n    Epoch 46 | Loss: 0.3670\n    Epoch 47 | Loss: 0.3655\n    Epoch 48 | Loss: 0.3638\n    Epoch 49 | Loss: 0.3620\n    Epoch 50 | Loss: 0.3602\n    Epoch 51 | Loss: 0.3586\n    Epoch 52 | Loss: 0.3571\n    Epoch 53 | Loss: 0.3573\n    Epoch 54 | Loss: 0.3564\n    Epoch 55 | Loss: 0.3544\n    Epoch 56 | Loss: 0.3542\n    Epoch 57 | Loss: 0.3539\n    Epoch 58 | Loss: 0.3536\n    Epoch 59 | Loss: 0.3533\n    Epoch 60 | Loss: 0.3529\n    Epoch 61 | Loss: 0.3525\n    Epoch 62 | Loss: 0.3522\n    Epoch 63 | Loss: 0.3518\n    Epoch 64 | Loss: 0.3514\n    Epoch 65 | Loss: 0.3511\n    Epoch 66 | Loss: 0.3508\n    Epoch 67 | Loss: 0.3505\n    Epoch 68 | Loss: 0.3502\n    Epoch 69 | Loss: 0.3504\n    Epoch 70 | Loss: 0.3498\n    Epoch 71 | Loss: 0.3497\n    Epoch 72 | Loss: 0.3439\n    Epoch 73 | Loss: 0.3194\n    Epoch 74 | Loss: 0.2869\n    Epoch 75 | Loss: 0.2505\n    Epoch 76 | Loss: 0.2138\n    Epoch 77 | Loss: 0.1789\n    Epoch 78 | Loss: 0.1476\n    Epoch 79 | Loss: 0.1206\n    Epoch 80 | Loss: 0.0984\n    Epoch 81 | Loss: 0.0811\n    Epoch 82 | Loss: 0.0682\n    Epoch 83 | Loss: 0.0587\n    Epoch 84 | Loss: 0.0516\n    Epoch 85 | Loss: 0.0459\n    Epoch 86 | Loss: 0.0407\n    Epoch 87 | Loss: 0.0356\n    Epoch 88 | Loss: 0.0307\n    Epoch 89 | Loss: 0.0262\n    Epoch 90 | Loss: 0.0223\n    Epoch 91 | Loss: 0.0191\n    Epoch 92 | Loss: 0.0164\n    Epoch 93 | Loss: 0.0142\n    Epoch 94 | Loss: 0.0124\n    Epoch 95 | Loss: 0.0111\n    Epoch 96 | Loss: 0.0101\n    Epoch 97 | Loss: 0.0093\n    Epoch 98 | Loss: 0.0087\n    Epoch 99 | Loss: 0.0081\nWe can see the loss converging. This dataset doesn‚Äôt really have a valid set or anything, so there are no metrics to be presented here. But we can visualize them directly which can be fun to see. Here, we can create an animation of the results of each epoch, and watch them fluctuate as the model converges.\nThis vis code was taken from dgl documentation. The dgl docs are a great place to start learning about graph neural networks!\n\nimport matplotlib.animation as animation\nimport matplotlib.pyplot as plt\n\ndef draw(i):\n    cls1color = '#00FFFF'\n    cls2color = '#FF00FF'\n    pos = {}\n    colors = []\n    for v in range(34):\n        pos[v] = all_logits[i][v].numpy()\n        cls = pos[v].argmax()\n        colors.append(cls1color if cls else cls2color)\n    ax.cla()\n    ax.axis('off')\n    ax.set_title('Epoch: %d' % i)\n    pos = nx.kamada_kawai_layout(g)\n    nx.draw_networkx(g.to_undirected(), pos, node_color=colors,\n            with_labels=True, node_size=300, ax=ax)\n\nfig = plt.figure(dpi=150)\nfig.clf()\nax = fig.subplots()\ndraw(0)  # draw the prediction of the first epoch\nplt.close()\n\nani = animation.FuncAnimation(fig, draw, frames=len(all_logits), interval=200)\n\nani.save(\"karate.gif\", writer=\"pillow\")"
  },
  {
    "objectID": "posts/gnns/intro_to_gnn.html",
    "href": "posts/gnns/intro_to_gnn.html",
    "title": "A Brief Intro To Graph Neural Networks",
    "section": "",
    "text": "Deep Learning has revolutionized machine learning on all types of tasks ranging from computer vision to natural language processing or sequence modeling. Most of these applications however involve mostly euclidean data that are constrained to some fixed dimensions.\nWhat happens when your data is of non-euclidean nature? Graphs are one way to represent such non-euclidean data, which represent it in form of objects linked with each other through relationships. Machine learning using graphs has always been around, however with the advances in deep learning, recently there have been some exciting developments for learning on graphs.\n\nWhat is a graph, you say? Graph is a set of vertices / nodes (our objects of interest), with edges (relationships between our objects). For example in a social media graph, an account would be a node, and them following someone could be an edge. Numerically, a graph can be represented as a matrix (adjacency), or as a list (of edges).\nWhat data can be represented in the form of graphs? A lot of it! Interactions on a social media site, financial transactions, citation networks, molecules, all these can be represented in the form of graphs and can then be leveraged for machine learning.\nGraph representation learning: when we do have a graph (i.e.¬†our nodes, their features, their edges, their features), our objective is to learn embeddings for each node, such that two ‚Äúsimilar‚Äù nodes will have their embeddings closer in space. This embedding for a node should bake into itself its relationships and its neighbourhood and their features (apart from its own). This embedding vector can then be used for our downstream tasks.\n\nLearning the embedding: while there are many ways to skin this particular cat, the one that‚Äôs hot right now is called ‚Äúmessage passing‚Äù or a graph convolution layer. The core concept is pretty simple. Lets say our current node of interest, has three neighbours. Each one of these will pass a ‚Äúmessage‚Äù to our node, this message being the current state of the node. These messages will be aggregated together with our node‚Äôs current state, and this will be used to update the node‚Äôs state to next state. After covering for all nodes, you‚Äôd get a complete pass over the entire graph, for a single graph convolution layer. Different frameworks will have different ways of passing messages, or updating them, but the underlying principle is pretty same.\nThe details of message passing, we‚Äôll go over in another post- since this is supposed to be a ‚Äúbrief‚Äù introduction."
  }
]